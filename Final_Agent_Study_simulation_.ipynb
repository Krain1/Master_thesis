{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE3a4lpXVvI8"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 0: Google Drive\n",
        "# =============================================================================\n",
        "\n",
        "#The required excel data are also in the online appendix. The google drive paths would need to be replaced or adapted in the respective places\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao8u3LoLQs0b"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 1: Initialization and Enhanced Memory Module\n",
        "# =============================================================================\n",
        "\n",
        "# Install all necessary packages\n",
        "!pip install -q playwright openai nest_asyncio pandas openpyxl jsonschema langchain langchain-community chromadb sentence-transformers duckduckgo-search beautifulsoup4 requests langgraph\n",
        "\n",
        "# Install Playwright browser driver\n",
        "!playwright install --with-deps\n",
        "\n",
        "# Imports for Enhanced Memory Module\n",
        "from typing import List, Dict, Any, Optional, Tuple, TypedDict\n",
        "from datetime import datetime\n",
        "import json\n",
        "import uuid\n",
        "import time\n",
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "# LangGraph imports for Tool Orchestration\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# Further imports for tools\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Tool/Agent Communication Protocol\n",
        "class MessageType(Enum):\n",
        "    REQUEST = \"request\"\n",
        "    RESPONSE = \"response\"\n",
        "    INSIGHT = \"insight\"\n",
        "    CHALLENGE = \"challenge\"\n",
        "    UPDATE = \"update\"\n",
        "    LEARNING_FEEDBACK = \"learning_feedback\"\n",
        "\n",
        "@dataclass\n",
        "class AgentMessage:\n",
        "    \"\"\"Structured message for communication\"\"\"\n",
        "    from_agent: str\n",
        "    to_agent: str\n",
        "    message_type: MessageType\n",
        "    content: dict\n",
        "    priority: int = 1  # 1=low, 2=medium, 3=high\n",
        "    timestamp: str = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.timestamp is None:\n",
        "            self.timestamp = datetime.now().isoformat()\n",
        "\n",
        "@dataclass\n",
        "class ThoughtNode:\n",
        "    \"\"\"Represents a thought in the chain-of-thought process\"\"\"\n",
        "    content: str\n",
        "    is_consistent: bool\n",
        "    revision_needed: bool\n",
        "    timestamp: str\n",
        "    confidence_score: Optional[float] = None\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"Shared state between all agents\"\"\"\n",
        "    # Current task\n",
        "    current_question: str\n",
        "    current_brand: str\n",
        "    current_product: Optional[str]\n",
        "    question_type: str\n",
        "\n",
        "    # Profile and Memory\n",
        "    profile: dict\n",
        "    memory: Any\n",
        "\n",
        "    # Communication\n",
        "    messages: List[AgentMessage]\n",
        "    learning_feedback: Optional[dict]\n",
        "\n",
        "    # Reasoning and decision\n",
        "    survey_reasoning: Optional[str]\n",
        "    bidding_reasoning: Optional[str]\n",
        "    confidence_score: float\n",
        "    tool_results: Optional[dict]\n",
        "    final_answer: int\n",
        "    final_reasoning: str\n",
        "    consistency_feedback: Optional[str]\n",
        "    use_brand_analysis: bool\n",
        "    needs_challenge: bool\n",
        "    initial_rating: int\n",
        "    pre_learning_rating: Optional[int]\n",
        "\n",
        "\n",
        "\n",
        "class EnhancedMemoryModule:\n",
        "    \"\"\"\n",
        "    Memory Module with:\n",
        "    - Vector-based Long-Term Memory for semantic retrieval\n",
        "    - Structured short term memory with context window\n",
        "    - Episodic memory for important decisions\n",
        "    - Chain-of-thought history for reasoning tracking\n",
        "    - Global Memory as Learning Feedback Storage\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agent_name: str, max_short_term_size: int = 20):\n",
        "        self.agent_name = agent_name\n",
        "        self.max_short_term_size = max_short_term_size\n",
        "\n",
        "        print(f\"Initialize memory for {agent_name}...\")\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "\n",
        "        # Long-Term Memory as Vector Store\n",
        "        unique_id = str(uuid.uuid4())[:8]\n",
        "        self.long_term_memory = Chroma(\n",
        "            embedding_function=self.embeddings,\n",
        "            collection_name=f\"agent_{agent_name.replace(' ', '_')}_{unique_id}\"\n",
        "        )\n",
        "\n",
        "        # Short-Term Memory with limited size\n",
        "        self.short_term_memory: List[Dict[str, Any]] = []\n",
        "\n",
        "        # Episodic Memory for important events\n",
        "        self.episodic_memory: List[Dict[str, Any]] = []\n",
        "\n",
        "        # Chain-of-Thought History\n",
        "        self.cot_history: List[ThoughtNode] = []\n",
        "\n",
        "        # Profile and Reflection\n",
        "        self.profile_data: Optional[Dict] = None\n",
        "        self.profile_reflection: Optional[str] = None\n",
        "        self.actual_self_reflection: Optional[str] = None\n",
        "        self.ideal_self_reflection: Optional[str] = None\n",
        "\n",
        "        # Agent - Tool Communication Log\n",
        "        self.agent_messages: List[AgentMessage] = []\n",
        "\n",
        "        # Survey ratings tracker for consistency\n",
        "        self._survey_ratings: Dict[str, Dict[str, int]] = {}\n",
        "\n",
        "        # Global Memory: Learning Feedback Storage\n",
        "        self.learning_feedback = {\n",
        "            \"survey_adjustments\": {},  # {question_type: {\"avg_diff\": float, \"samples\": int}}\n",
        "            \"bidding_adjustments\": {},  # {brand: {\"avg_diff\": float, \"samples\": int}}\n",
        "            \"profile_based_patterns\": {},  # Pattern based on profile similarity\n",
        "            \"accumulated_wisdom\": []  # List of insights across multiple agents\n",
        "        }\n",
        "\n",
        "        print(f\"‚úÖ Memory Module for {agent_name} initialized with Learning Support!\")\n",
        "\n",
        "    def store_profile(self, profile: dict, general_reflection: str, actual_self_reflection: str, ideal_self_reflection: str):\n",
        "        \"\"\"Saves profile and its psychological reflections in the LTM\"\"\"\n",
        "        self.profile_data = profile\n",
        "        self.profile_reflection = general_reflection\n",
        "        self.actual_self_reflection = actual_self_reflection\n",
        "        self.ideal_self_reflection = ideal_self_reflection\n",
        "\n",
        "        # Profile components as separate documents in the Vector Store\n",
        "        docs = []\n",
        "\n",
        "        # 1. Demographic data\n",
        "        demo_text = f\"Demographics: {profile['age']} years old {profile['gender']}, works as {profile['occupation']}, income: {profile['income_range']}\"\n",
        "        docs.append(Document(\n",
        "            page_content=demo_text,\n",
        "            metadata={\"type\": \"demographics\", \"timestamp\": datetime.now().isoformat()}\n",
        "        ))\n",
        "\n",
        "        # 2. Actual Self\n",
        "        actual_text = f\"Actual self traits: {', '.join(profile['actual_traits'])}. Top strength: {profile['actual_top_strength']}\"\n",
        "        docs.append(Document(\n",
        "            page_content=actual_text,\n",
        "            metadata={\"type\": \"actual_self\", \"timestamp\": datetime.now().isoformat()}\n",
        "        ))\n",
        "\n",
        "        # 3. Ideal Self\n",
        "        ideal_text = f\"Ideal self traits: {', '.join(profile['ideal_traits'])}. Top ideal strength: {profile['ideal_top_strength']}\"\n",
        "        docs.append(Document(\n",
        "            page_content=ideal_text,\n",
        "            metadata={\"type\": \"ideal_self\", \"timestamp\": datetime.now().isoformat()}\n",
        "        ))\n",
        "\n",
        "        # 4. Add reflections\n",
        "        docs.extend([\n",
        "            Document(page_content=general_reflection, metadata={\"type\": \"psychological_reflection\", \"timestamp\": datetime.now().isoformat()}),\n",
        "            Document(page_content=actual_self_reflection, metadata={\"type\": \"actual_self_reflection\", \"timestamp\": datetime.now().isoformat()}),\n",
        "            Document(page_content=ideal_self_reflection, metadata={\"type\": \"ideal_self_reflection\", \"timestamp\": datetime.now().isoformat()})\n",
        "        ])\n",
        "\n",
        "        # Add all documents to the Vector Store\n",
        "        self.long_term_memory.add_documents(docs)\n",
        "        print(f\"üìù Profile for {profile['username']} saved in LTM\")\n",
        "\n",
        "    def add_survey_reasoning(self, brand: str, question_code: str, answer: int, reasoning: str, confidence: float = None):\n",
        "        \"\"\"Adds survey reasoning to the STM\"\"\"\n",
        "        # Determine the interaction type\n",
        "        if question_code.startswith(\"Q9\") or question_code.startswith(\"Q10\"):\n",
        "            interaction_type = \"self_congruence\"\n",
        "        elif question_code.startswith(\"Q11\"):\n",
        "            interaction_type = \"attachment\"\n",
        "        else:\n",
        "            interaction_type = \"other\"\n",
        "\n",
        "        # Create Interaction Dictionary\n",
        "        interaction = {\n",
        "            \"brand\": brand,\n",
        "            \"type\": interaction_type,\n",
        "            \"question\": f\"{brand}_{question_code}\",\n",
        "            \"answer\": answer,\n",
        "            \"reasoning\": reasoning,\n",
        "            \"confidence\": confidence,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Add to STM\n",
        "        self.short_term_memory.append(interaction)\n",
        "\n",
        "        # Limit STM\n",
        "        if len(self.short_term_memory) > self.max_short_term_size:\n",
        "            self.short_term_memory = self.short_term_memory[-self.max_short_term_size:]\n",
        "\n",
        "        # Items indicating high brand attachment (4-5) added in the episodic memory\n",
        "        if answer >= 4 and interaction_type == \"attachment\":\n",
        "            episodic_entry = {\n",
        "                \"brand\": brand,\n",
        "                \"type\": \"high_attachment\",\n",
        "                \"reasoning\": reasoning,\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "            self.episodic_memory.append(episodic_entry)\n",
        "            print(f\"   üíæ High Attachment to {brand} - Reasoning saved in Episodic Memory\")\n",
        "\n",
        "        # Update survey ratings tracker\n",
        "        if brand not in self._survey_ratings:\n",
        "            self._survey_ratings[brand] = {}\n",
        "        self._survey_ratings[brand][question_code] = answer\n",
        "\n",
        "    def add_brand_reasoning(self, brand: str, interaction_type: str, question: str, reasoning: str, rating: int = None):\n",
        "        \"\"\"General method for brand reasonings\"\"\"\n",
        "        interaction = {\n",
        "            \"brand\": brand,\n",
        "            \"type\": interaction_type,\n",
        "            \"question\": question,\n",
        "            \"reasoning\": reasoning,\n",
        "            \"rating\": rating,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        self.short_term_memory.append(interaction)\n",
        "\n",
        "        if len(self.short_term_memory) > self.max_short_term_size:\n",
        "            self.short_term_memory = self.short_term_memory[-self.max_short_term_size:]\n",
        "\n",
        "    def add_cot_thought(self, thought: str, is_consistent: bool, revision_needed: bool = False, confidence: float = None):\n",
        "        \"\"\"Adds a chain-of-thought node\"\"\"\n",
        "        node = ThoughtNode(\n",
        "            content=thought,\n",
        "            is_consistent=is_consistent,\n",
        "            revision_needed=revision_needed,\n",
        "            timestamp=datetime.now().isoformat(),\n",
        "            confidence_score=confidence\n",
        "        )\n",
        "        self.cot_history.append(node)\n",
        "\n",
        "    def add_agent_message(self, message: AgentMessage):\n",
        "        \"\"\"Saves agent communications with tools\"\"\"\n",
        "        self.agent_messages.append(message)\n",
        "\n",
        "        # Save important messages in the STM too\n",
        "        if message.priority >= 2:\n",
        "            self.short_term_memory.append({\n",
        "                \"type\": \"agent_communication\",\n",
        "                \"from\": message.from_agent,\n",
        "                \"message_type\": message.message_type.value,\n",
        "                \"content_summary\": message.content.get(\"summary\", str(message.content)[:100]),\n",
        "                \"timestamp\": message.timestamp\n",
        "            })\n",
        "\n",
        "    def add_learning_feedback(self, feedback_message: AgentMessage):\n",
        "        \"\"\"Processes and saves learning feedback\"\"\"\n",
        "        if feedback_message.message_type != MessageType.LEARNING_FEEDBACK:\n",
        "            return\n",
        "\n",
        "        content = feedback_message.content\n",
        "\n",
        "        # Survey adjustments\n",
        "        for question, feedback in content.get(\"survey_feedback\", {}).items():\n",
        "            if feedback[\"diff\"] is not None:  # Skip -77 values (none values)\n",
        "                key = question.replace(f\"{content.get('brand', '')}_\", \"\")\n",
        "                if key not in self.learning_feedback[\"survey_adjustments\"]:\n",
        "                    self.learning_feedback[\"survey_adjustments\"][key] = {\n",
        "                        \"total_diff\": 0,\n",
        "                        \"samples\": 0,\n",
        "                        \"avg_diff\": 0\n",
        "                    }\n",
        "\n",
        "                adj = self.learning_feedback[\"survey_adjustments\"][key]\n",
        "                adj[\"total_diff\"] += feedback[\"diff\"]\n",
        "                adj[\"samples\"] += 1\n",
        "                adj[\"avg_diff\"] = adj[\"total_diff\"] / adj[\"samples\"]\n",
        "\n",
        "        # Bidding adjustments\n",
        "        for brand, feedback in content.get(\"bidding_feedback\", {}).items():\n",
        "            if feedback[\"diff\"] is not None:\n",
        "                if brand not in self.learning_feedback[\"bidding_adjustments\"]:\n",
        "                    self.learning_feedback[\"bidding_adjustments\"][brand] = {\n",
        "                        \"total_diff\": 0,\n",
        "                        \"samples\": 0,\n",
        "                        \"avg_diff\": 0\n",
        "                    }\n",
        "\n",
        "                adj = self.learning_feedback[\"bidding_adjustments\"][brand]\n",
        "                adj[\"total_diff\"] += feedback[\"diff\"]\n",
        "                adj[\"samples\"] += 1\n",
        "                adj[\"avg_diff\"] = adj[\"total_diff\"] / adj[\"samples\"]\n",
        "\n",
        "        # Store profile-based patterns\n",
        "        if \"profile_match_score\" in content:\n",
        "            pattern = {\n",
        "                \"match_score\": content[\"profile_match_score\"],\n",
        "                \"recommendations\": content.get(\"recommendations\", {}),\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "            self.learning_feedback[\"profile_based_patterns\"][feedback_message.from_agent] = pattern\n",
        "\n",
        "        # Add to accumulated wisdom\n",
        "        if \"recommendations\" in content:\n",
        "            self.learning_feedback[\"accumulated_wisdom\"].append({\n",
        "                \"insight\": content[\"recommendations\"],\n",
        "                \"from_agent\": feedback_message.from_agent,\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "        print(f\"   üìö Learning feedback processed and saved\")\n",
        "\n",
        "    def get_learning_adjustment(self, question_type: str, brand: str = None) -> float:\n",
        "        \"\"\"Gets average adaptation for a question type\"\"\"\n",
        "        if question_type in self.learning_feedback[\"survey_adjustments\"]:\n",
        "            adj = self.learning_feedback[\"survey_adjustments\"][question_type]\n",
        "            if adj[\"samples\"] > 0:\n",
        "                return adj[\"avg_diff\"]\n",
        "        return 0.0\n",
        "\n",
        "    def get_bidding_adjustment(self, brand: str) -> float:\n",
        "        \"\"\"Get average adjustment for bidding a brand\"\"\"\n",
        "        if brand in self.learning_feedback[\"bidding_adjustments\"]:\n",
        "            adj = self.learning_feedback[\"bidding_adjustments\"][brand]\n",
        "            if adj[\"samples\"] > 0:\n",
        "                return adj[\"avg_diff\"]\n",
        "        return 0.0\n",
        "\n",
        "    def get_accumulated_wisdom(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Returns collected wisdom/insights\"\"\"\n",
        "        return self.learning_feedback[\"accumulated_wisdom\"]\n",
        "\n",
        "    def get_brand_specific_memories(self, brand: str, include_all_brands: bool = False) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Retrieves memories filtered by brand context.\n",
        "\n",
        "        Args:\n",
        "            brand: The current brand to filter for\n",
        "            include_all_brands: If True, returns all brand memories (for bidding)\n",
        "                               If False, returns only current brand memories (for survey)\n",
        "\n",
        "        Returns:\n",
        "            List of filtered memory entries\n",
        "        \"\"\"\n",
        "        if include_all_brands:\n",
        "            # For bidding: Return all brand-related memories\n",
        "            return [m for m in self.short_term_memory\n",
        "                    if m.get(\"brand\") is not None or m.get(\"type\") == \"profile\"]\n",
        "        else:\n",
        "            # For survey: Return only memories for the current brand\n",
        "            return [m for m in self.short_term_memory\n",
        "                    if m.get(\"brand\") == brand or m.get(\"type\") in [\"profile\", \"agent_communication\"]]\n",
        "\n",
        "    def get_brand_reasoning_history(self, brand: str, cross_brand: bool = False) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Retrieves reasoning history with brand filtering.\n",
        "\n",
        "        Args:\n",
        "            brand: The brand to get history for\n",
        "            cross_brand: If True, includes all brand reasonings (for bidding context)\n",
        "\n",
        "        Returns:\n",
        "            List of reasoning entries\n",
        "        \"\"\"\n",
        "        if cross_brand:\n",
        "            return self.short_term_memory\n",
        "        else:\n",
        "            return [m for m in self.short_term_memory if m.get(\"brand\") == brand]\n",
        "\n",
        "    def get_episodic_memories(self, brand: str = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Fetches Episodic Memories, optionally filtered by brand\"\"\"\n",
        "        if brand:\n",
        "            return [m for m in self.episodic_memory if m.get(\"brand\") == brand]\n",
        "        return self.episodic_memory\n",
        "\n",
        "    def get_cot_history(self) -> List[ThoughtNode]:\n",
        "        \"\"\"Returns the chain-of-thought history\"\"\"\n",
        "        return self.cot_history\n",
        "\n",
        "    def get_survey_rating(self, brand: str, question_code: str) -> Optional[int]:\n",
        "        \"\"\"Obtains a specific survey rating\"\"\"\n",
        "        return self._survey_ratings.get(brand, {}).get(question_code)\n",
        "\n",
        "    def retrieve_long_term(self) -> Dict[str, Any]:\n",
        "        \"\"\"Compatibility method - returns profile and reflections\"\"\"\n",
        "        return {\n",
        "            \"profile_info\": self.profile_data,\n",
        "            \"reflective_summary\": self.profile_reflection,\n",
        "            \"actual_self_reflection\": self.actual_self_reflection,\n",
        "            \"ideal_self_reflection\": self.ideal_self_reflection\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Enhanced Memory Module successfully defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YGzIaUuYvdh"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 2: Tool Orchestrators\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class BrandPersonalityToolOrchestrator:\n",
        "    \"\"\"Tool orchestrator for brand personality research with critical Big Five assessment\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.name = \"BrandPersonalityToolOrchestrator\"\n",
        "        # Big Five Personality Model from HuggingFace\n",
        "        self.big_five_model_name = \"Minej/bert-base-personality\"\n",
        "        print(\"   üß† Loading Big Five Personality Model...\")\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.big_five_model_name)\n",
        "        self.big_five_model = BertForSequenceClassification.from_pretrained(self.big_five_model_name)\n",
        "        self.cached_results = {}  # Cache for Brand Personalities\n",
        "\n",
        "        # Initialize Sentence Transformer for semantic comparisons\n",
        "        print(\"   üîç Loading Sentence Transformer for semantic trait comparisons...\")\n",
        "        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        print(\"‚úÖ Brand Personality Tool Orchestrator initialized with critical Big Five assessment\")\n",
        "\n",
        "    def get_brand_personality(self, brand: str, client, model: str) -> Dict[str, Any]:\n",
        "        \"\"\"Researches and creates a critical brand personality profile with Big Five\"\"\"\n",
        "\n",
        "        # Check Cache\n",
        "        if brand in self.cached_results:\n",
        "            print(f\"   üìã Using cached Brand Personality for {brand}\")\n",
        "            return self.cached_results[brand]\n",
        "\n",
        "        print(f\"   üîç Researching Brand Personality for {brand}...\")\n",
        "\n",
        "        # Critical brand personality description accounting for negative aspects\n",
        "        system_msg = (\n",
        "            f\"You are a critical brand psychology expert analyzing '{brand}'. \"\n",
        "            \"Your analysis must be balanced and realistic, not promotional. \"\n",
        "            \"Structure your response as follows:\\n\\n\"\n",
        "            \"1. POSITIVE TRAITS: Key positive characteristics and values (3-4 traits)\\n\"\n",
        "            \"2. NEGATIVE ASPECTS: Critical weaknesses, limitations, or negative associations (3-4 traits)\\n\"\n",
        "            \"3. CONTROVERSIAL ELEMENTS: Aspects that divide opinion (2-3 points)\\n\"\n",
        "            \"4. TARGET DEMOGRAPHIC: Who this brand appeals to and who it alienates\\n\\n\"\n",
        "            \"Be specific and critical. Consider:\\n\"\n",
        "            \"- Pricing and exclusivity issues\\n\"\n",
        "            \"- Environmental or ethical concerns\\n\"\n",
        "            \"- Cultural criticisms\\n\"\n",
        "            \"- Quality vs. marketing perception gaps\\n\"\n",
        "            \"- Negative consumer experiences\"\n",
        "        )\n",
        "\n",
        "        user_msg = f\"Provide a critical, balanced personality analysis of {brand}:\"\n",
        "\n",
        "        resp = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_msg},\n",
        "                {\"role\": \"user\", \"content\": user_msg}\n",
        "            ],\n",
        "            temperature=0.4,\n",
        "            max_tokens=800\n",
        "        )\n",
        "\n",
        "        brand_description = resp.choices[0].message.content.strip()\n",
        "        print(f\"   üìù Brand Description generated ({len(brand_description)} characters)\")\n",
        "\n",
        "        # Critical Big Five assessment with penalty for negative aspects\n",
        "        brand_big_five = self.assess_big_five_critical(brand_description, brand)\n",
        "        print(f\"   üéØ Big Five Scores for {brand}:\")\n",
        "        for trait, score in brand_big_five.items():\n",
        "            print(f\"      - {trait}: {score:.1f}\")\n",
        "\n",
        "        # Extract negative aspects for later use\n",
        "        negative_aspects = self._extract_negative_aspects(brand_description)\n",
        "\n",
        "        # Create Brand Profile\n",
        "        brand_profile = {\n",
        "            \"description\": brand_description,\n",
        "            \"big_five\": brand_big_five,\n",
        "            \"negative_aspects\": negative_aspects,\n",
        "            \"analyzed_at\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Cache the result\n",
        "        self.cached_results[brand] = brand_profile\n",
        "        print(f\"   ‚úÖ Brand Personality for {brand} created and cached\")\n",
        "\n",
        "        return brand_profile\n",
        "\n",
        "    def assess_big_five_critical(self, text: str, brand: str) -> Dict[str, float]:\n",
        "        \"\"\"Performs critical Big Five assessment with adjustments\"\"\"\n",
        "        # Standard Big Five Assessment\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.big_five_model(**inputs)\n",
        "            logits = outputs.logits.squeeze().detach().numpy()\n",
        "\n",
        "        labels = ['Extroversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness']\n",
        "\n",
        "        # Base scores\n",
        "        scores = {}\n",
        "        for i, label in enumerate(labels):\n",
        "            score = 1 / (1 + np.exp(-logits[i])) * 100\n",
        "            scores[label] = float(score)\n",
        "\n",
        "        # CRITICAL ADJUSTMENTS\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Negative keywords\n",
        "        negative_indicators = {\n",
        "            'Agreeableness': ['controversial', 'divisive', 'criticism', 'unethical', 'exploitative'],\n",
        "            'Conscientiousness': ['unreliable', 'quality issues', 'inconsistent', 'shortcuts'],\n",
        "            'Neuroticism': ['unstable', 'volatile', 'unpredictable', 'crisis'],\n",
        "            'Openness': ['traditional', 'conservative', 'closed-minded', 'rigid']\n",
        "        }\n",
        "\n",
        "        # Apply penalties for negative aspects\n",
        "        for trait, keywords in negative_indicators.items():\n",
        "            penalty = sum(5 for keyword in keywords if keyword in text_lower)\n",
        "            if trait in scores:\n",
        "                scores[trait] = max(10, scores[trait] - penalty)\n",
        "\n",
        "        # No brand-specific adjustments\n",
        "        brand_adjustments = {}\n",
        "\n",
        "        if brand in brand_adjustments:\n",
        "            for trait, adjustment in brand_adjustments[brand].items():\n",
        "                if trait in scores:\n",
        "                    scores[trait] = max(0, min(100, scores[trait] + adjustment))\n",
        "\n",
        "        # Round final scores\n",
        "        return {trait: round(score, 2) for trait, score in scores.items()}\n",
        "\n",
        "    def _extract_negative_aspects(self, description: str) -> List[str]:\n",
        "        \"\"\"Extracts negative aspects from the brand description\"\"\"\n",
        "        negative_aspects = []\n",
        "\n",
        "        # Search for sections with negative aspects\n",
        "        lines = description.split('\\n')\n",
        "        capture = False\n",
        "\n",
        "        for line in lines:\n",
        "            if any(keyword in line.upper() for keyword in ['NEGATIVE', 'CONTROVERSIAL', 'WEAKNESS', 'LIMITATION']):\n",
        "                capture = True\n",
        "            elif any(keyword in line.upper() for keyword in ['POSITIVE', 'TARGET']) and capture:\n",
        "                capture = False\n",
        "            elif capture and line.strip():\n",
        "                negative_aspects.append(line.strip())\n",
        "\n",
        "        return negative_aspects\n",
        "\n",
        "    def _calculate_trait_similarity(self, trait1: str, trait2: str) -> float:\n",
        "        \"\"\"Calculates semantic similarity between two traits\"\"\"\n",
        "        if not trait1 or not trait2:\n",
        "            return 0.0\n",
        "\n",
        "        emb1 = self.semantic_model.encode(str(trait1).lower())\n",
        "        emb2 = self.semantic_model.encode(str(trait2).lower())\n",
        "\n",
        "        # Cosine similarity\n",
        "        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "        return float(similarity)\n",
        "\n",
        "    def _analyze_primary_trait_emphasis(self, primary_trait: str, brand_description: str) -> float:\n",
        "        \"\"\"Analyzes how strongly the primary trait is emphasized in the brand description\"\"\"\n",
        "        # Divide description into sentences\n",
        "        sentences = brand_description.split('.')\n",
        "\n",
        "        # Search for semantically similar terms to the primary trait\n",
        "        emphasis_score = 0.0\n",
        "        trait_embedding = self.semantic_model.encode(primary_trait.lower())\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Extract important words from the sentence\n",
        "            words = [word.strip().lower() for word in sentence.split() if len(word) > 3]\n",
        "\n",
        "            for word in words:\n",
        "                try:\n",
        "                    word_embedding = self.semantic_model.encode(word)\n",
        "                    similarity = np.dot(trait_embedding, word_embedding) / (\n",
        "                        np.linalg.norm(trait_embedding) * np.linalg.norm(word_embedding)\n",
        "                    )\n",
        "\n",
        "                    # High similarity indicates emphasis on the trait\n",
        "                    if similarity > 0.6:  # Threshold for semantic similarity\n",
        "                        emphasis_score += similarity\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        return min(1.0, emphasis_score)  # Normalize to 0-1\n",
        "\n",
        "    def _analyze_negative_trait_conflicts(self, user_traits: List[str], negative_aspects: List[str]) -> float:\n",
        "        \"\"\"Analyzes conflicts between user traits and negative brand aspects\"\"\"\n",
        "        if not user_traits or not negative_aspects:\n",
        "            return 0.0\n",
        "\n",
        "        conflict_score = 0.0\n",
        "\n",
        "        for user_trait in user_traits:\n",
        "            user_embedding = self.semantic_model.encode(user_trait.lower())\n",
        "\n",
        "            for negative_aspect in negative_aspects:\n",
        "                # Extract keywords from negative aspects\n",
        "                negative_words = [word.strip().lower() for word in negative_aspect.split()\n",
        "                                if len(word) > 3 and word.lower() not in ['brand', 'company', 'product']]\n",
        "\n",
        "                for neg_word in negative_words:\n",
        "                    try:\n",
        "                        neg_embedding = self.semantic_model.encode(neg_word)\n",
        "\n",
        "                        # Check for semantic proximity (conflict)\n",
        "                        similarity = np.dot(user_embedding, neg_embedding) / (\n",
        "                            np.linalg.norm(user_embedding) * np.linalg.norm(neg_embedding)\n",
        "                        )\n",
        "\n",
        "                        # High similarity between user trait and negative aspect = conflict\n",
        "                        if similarity > 0.5:\n",
        "                            conflict_score += similarity\n",
        "\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "        return min(1.0, conflict_score)  # Normalize to 0-1\n",
        "\n",
        "    def compare_with_user_reflection(self, brand: str, reflection_type: str, reflection_text: str, client, model: str) -> Dict[str, Any]:\n",
        "        \"\"\"Compares Brand Personality with User Reflection using the critical Big Five analysis\"\"\"\n",
        "\n",
        "        # Get Brand Personality (from cache or generate new)\n",
        "        brand_profile = self.get_brand_personality(brand, client, model)\n",
        "\n",
        "        print(f\"   ü§ù Comparing {reflection_type} with {brand} Personality...\")\n",
        "\n",
        "        # Critical Big Five assessment with penalty for negative aspects\n",
        "        user_big_five = self.assess_user_big_five_critical(reflection_text, reflection_type)\n",
        "        print(f\"   üë§ Critical User Big Five Scores ({reflection_type}):\")\n",
        "        for trait, score in user_big_five.items():\n",
        "            print(f\"      - {trait}: {score:.1f}\")\n",
        "\n",
        "        comparison = self.compare_big_five_profiles_critical(user_big_five, brand_profile[\"big_five\"])\n",
        "\n",
        "        print(f\"   üìä Personality Match Score: {comparison['match_score']:.1f}% ({comparison['interpretation']})\")\n",
        "        print(f\"   üìà Biggest differences:\")\n",
        "        sorted_diffs = sorted(comparison['differences'].items(), key=lambda x: x[1], reverse=True)\n",
        "        for trait, diff in sorted_diffs[:3]:\n",
        "            print(f\"      - {trait}: {diff:.1f} Points difference\")\n",
        "\n",
        "        return {\n",
        "            \"brand_personality\": brand_profile,\n",
        "            \"user_comparison\": {\n",
        "                \"comparison_type\": reflection_type,\n",
        "                \"user_big_five\": user_big_five,\n",
        "                \"comparison\": comparison,\n",
        "                \"reflection_based\": True,\n",
        "                \"negative_aspects_considered\": len(brand_profile.get(\"negative_aspects\", []))\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def compare_with_user_profile_enhanced(self, brand: str, reflection_type: str, reflection_text: str,\n",
        "                                         user_profile: dict, client, model: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extended comparison method with Primary Trait Analysis and Conflict Detection\"\"\"\n",
        "\n",
        "        # Get Brand Personality\n",
        "        brand_profile = self.get_brand_personality(brand, client, model)\n",
        "\n",
        "        print(f\"   ü§ù Advanced analysis {reflection_type} with {brand} Personality...\")\n",
        "\n",
        "        # Big Five Assessment\n",
        "        user_big_five = self.assess_user_big_five_critical(reflection_text, reflection_type)\n",
        "\n",
        "        # Advanced analysis with Primary Traits\n",
        "        if reflection_type == \"actual_self\":\n",
        "            primary_trait = user_profile.get('actual_traits', [''])[0] if user_profile.get('actual_traits') else ''\n",
        "            user_traits = user_profile.get('actual_traits', [])\n",
        "        else:  # ideal_self\n",
        "            primary_trait = user_profile.get('ideal_traits', [''])[0] if user_profile.get('ideal_traits') else ''\n",
        "            user_traits = user_profile.get('ideal_traits', [])\n",
        "\n",
        "        # Analyze Primary Trait Emphasis in Brand\n",
        "        primary_trait_emphasis = 0.0\n",
        "        if primary_trait:\n",
        "            primary_trait_emphasis = self._analyze_primary_trait_emphasis(\n",
        "                primary_trait, brand_profile[\"description\"]\n",
        "            )\n",
        "            print(f\"   üéØ Primary Trait '{primary_trait}' Emphasis in {brand}: {primary_trait_emphasis:.2f}\")\n",
        "\n",
        "        # Analyze conflicts with negative aspects\n",
        "        conflict_score = self._analyze_negative_trait_conflicts(\n",
        "            user_traits, brand_profile.get(\"negative_aspects\", [])\n",
        "        )\n",
        "        if conflict_score > 0:\n",
        "            print(f\"   ‚ö†Ô∏è Trait conflict score: {conflict_score:.2f}\")\n",
        "\n",
        "        # Standard comparison\n",
        "        comparison = self.compare_big_five_profiles_critical(user_big_five, brand_profile[\"big_five\"])\n",
        "\n",
        "        # Advanced match score calculation\n",
        "        enhanced_match_score = comparison['match_score']\n",
        "\n",
        "        # Primary Trait Penalty: When the most important trait is not emphasized in Brand personality\n",
        "        if primary_trait and primary_trait_emphasis < 0.3:  # Weak emphasis\n",
        "            primary_trait_penalty = (0.3 - primary_trait_emphasis) * 40  # penalty\n",
        "            enhanced_match_score = max(0, enhanced_match_score - primary_trait_penalty)\n",
        "            print(f\"   üìâ Primary Trait Penalty: -{primary_trait_penalty:.1f} points\")\n",
        "\n",
        "        # Conflict Penalty: Conflicts between user traits and negative brand aspects\n",
        "        if conflict_score > 0.2:  # Significant conflict\n",
        "            conflict_penalty = conflict_score * 15  # penalty\n",
        "            enhanced_match_score = max(0, enhanced_match_score - conflict_penalty)\n",
        "            print(f\"   üìâ Conflict Penalty: -{conflict_penalty:.1f} points\")\n",
        "\n",
        "        # Update comparison with enhanced score\n",
        "        enhanced_comparison = comparison.copy()\n",
        "        enhanced_comparison['match_score'] = round(enhanced_match_score, 2)\n",
        "        enhanced_comparison['interpretation'] = self._interpret_match_score_critical(enhanced_match_score)\n",
        "        enhanced_comparison['primary_trait_emphasis'] = primary_trait_emphasis\n",
        "        enhanced_comparison['conflict_score'] = conflict_score\n",
        "\n",
        "        print(f\"   üìä Enhanced Match Score: {enhanced_match_score:.1f}% ({enhanced_comparison['interpretation']})\")\n",
        "\n",
        "        return {\n",
        "            \"brand_personality\": brand_profile,\n",
        "            \"user_comparison\": {\n",
        "                \"comparison_type\": reflection_type,\n",
        "                \"user_big_five\": user_big_five,\n",
        "                \"comparison\": enhanced_comparison,\n",
        "                \"reflection_based\": True,\n",
        "                \"primary_trait\": primary_trait,\n",
        "                \"primary_trait_emphasis\": primary_trait_emphasis,\n",
        "                \"conflict_score\": conflict_score,\n",
        "                \"negative_aspects_considered\": len(brand_profile.get(\"negative_aspects\", []))\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def assess_user_big_five_critical(self, text: str, reflection_type: str) -> Dict[str, float]:\n",
        "        \"\"\"Critical Big Five assessment for user reflections\"\"\"\n",
        "        # Standard Assessment\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.big_five_model(**inputs)\n",
        "            logits = outputs.logits.squeeze().detach().numpy()\n",
        "\n",
        "        labels = ['Extroversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness']\n",
        "\n",
        "        scores = {}\n",
        "        for i, label in enumerate(labels):\n",
        "            score = 1 / (1 + np.exp(-logits[i])) * 100\n",
        "\n",
        "            # Critical normalization for user scores\n",
        "            normalized_score = 20 + (score * 0.8)\n",
        "\n",
        "            scores[label] = float(normalized_score)\n",
        "\n",
        "        return {trait: round(score, 2) for trait, score in scores.items()}\n",
        "\n",
        "    def compare_big_five_profiles_critical(self, profile1: Dict[str, float], profile2: Dict[str, float]) -> Dict[str, Any]:\n",
        "        \"\"\"Critical comparison of Big Five profiles\"\"\"\n",
        "        differences = {}\n",
        "        weighted_difference = 0\n",
        "\n",
        "        # Weights for the big five traits - different relative importance for the relationship between brand and people\n",
        "        trait_weights = {\n",
        "            'Agreeableness': 0.8,\n",
        "            'Conscientiousness': 1.7,\n",
        "            'Openness': 0.6,\n",
        "            'Extroversion': 1.9,\n",
        "            'Neuroticism': 1.1\n",
        "        }\n",
        "\n",
        "        for trait in profile1:\n",
        "            diff = abs(profile1[trait] - profile2[trait])\n",
        "            differences[trait] = round(diff, 2)\n",
        "\n",
        "            # Weighted difference\n",
        "            weight = trait_weights.get(trait, 1.0)\n",
        "            weighted_difference += diff * weight\n",
        "\n",
        "        # Average weighted deviation\n",
        "        total_weight = sum(trait_weights.values())\n",
        "        avg_weighted_difference = weighted_difference / total_weight\n",
        "\n",
        "        # Large differences are penalized disproportionately\n",
        "        match_score = max(0, 100 - (avg_weighted_difference * 1.5) ** 1.2)\n",
        "\n",
        "        return {\n",
        "            \"differences\": differences,\n",
        "            \"average_difference\": round(avg_weighted_difference, 2),\n",
        "            \"match_score\": round(match_score, 2),\n",
        "            \"interpretation\": self._interpret_match_score_critical(match_score)\n",
        "        }\n",
        "\n",
        "    def _interpret_match_score_critical(self, score: float) -> str:\n",
        "        \"\"\"Interpretation of the match score\"\"\"\n",
        "        if score >= 85:\n",
        "            return \"Good personality match\"\n",
        "        elif score >= 70:\n",
        "            return \"Moderate personality match\"\n",
        "        elif score >= 50:\n",
        "            return \"Weak personality match\"\n",
        "        elif score >= 30:\n",
        "            return \"Poor personality match\"\n",
        "        else:\n",
        "            return \"Very poor personality match\"\n",
        "\n",
        "    def create_challenge_message(self, evaluation: dict, initial_rating: int) -> Optional[AgentMessage]:\n",
        "        \"\"\"Creates challenge message\"\"\"\n",
        "        match_score = evaluation.get(\"user_comparison\", {}).get(\"comparison\", {}).get(\"match_score\", 50)\n",
        "        negative_aspects = evaluation.get(\"user_comparison\", {}).get(\"negative_aspects_considered\", 0)\n",
        "\n",
        "        # Challenge criteria\n",
        "        if match_score < 50 and initial_rating >= 4:\n",
        "            return AgentMessage(\n",
        "                from_agent=self.name,\n",
        "                to_agent=\"MainAgent\",\n",
        "                message_type=MessageType.CHALLENGE,\n",
        "                content={\n",
        "                    \"summary\": \"Poor brand congruence detected - rating seems too high\",\n",
        "                    \"match_score\": match_score,\n",
        "                    \"suggestion\": \"The brand personality significantly differs from yours. Consider lowering your rating.\",\n",
        "                    \"reasoning\": f\"Critical analysis shows only {match_score:.1f}% match with {negative_aspects} negative aspects\",\n",
        "                    \"severity\": \"high\"\n",
        "                },\n",
        "                priority=3  # Higher priority for poor matches\n",
        "            )\n",
        "        elif match_score < 70 and initial_rating >= 4:\n",
        "            return AgentMessage(\n",
        "                from_agent=self.name,\n",
        "                to_agent=\"MainAgent\",\n",
        "                message_type=MessageType.CHALLENGE,\n",
        "                content={\n",
        "                    \"summary\": \"Weak brand congruence - high rating questionable\",\n",
        "                    \"match_score\": match_score,\n",
        "                    \"suggestion\": \"Consider if this moderate alignment justifies such a high rating.\",\n",
        "                    \"reasoning\": f\"Analysis shows {match_score:.1f}% personality match\",\n",
        "                    \"severity\": \"medium\"\n",
        "                },\n",
        "                priority=2\n",
        "            )\n",
        "        elif match_score > 84 and initial_rating <= 2:\n",
        "            return AgentMessage(\n",
        "                from_agent=self.name,\n",
        "                to_agent=\"MainAgent\",\n",
        "                message_type=MessageType.CHALLENGE,\n",
        "                content={\n",
        "                    \"summary\": \"High brand congruence despite low rating\",\n",
        "                    \"match_score\": match_score,\n",
        "                    \"suggestion\": \"Your personality aligns well with the brand - the low rating may not reflect this.\",\n",
        "                    \"reasoning\": f\"Analysis shows strong {match_score:.1f}% personality match\",\n",
        "                    \"severity\": \"low\"\n",
        "                },\n",
        "                priority=2\n",
        "            )\n",
        "        return None\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ENHANCED FEEDBACK TOOL ORCHESTRATOR WITH SEMANTIC SIMILARITY\n",
        "# =============================================================================\n",
        "\n",
        "class FeedbackToolOrchestrator:\n",
        "    \"\"\"Enhanced Tool Orchestrator for feedback based on human training data with semantic similarity\"\"\"\n",
        "\n",
        "    def __init__(self, training_data_path: str = None):\n",
        "        self.name = \"FeedbackToolOrchestrator\"\n",
        "        self.training_data_path = training_data_path or '/content/drive/MyDrive/profiles_survey/Training_survey_data.xlsx'\n",
        "\n",
        "        # Initialize Sentence Transformer for semantic similarity\n",
        "        print(\"   üß† Loading Sentence Transformer for semantic similarity...\")\n",
        "        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Cache for Trait-Embeddings\n",
        "        self.trait_embeddings_cache = {}\n",
        "\n",
        "        # One-time loading of training data\n",
        "        try:\n",
        "            print(f\"   üìä Loading training data from {self.training_data_path}\")\n",
        "            self.training_data = pd.read_excel(self.training_data_path)\n",
        "            print(f\"   ‚úÖ {len(self.training_data)} training examples loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Warning: Could not load training data: {e}\")\n",
        "            self.training_data = pd.DataFrame()  # Empty DataFrame as Fallback\n",
        "\n",
        "        # Initialize column_mapping\n",
        "        self.column_mapping = {\n",
        "            'nike_actual_consistent': 'Nike_Q9_consistent',\n",
        "            'nike_actual_mirror': 'Nike_Q9_mirror',\n",
        "            'nike_ideal_consistent': 'Nike_Q10_consistent',\n",
        "            'nike_ideal_mirror': 'Nike_Q10_mirror',\n",
        "            'nike_affection': 'Nike_Q11_Affection',\n",
        "            'nike_love': 'Nike_Q11_Love',\n",
        "            'nike_connection': 'Nike_Q11_Connection',\n",
        "            'nike_passion': 'Nike_Q11_Passion',\n",
        "            'nike_delight': 'Nike_Q11_Delight',\n",
        "            'nike_captivation': 'Nike_Q11_Captivation',\n",
        "            'apple_actual_consistent': 'Apple_Q9_consistent',\n",
        "            'apple_actual_mirror': 'Apple_Q9_mirror',\n",
        "            'apple_ideal_consistent': 'Apple_Q10_consistent',\n",
        "            'apple_ideal_mirror': 'Apple_Q10_mirror',\n",
        "            'apple_affection': 'Apple_Q11_Affection',\n",
        "            'apple_love': 'Apple_Q11_Love',\n",
        "            'apple_connection': 'Apple_Q11_Connection',\n",
        "            'apple_passion': 'Apple_Q11_Passion',\n",
        "            'apple_delight': 'Apple_Q11_Delight',\n",
        "            'apple_captivation': 'Apple_Q11_Captivation',\n",
        "            'levis_actual_consistent': \"Levi's_Q9_consistent\",\n",
        "            'levis_actual_mirror': \"Levi's_Q9_mirror\",\n",
        "            'levis_ideal_consistent': \"Levi's_Q10_consistent\",\n",
        "            'levis_ideal_mirror': \"Levi's_Q10_mirror\",\n",
        "            'levis_affection': \"Levi's_Q11_Affection\",\n",
        "            'levis_love': \"Levi's_Q11_Love\",\n",
        "            'levis_connection': \"Levi's_Q11_Connection\",\n",
        "            'levis_passion': \"Levi's_Q11_Passion\",\n",
        "            'levis_delight': \"Levi's_Q11_Delight\",\n",
        "            'levis_captivation': \"Levi's_Q11_Captivation\",\n",
        "            'bid Nike': 'bid_Nike',\n",
        "            'bid Apple': 'bid_Apple',\n",
        "            'bid Levis': \"bid_Levi's\"\n",
        "        }\n",
        "\n",
        "\n",
        "    def load_training_data(self, path: str = None):\n",
        "        \"\"\"\n",
        "        Loads training data and column_mapping\n",
        "        \"\"\"\n",
        "        self.column_mapping = {\n",
        "            'nike_actual_consistent': 'Nike_Q9_consistent',\n",
        "            'nike_actual_mirror': 'Nike_Q9_mirror',\n",
        "            'nike_ideal_consistent': 'Nike_Q10_consistent',\n",
        "            'nike_ideal_mirror': 'Nike_Q10_mirror',\n",
        "            'nike_affection': 'Nike_Q11_Affection',\n",
        "            'nike_love': 'Nike_Q11_Love',\n",
        "            'nike_connection': 'Nike_Q11_Connection',\n",
        "            'nike_passion': 'Nike_Q11_Passion',\n",
        "            'nike_delight': 'Nike_Q11_Delight',\n",
        "            'nike_captivation': 'Nike_Q11_Captivation',\n",
        "            'apple_actual_consistent': 'Apple_Q9_consistent',\n",
        "            'apple_actual_mirror': 'Apple_Q9_mirror',\n",
        "            'apple_ideal_consistent': 'Apple_Q10_consistent',\n",
        "            'apple_ideal_mirror': 'Apple_Q10_mirror',\n",
        "            'apple_affection': 'Apple_Q11_Affection',\n",
        "            'apple_love': 'Apple_Q11_Love',\n",
        "            'apple_connection': 'Apple_Q11_Connection',\n",
        "            'apple_passion': 'Apple_Q11_Passion',\n",
        "            'apple_delight': 'Apple_Q11_Delight',\n",
        "            'apple_captivation': 'Apple_Q11_Captivation',\n",
        "            'levis_actual_consistent': \"Levi's_Q9_consistent\",\n",
        "            'levis_actual_mirror': \"Levi's_Q9_mirror\",\n",
        "            'levis_ideal_consistent': \"Levi's_Q10_consistent\",\n",
        "            'levis_ideal_mirror': \"Levi's_Q10_mirror\",\n",
        "            'levis_affection': \"Levi's_Q11_Affection\",\n",
        "            'levis_love': \"Levi's_Q11_Love\",\n",
        "            'levis_connection': \"Levi's_Q11_Connection\",\n",
        "            'levis_passion': \"Levi's_Q11_Passion\",\n",
        "            'levis_delight': \"Levi's_Q11_Delight\",\n",
        "            'levis_captivation': \"Levi's_Q11_Captivation\",\n",
        "            'bid Nike': 'bid_Nike',\n",
        "            'bid Apple': 'bid_Apple',\n",
        "            'bid Levis': \"bid_Levi's\"\n",
        "        }\n",
        "\n",
        "    def _get_trait_embedding(self, trait: str) -> np.ndarray:\n",
        "        \"\"\"Get or calculate embedding for a trait\"\"\"\n",
        "        if trait not in self.trait_embeddings_cache:\n",
        "            self.trait_embeddings_cache[trait] = self.semantic_model.encode(trait)\n",
        "        return self.trait_embeddings_cache[trait]\n",
        "\n",
        "    def _calculate_trait_similarity(self, trait1: str, trait2: str) -> float:\n",
        "        \"\"\"Calculates semantic similarity between two traits\"\"\"\n",
        "        if pd.isna(trait1) or pd.isna(trait2) or str(trait1).strip() == '' or str(trait2).strip() == '':\n",
        "            return 0.0\n",
        "\n",
        "        emb1 = self._get_trait_embedding(str(trait1).lower())\n",
        "        emb2 = self._get_trait_embedding(str(trait2).lower())\n",
        "\n",
        "        # Cosine similarity\n",
        "        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "        return float(similarity)\n",
        "\n",
        "    def find_best_human_matches(self, profile: dict, n_matches: int = 3) -> List[Tuple[pd.Series, float]]:\n",
        "        \"\"\"Finds the n best human matches based on extended profile with semantic similarity\"\"\"\n",
        "        if self.training_data is None:\n",
        "            self.load_training_data()\n",
        "\n",
        "        # Calculate similarity score for already covered human counterparts in training data\n",
        "        match_scores = []\n",
        "\n",
        "        for idx, human in self.training_data.iterrows():\n",
        "            score = 0\n",
        "            weights = 0\n",
        "\n",
        "            # DEMOGRAPHIC DATA (weighted)\n",
        "            # Age\n",
        "            if 'age' in human and pd.notna(human['age']):\n",
        "                age_diff = abs(profile['age'] - human['age'])\n",
        "                age_score = max(0, 100 - age_diff)\n",
        "                score += age_score * 2\n",
        "                weights += 1\n",
        "\n",
        "            # Gender\n",
        "            if 'gender' in human and pd.notna(human['gender']):\n",
        "                if profile['gender'].lower() == str(human['gender']).lower():\n",
        "                    score += 100 * 1\n",
        "                weights += 1\n",
        "\n",
        "            # Occupation\n",
        "            if 'occupation' in human and pd.notna(human['occupation']):\n",
        "                if profile['occupation'].lower() in str(human['occupation']).lower() or \\\n",
        "                   str(human['occupation']).lower() in profile['occupation'].lower():\n",
        "                    score += 100 * 1\n",
        "                weights += 1\n",
        "\n",
        "            # Income\n",
        "            if 'income' in human and pd.notna(human['income']):\n",
        "                if profile['income_range'].lower() in str(human['income']).lower() or \\\n",
        "                   str(human['income']).lower() in profile['income_range'].lower():\n",
        "                    score += 100 * 2\n",
        "                weights += 1\n",
        "\n",
        "            # ACTUAL TRAITS (weighted in descending order, as the first characteristic listed best describes the human role counterparts according to the survey)\n",
        "            trait_weights = [3.0, 2.5, 2.0, 1.5]  # First traits more important\n",
        "            for i in range(4):\n",
        "                if f'actual_{i+1}' in human and i < len(profile.get('actual_traits', [])):\n",
        "                    similarity = self._calculate_trait_similarity(\n",
        "                        profile['actual_traits'][i],\n",
        "                        human[f'actual_{i+1}']\n",
        "                    )\n",
        "                    score += similarity * 100 * trait_weights[i]\n",
        "                    weights += trait_weights[i]\n",
        "\n",
        "            # IDEAL TRAITS (weighted in descending order, as the first characteristic listed best describes the human role counterparts according to the survey)\n",
        "            for i in range(4):\n",
        "                if f'ideal_{i+1}' in human and i < len(profile.get('ideal_traits', [])):\n",
        "                    similarity = self._calculate_trait_similarity(\n",
        "                        profile['ideal_traits'][i],\n",
        "                        human[f'ideal_{i+1}']\n",
        "                    )\n",
        "                    score += similarity * 100 * trait_weights[i]\n",
        "                    weights += trait_weights[i]\n",
        "\n",
        "            # TOP STRENGTHS\n",
        "            strength_weight = 3.0\n",
        "            if 'actual_top_strength' in human and pd.notna(human['actual_top_strength']):\n",
        "                similarity = self._calculate_trait_similarity(\n",
        "                    profile.get('actual_top_strength', ''),\n",
        "                    human['actual_top_strength']\n",
        "                )\n",
        "                score += similarity * 100 * strength_weight\n",
        "                weights += strength_weight\n",
        "\n",
        "            if 'ideal_top_strength' in human and pd.notna(human['ideal_top_strength']):\n",
        "                similarity = self._calculate_trait_similarity(\n",
        "                    profile.get('ideal_top_strength', ''),\n",
        "                    human['ideal_top_strength']\n",
        "                )\n",
        "                score += similarity * 100 * strength_weight\n",
        "                weights += strength_weight\n",
        "\n",
        "            final_score = score / weights if weights > 0 else 0\n",
        "            match_scores.append((idx, final_score))\n",
        "\n",
        "        # Sort by score and take the best n matches\n",
        "        match_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Get the actual number of available matches\n",
        "        available_matches = min(n_matches, len(match_scores))\n",
        "\n",
        "        if available_matches == 0:\n",
        "            print(f\"   ‚ö†Ô∏è No human matches available\")\n",
        "            return []\n",
        "\n",
        "        best_matches = []\n",
        "        for i in range(available_matches):\n",
        "            idx, score = match_scores[i]\n",
        "            best_matches.append((self.training_data.iloc[idx], score))\n",
        "\n",
        "        print(f\"   üéØ {available_matches} best Human Matches found\")\n",
        "\n",
        "        return best_matches\n",
        "\n",
        "    def calculate_feedback(self, agent_responses: dict, human_data: pd.Series, brand: str) -> dict:\n",
        "        \"\"\"Calculates feedback for a brand\"\"\"\n",
        "        feedback = {\n",
        "            \"survey_feedback\": {},\n",
        "            \"bidding_feedback\": {}\n",
        "        }\n",
        "\n",
        "        # Survey Feedback\n",
        "        for orig_col, mapped_col in self.column_mapping.items():\n",
        "            if brand in mapped_col:\n",
        "                human_value = human_data.get(orig_col, -77)\n",
        "\n",
        "                # Skip -77\n",
        "                if human_value == -77 or pd.isna(human_value):\n",
        "                    continue\n",
        "\n",
        "                # Extract Question Code\n",
        "                question_code = mapped_col.split('_', 1)[1]\n",
        "\n",
        "                if question_code in agent_responses:\n",
        "                    agent_value = agent_responses[question_code]\n",
        "                    diff = human_value - agent_value\n",
        "\n",
        "                    feedback[\"survey_feedback\"][mapped_col] = {\n",
        "                        \"human\": int(human_value),\n",
        "                        \"agent\": int(agent_value),\n",
        "                        \"diff\": diff\n",
        "                    }\n",
        "\n",
        "        # Bidding Feedback\n",
        "        bid_col = f\"bid_{brand}\" if brand != \"Levi's\" else \"bid_Levi's\"\n",
        "        if bid_col in self.column_mapping.values():\n",
        "            orig_bid_col = next(k for k, v in self.column_mapping.items() if v == bid_col)\n",
        "            human_bid = human_data.get(orig_bid_col, -77)\n",
        "\n",
        "            if human_bid != -77 and pd.notna(human_bid) and f\"bid_{brand}\" in agent_responses:\n",
        "                agent_bid = agent_responses[f\"bid_{brand}\"]\n",
        "                diff = human_bid - agent_bid\n",
        "\n",
        "                feedback[\"bidding_feedback\"][brand] = {\n",
        "                    \"human\": float(human_bid),\n",
        "                    \"agent\": float(agent_bid),\n",
        "                    \"diff\": diff\n",
        "                }\n",
        "\n",
        "        return feedback\n",
        "\n",
        "    def create_feedback_message(self, agent_responses: dict, profile: dict, brand: str) -> AgentMessage:\n",
        "        \"\"\"Creates structured feedback message based on the 3 best matches\"\"\"\n",
        "        # Find the 3 best Human Matches\n",
        "        best_matches = self.find_best_human_matches(profile, n_matches=3)\n",
        "\n",
        "        if not best_matches:\n",
        "            # No matches found\n",
        "            return AgentMessage(\n",
        "                from_agent=self.name,\n",
        "                to_agent=\"MainAgent\",\n",
        "                message_type=MessageType.LEARNING_FEEDBACK,\n",
        "                content={\n",
        "                    \"summary\": \"No human matches found for learning\",\n",
        "                    \"profile_match_score\": 0.0,\n",
        "                    \"brand\": brand,\n",
        "                    \"survey_feedback\": {},\n",
        "                    \"bidding_feedback\": {},\n",
        "                    \"recommendations\": {},\n",
        "                    \"matches_used\": 0\n",
        "                },\n",
        "                priority=3\n",
        "            )\n",
        "\n",
        "        # Calculate average feedback over all matches\n",
        "        aggregated_feedback = {\n",
        "            \"survey_feedback\": {},\n",
        "            \"bidding_feedback\": {}\n",
        "        }\n",
        "\n",
        "        # Collect feedback from all matches\n",
        "        all_survey_diffs = {}\n",
        "        all_bidding_diffs = {}\n",
        "\n",
        "        for human_match, match_score in best_matches:\n",
        "            single_feedback = self.calculate_feedback(agent_responses, human_match, brand)\n",
        "\n",
        "            # Aggregate Survey Feedback\n",
        "            for key, values in single_feedback[\"survey_feedback\"].items():\n",
        "                if key not in all_survey_diffs:\n",
        "                    all_survey_diffs[key] = []\n",
        "                all_survey_diffs[key].append(values[\"diff\"])\n",
        "\n",
        "            # Aggregate Bidding Feedback\n",
        "            for key, values in single_feedback[\"bidding_feedback\"].items():\n",
        "                if key not in all_bidding_diffs:\n",
        "                    all_bidding_diffs[key] = []\n",
        "                all_bidding_diffs[key].append(values[\"diff\"])\n",
        "\n",
        "        # Calculate averages\n",
        "        for key, diffs in all_survey_diffs.items():\n",
        "            avg_diff = sum(diffs) / len(diffs)\n",
        "            aggregated_feedback[\"survey_feedback\"][key] = {\n",
        "                \"human_avg\": agent_responses.get(key.split('_', 1)[1], 0) + avg_diff,\n",
        "                \"agent\": agent_responses.get(key.split('_', 1)[1], 0),\n",
        "                \"diff\": avg_diff,\n",
        "                \"matches_used\": len(diffs)\n",
        "            }\n",
        "\n",
        "        for key, diffs in all_bidding_diffs.items():\n",
        "            avg_diff = sum(diffs) / len(diffs)\n",
        "            aggregated_feedback[\"bidding_feedback\"][key] = {\n",
        "                \"human_avg\": agent_responses.get(f\"bid_{key}\", 0) + avg_diff,\n",
        "                \"agent\": agent_responses.get(f\"bid_{key}\", 0),\n",
        "                \"diff\": avg_diff,\n",
        "                \"matches_used\": len(diffs)\n",
        "            }\n",
        "\n",
        "        # Analyze patterns and create recommendations\n",
        "        survey_diffs = [f[\"diff\"] for f in aggregated_feedback[\"survey_feedback\"].values()]\n",
        "        recommendations = {}\n",
        "\n",
        "        if survey_diffs:\n",
        "            avg_survey_diff = sum(survey_diffs) / len(survey_diffs)\n",
        "            if avg_survey_diff > 0.5:\n",
        "                recommendations[\"general_tendency\"] = f\"Your ratings tend to be {abs(avg_survey_diff):.1f} points too low\"\n",
        "            elif avg_survey_diff < -0.5:\n",
        "                recommendations[\"general_tendency\"] = f\"Your ratings tend to be {abs(avg_survey_diff):.1f} points too high\"\n",
        "\n",
        "            # Specific emotion patterns\n",
        "            emotion_diffs = {k.split('_')[-1]: v[\"diff\"]\n",
        "                            for k, v in aggregated_feedback[\"survey_feedback\"].items()\n",
        "                            if \"Q11\" in k}\n",
        "            if emotion_diffs:\n",
        "                if \"Love\" in emotion_diffs and emotion_diffs[\"Love\"] > 1:\n",
        "                    recommendations[\"emotion_adjustment\"] = \"Love ratings are too conservative\"\n",
        "                elif \"Passion\" in emotion_diffs and emotion_diffs[\"Passion\"] > 1:\n",
        "                    recommendations[\"emotion_adjustment\"] = \"Passion ratings need to be higher\"\n",
        "\n",
        "        # Bidding Recommendations\n",
        "        for brand_name, bid_feedback in aggregated_feedback[\"bidding_feedback\"].items():\n",
        "            if abs(bid_feedback[\"diff\"]) > 20:\n",
        "                if bid_feedback[\"diff\"] > 0:\n",
        "                    recommendations[f\"bidding_{brand_name}\"] = f\"Bids for {brand_name} are ‚Ç¨{abs(bid_feedback['diff']):.2f} too low\"\n",
        "                else:\n",
        "                    recommendations[f\"bidding_{brand_name}\"] = f\"Bids for {brand_name} are ‚Ç¨{abs(bid_feedback['diff']):.2f} too high\"\n",
        "\n",
        "        # Calculate average match score\n",
        "        avg_match_score = sum(score for _, score in best_matches) / len(best_matches)\n",
        "\n",
        "        return AgentMessage(\n",
        "            from_agent=self.name,\n",
        "            to_agent=\"MainAgent\",\n",
        "            message_type=MessageType.LEARNING_FEEDBACK,\n",
        "            content={\n",
        "                \"summary\": f\"Learning feedback based on {len(best_matches)} matches (avg {avg_match_score:.1f}% similarity)\",\n",
        "                \"profile_match_score\": avg_match_score,\n",
        "                \"brand\": brand,\n",
        "                \"survey_feedback\": aggregated_feedback[\"survey_feedback\"],\n",
        "                \"bidding_feedback\": aggregated_feedback[\"bidding_feedback\"],\n",
        "                \"recommendations\": recommendations,\n",
        "                \"matches_used\": len(best_matches),\n",
        "                \"human_references\": [\n",
        "                    {\n",
        "                        \"age\": match.get('age', 'unknown'),\n",
        "                        \"gender\": match.get('gender', 'unknown'),\n",
        "                        \"occupation\": match.get('occupation', 'unknown'),\n",
        "                        \"match_score\": f\"{score:.1f}%\"\n",
        "                    }\n",
        "                    for match, score in best_matches\n",
        "                ]\n",
        "            },\n",
        "            priority=3  # High priority\n",
        "        )\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# PRICE RESEARCH TOOL ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "class PriceResearchToolOrchestrator:\n",
        "    \"\"\"Tool Orchestrator for price research\"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        self.name = \"PriceResearchToolOrchestrator\"\n",
        "        self.cached_prices = {}\n",
        "\n",
        "        # URLs that allow scraping\n",
        "        self.price_configs = {\n",
        "            \"Apple\": {\n",
        "                \"url\": \"https://www.apple.com/de/shop/buy-airpods/airpods-4\",\n",
        "                \"product_name\": \"Apple AirPods 4\",\n",
        "                \"selectors\": [\n",
        "                    '[data-test=\"mms-product-tile\"] .price',\n",
        "                    '.price .sr-only',\n",
        "                    '.price-current',\n",
        "                    '[class*=\"price\"] span'\n",
        "                ]\n",
        "            },\n",
        "            \"Nike\": {\n",
        "                \"url\": \"https://www.billiger.de/search?searchstring=nike+pegasus+41+erwachsene\",\n",
        "                \"product_name\": \"Nike Pegasus Running Shoes\",\n",
        "                \"selectors\": [\n",
        "                    '.product-price .price',\n",
        "                    '.price-unit',\n",
        "                    '.product-box-price .price',\n",
        "                    '[class*=\"price\"]'\n",
        "                ]\n",
        "            },\n",
        "            \"Levi's\": {\n",
        "                \"url\": \"https://www.billiger.de/search?searchstring=levis+501+original+jeans+straight+fit\",\n",
        "                \"product_name\": \"Levi's 501 Original Jeans\",\n",
        "                \"selectors\": [\n",
        "                    '.price .sr-only',\n",
        "                    '.price-sales',\n",
        "                    '.price-current',\n",
        "                    '[class*=\"price\"] span'\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        print(\"‚úÖ Price Research Tool Orchestrator initialized\")\n",
        "\n",
        "    def _extract_prices_from_text(self, text: str, brand: str) -> List[float]:\n",
        "        \"\"\"Extracts prices from text with different formats\"\"\"\n",
        "        price_patterns = [\n",
        "            r'‚Ç¨\\s*(\\d+(?:,\\d+)?(?:\\.\\d+)?)',  # ‚Ç¨99.99 or ‚Ç¨99,99\n",
        "            r'(\\d+(?:,\\d+)?(?:\\.\\d+)?)\\s*‚Ç¨',  # 99.99‚Ç¨ or 99,99‚Ç¨\n",
        "            r'(\\d+(?:\\.\\d+)?)\\s*EUR',         # 99.99 EUR\n",
        "            r'ab\\s*‚Ç¨?\\s*(\\d+(?:,\\d+)?)',      # From ‚Ç¨99\n",
        "            r'(\\d{2,3}(?:,\\d+)?)\\s*‚Ç¨'         # At least 2-digit prices\n",
        "        ]\n",
        "\n",
        "        prices = []\n",
        "        for pattern in price_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    # Convert German format to float\n",
        "                    price_str = match.replace(',', '.')\n",
        "                    price_float = float(price_str)\n",
        "\n",
        "                    # Plausibility checks depending on the product\n",
        "                    if brand == \"Apple\" and 50 < price_float < 300:  # AirPods\n",
        "                        prices.append(price_float)\n",
        "                    elif brand == \"Nike\" and 30 < price_float < 250:  # Shoes\n",
        "                        prices.append(price_float)\n",
        "                    elif brand == \"Levi's\" and 20 < price_float < 200:  # Jeans\n",
        "                        prices.append(price_float)\n",
        "\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "        return sorted(list(set(prices)))  # Sorted and without duplicates\n",
        "\n",
        "    def research_price(self, brand: str) -> Dict:\n",
        "        \"\"\"Researches prices for a brand\"\"\"\n",
        "        print(f\"\\nüí∞ Researching prices for {brand}...\")\n",
        "\n",
        "        if brand not in self.price_configs:\n",
        "            return {\n",
        "                \"brand\": brand,\n",
        "                \"status\": \"error\",\n",
        "                \"message\": \"Brand not configured\"\n",
        "            }\n",
        "\n",
        "        config = self.price_configs[brand]\n",
        "        url = config[\"url\"]\n",
        "        product_name = config[\"product_name\"]\n",
        "\n",
        "        print(f\"   üåê Scraping {url}\")\n",
        "        print(f\"   üì¶ Product: {product_name}\")\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "            'Accept-Language': 'de-DE,de;q=0.9,en;q=0.8',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=15)\n",
        "            print(f\"   üì° HTTP Status: {response.status_code}\")\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                raise Exception(f\"HTTP {response.status_code}\")\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            all_prices = []\n",
        "\n",
        "            # Search for prices with different selectors\n",
        "            for selector in config[\"selectors\"]:\n",
        "                try:\n",
        "                    price_elements = soup.select(selector)\n",
        "\n",
        "                    for element in price_elements[:10]:\n",
        "                        text = element.get_text(strip=True)\n",
        "                        prices = self._extract_prices_from_text(text, brand)\n",
        "                        all_prices.extend(prices)\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "            # Fallback: Search entire text for prices\n",
        "            if not all_prices:\n",
        "                print(\"   üîÑ Using fallback search...\")\n",
        "                page_prices = self._extract_prices_from_text(response.text[:10000], brand)\n",
        "                all_prices.extend(page_prices)\n",
        "\n",
        "            # Remove duplicates and sort\n",
        "            unique_prices = sorted(list(set(all_prices)))\n",
        "\n",
        "            if unique_prices:\n",
        "                min_price = min(unique_prices)\n",
        "                avg_price = sum(unique_prices[:5]) / min(5, len(unique_prices))\n",
        "\n",
        "                return {\n",
        "                    \"brand\": brand,\n",
        "                    \"product\": product_name,\n",
        "                    \"status\": \"success\",\n",
        "                    \"lowest_price\": min_price,\n",
        "                    \"average_price\": round(avg_price, 2),\n",
        "                    \"all_prices\": unique_prices[:10],\n",
        "                    \"source\": url\n",
        "                }\n",
        "            else:\n",
        "                # Fallback\n",
        "                fallback_prices = {\n",
        "                    \"Apple\": 129.0,\n",
        "                    \"Nike\": 89.0,\n",
        "                    \"Levi's\": 59.0\n",
        "                }\n",
        "\n",
        "                return {\n",
        "                    \"brand\": brand,\n",
        "                    \"product\": product_name,\n",
        "                    \"status\": \"fallback\",\n",
        "                    \"lowest_price\": fallback_prices.get(brand, 80.0),\n",
        "                    \"message\": \"No prices found - using fallback\",\n",
        "                    \"source\": \"fallback\"\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Error: {str(e)}\")\n",
        "\n",
        "            # Fallback\n",
        "            fallback_prices = {\n",
        "                \"Apple\": 129.0,\n",
        "                \"Nike\": 89.0,\n",
        "                \"Levi's\": 59.0\n",
        "            }\n",
        "\n",
        "            return {\n",
        "                \"brand\": brand,\n",
        "                \"product\": product_name,\n",
        "                \"status\": \"error\",\n",
        "                \"lowest_price\": fallback_prices.get(brand, 80.0),\n",
        "                \"error\": str(e),\n",
        "                \"source\": \"fallback\"\n",
        "            }\n",
        "\n",
        "    def create_price_message(self, product: str) -> AgentMessage:\n",
        "        \"\"\"Creates structured price message with optional cache\"\"\"\n",
        "        # Derive brand from product\n",
        "        brand = next((b for b in self.price_configs if b.lower() in product.lower()), \"Unknown\")\n",
        "\n",
        "        # Check cache\n",
        "        if brand in self.cached_prices:\n",
        "            price = self.cached_prices[brand]\n",
        "            source = \"cache\"\n",
        "            status = \"cached\"\n",
        "            print(f\"   üß† Price for {brand} from Cache: ‚Ç¨{price:.2f}\")\n",
        "        else:\n",
        "            result = self.research_price(brand)\n",
        "            price = result.get(\"lowest_price\", 80.0)\n",
        "            self.cached_prices[brand] = price\n",
        "            source = result.get(\"source\", \"unknown\")\n",
        "            status = result.get(\"status\", \"unknown\")\n",
        "            print(f\"   üì¶ New price for {brand} searched: ‚Ç¨{price:.2f}\")\n",
        "\n",
        "        return AgentMessage(\n",
        "            from_agent=self.name,\n",
        "            to_agent=\"MainAgent\",\n",
        "            message_type=MessageType.RESPONSE,\n",
        "            content={\n",
        "                \"summary\": f\"Reference price for {product}: ‚Ç¨{price:.2f}\",\n",
        "                \"reference_price\": price,\n",
        "                \"product\": product,\n",
        "                \"source\": source,\n",
        "                \"status\": status\n",
        "            },\n",
        "            priority=2\n",
        "        )\n",
        "\n",
        "\n",
        "# Utility function for Brand Attachment score\n",
        "def calculate_brand_attachment_score(memory: EnhancedMemoryModule, brand: str) -> float:\n",
        "    \"\"\"Calculates Brand Attachment Score from survey responses\"\"\"\n",
        "    attachment_questions = [\"Q11_Affection\", \"Q11_Love\", \"Q11_Connection\",\n",
        "                          \"Q11_Passion\", \"Q11_Delight\", \"Q11_Captivation\"]\n",
        "\n",
        "    scores = []\n",
        "    for q in attachment_questions:\n",
        "        rating = memory.get_survey_rating(brand, q)\n",
        "        if rating:\n",
        "            scores.append(rating)\n",
        "\n",
        "    if not scores:\n",
        "        return 0.5  # Neutral default\n",
        "\n",
        "    # Normalize to 0-1 scale\n",
        "    avg_score = sum(scores) / len(scores)\n",
        "    return (avg_score - 1) / 4  # Convert 1-5 to 0-1 scale\n",
        "\n",
        "\n",
        "print(\"‚úÖ Helper Tool Orchestrators defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1no4JwfYx-g"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 3: Setup and Profile-Loading\n",
        "# =============================================================================\n",
        "\n",
        "import os, re, json, nest_asyncio, pandas as pd\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "\n",
        "nest_asyncio.apply()\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# UniGPT-Client\n",
        "os.environ[\"OPENAI_API_KEY\"]  = \"****\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://gpt.uni-muenster.de/v1\"\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key  = os.environ[\"OPENAI_API_KEY\"],\n",
        "    base_url = os.environ[\"OPENAI_API_BASE\"]\n",
        ")\n",
        "model = \"Llama-3.3-70B\"\n",
        "print(f\"‚úÖ LLM Client configured: {model}\")\n",
        "\n",
        "# Import profile data from Excel\n",
        "print(\"\\nüìä Load profiles from Excel...\")\n",
        "df_profiles = pd.read_excel('/content/drive/MyDrive/profiles_survey/All_profiles.xlsx')\n",
        "print(f\"   Found: {len(df_profiles)} Profile\")\n",
        "\n",
        "# Helper function for filtering empty traits\n",
        "def filter_traits(row, trait_type, max_count=4):\n",
        "    \"\"\"Filters empty/NaN traits from a row\"\"\"\n",
        "    traits = []\n",
        "    for i in range(1, max_count + 1):\n",
        "        trait_value = row[f\"{trait_type}_{i}\"]\n",
        "        # Check for NaN, None, empty strings or whitespace only\n",
        "        if pd.notna(trait_value) and str(trait_value).strip():\n",
        "            traits.append(str(trait_value).strip())\n",
        "    return traits\n",
        "\n",
        "# Convert profiles to structured format\n",
        "profiles = []\n",
        "for _, row in df_profiles.iterrows():\n",
        "    actual_traits = filter_traits(row, \"actual\")\n",
        "    ideal_traits = filter_traits(row, \"ideal\")\n",
        "\n",
        "    profiles.append({\n",
        "        \"age\":                  int(row.age),\n",
        "        \"income_range\":         row.income,\n",
        "        \"occupation\":           row.occupation,\n",
        "        \"gender\":               row.gender,\n",
        "        \"actual_traits\":        actual_traits,\n",
        "        \"actual_top_strength\":  row.actual_top_strength,\n",
        "        \"ideal_traits\":         ideal_traits,\n",
        "        \"ideal_top_strength\":   row.ideal_top_strength,\n",
        "        \"username\":             f\"{row.username} Agent\"\n",
        "    })\n",
        "\n",
        "# Translation function\n",
        "def translate_to_english(text: str) -> str:\n",
        "    \"\"\"Translates text into English (if necessary)\"\"\"\n",
        "    res = client.chat.completions.create(\n",
        "        model   = model,\n",
        "        messages= [\n",
        "            {\"role\":\"system\",\"content\":\"You are a precise translator. If the input is already in English, return it exactly as-is. Otherwise translate literally without introducing synonyms or rephrasings.\"},\n",
        "            {\"role\":\"user\",\"content\":f\"Translate into English (or return unchanged): \\\"{text}\\\"\"}\n",
        "        ],\n",
        "        temperature=0.1\n",
        "    )\n",
        "    return res.choices[0].message.content.strip()\n",
        "\n",
        "# Pre-processing function for profiles\n",
        "def preprocess_profile(profile: dict) -> dict:\n",
        "    \"\"\"Translates all traits of a profile into English\"\"\"\n",
        "    p = dict(profile)\n",
        "    p[\"actual_traits\"]       = [translate_to_english(t) for t in p[\"actual_traits\"]]\n",
        "    p[\"actual_top_strength\"] = translate_to_english(p[\"actual_top_strength\"])\n",
        "    p[\"ideal_traits\"]        = [translate_to_english(t) for t in p[\"ideal_traits\"]]\n",
        "    p[\"ideal_top_strength\"]  = translate_to_english(p[\"ideal_top_strength\"])\n",
        "    return p\n",
        "\n",
        "# Translate all profiles\n",
        "print(\"\\nüåê Translate profiles into English...\")\n",
        "profiles_en = []\n",
        "for i, p in enumerate(profiles):\n",
        "    print(f\"   Process {p['username']}... \", end=\"\")\n",
        "    profiles_en.append(preprocess_profile(p))\n",
        "    print(\"‚úì\")\n",
        "\n",
        "# Reflection functions\n",
        "def reflect_profile(profile: dict) -> str:\n",
        "    \"\"\"Creates a compact psychological summary of the profiles\"\"\"\n",
        "    system_msg = (\n",
        "        \"You are a psychologist. Create a SINGLE PARAGRAPH psychological summary \"\n",
        "        \"that integrates the person's actual self, ideal self, actual top strength, ideal top strength and demographics \"\n",
        "        \"into a cohesive personality profile. Focus on the key psychological dynamics \"\n",
        "        \"and self-concept. Keep it concise but insightful.\"\n",
        "    )\n",
        "\n",
        "    user_content = (\n",
        "        f\"Profile:\\n\"\n",
        "        f\"- Age: {profile['age']}, Gender: {profile['gender']}\\n\"\n",
        "        f\"- Occupation: {profile['occupation']}, Income: {profile['income_range']}\\n\"\n",
        "        f\"- Actual Self: {', '.join(profile['actual_traits'])} (top: {profile['actual_top_strength']})\\n\"\n",
        "        f\"- Ideal Self: {', '.join(profile['ideal_traits'])} (top: {profile['ideal_top_strength']})\\n\"\n",
        "    )\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model    = model,\n",
        "        messages = [\n",
        "            {\"role\":\"system\", \"content\": system_msg},\n",
        "            {\"role\":\"user\",   \"content\": user_content}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=500\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "def reflect_actual_self(profile: dict) -> str:\n",
        "    \"\"\"Creates a psychological expert reflection on the Actual Self\"\"\"\n",
        "    system_msg = (\n",
        "        \"You are a psychology expert. Analyze how this person sees themselves (actual self) \"\n",
        "        \"based on their traits and top strength. Consider that traits are listed in descending order of importance. \"\n",
        "        \"Focus on their self-perception, self-concept, and how they view their current personality. \"\n",
        "        \"Keep it focused and concise.\"\n",
        "    )\n",
        "\n",
        "    user_content = (\n",
        "        f\"Actual Self Analysis:\\n\"\n",
        "        f\"- Actual traits (in order of importance): {', '.join(profile['actual_traits'])}\\n\"\n",
        "        f\"- Actual top strength: {profile['actual_top_strength']}\\n\"\n",
        "        f\"How does this person see themselves? What is their self-concept?\"\n",
        "    )\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model    = model,\n",
        "        messages = [\n",
        "            {\"role\":\"system\", \"content\": system_msg},\n",
        "            {\"role\":\"user\",   \"content\": user_content}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "def reflect_ideal_self(profile: dict) -> str:\n",
        "    \"\"\"Creates a psychological expert reflection on the Ideal Self\"\"\"\n",
        "    system_msg = (\n",
        "        \"You are a psychology expert. Analyze this person's ideal self and aspirations \"\n",
        "        \"based on their ideal traits and top ideal strength. Consider that traits are listed in descending order of importance. \"\n",
        "        \"Focus on their aspirations, what they want to become, and gaps between actual and ideal self. \"\n",
        "        \"Keep it focused and concise.\"\n",
        "    )\n",
        "\n",
        "    user_content = (\n",
        "        f\"Ideal Self Analysis:\\n\"\n",
        "        f\"- Ideal traits (in order of importance): {', '.join(profile['ideal_traits'])}\\n\"\n",
        "        f\"- Ideal top strength: {profile['ideal_top_strength']}\\n\"\n",
        "        f\"What does this person aspire to be? What are their self-improvement goals?\"\n",
        "    )\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model    = model,\n",
        "        messages = [\n",
        "            {\"role\":\"system\", \"content\": system_msg},\n",
        "            {\"role\":\"user\",   \"content\": user_content}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "# Initialize helper tool orchestrators (global, one-time)\n",
        "print(\"\\nü§ñ Initialize Helper Tool Orchestrators...\")\n",
        "# Renaming\n",
        "brand_personality_agent = BrandPersonalityToolOrchestrator()\n",
        "price_research_agent = PriceResearchToolOrchestrator()\n",
        "feedback_agent = FeedbackToolOrchestrator()\n",
        "\n",
        "print(\"\\n‚úÖ Setup done!\")\n",
        "print(f\"   Number Profiles: {len(profiles_en)}\")\n",
        "print(\"   Helper Tool Orchestrators: Ready\")\n",
        "\n",
        "brands = [\"Nike\", \"Apple\", \"Levi's\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZogpTeahY13s"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 4: Agent Enhanced Survey\n",
        "# =============================================================================\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Question templates\n",
        "actual_qs = {\n",
        "    \"Q9_consistent\": \"The personality of {brand} is consistent with my actual self.\",\n",
        "    \"Q9_mirror\":     \"The personality of {brand} is a mirror image of my actual self.\"\n",
        "}\n",
        "ideal_qs = {\n",
        "    \"Q10_consistent\": \"The personality of {brand} is consistent with how I ideally would like to be.\",\n",
        "    \"Q10_mirror\":     \"The personality of {brand} is a mirror image of the person I ideally would like to be.\"\n",
        "}\n",
        "\n",
        "attachment_items = [\"Affection\", \"Love\", \"Connection\", \"Passion\", \"Delight\", \"Captivation\"]\n",
        "\n",
        "class AgentSurveyOrchestrator:\n",
        "    \"\"\"Orchestrates the Agent Survey with LangGraph, Learning and Brand Analysis Tracking\"\"\"\n",
        "\n",
        "    def __init__(self, memory: EnhancedMemoryModule, profile: dict, shared_memory: EnhancedMemoryModule):\n",
        "        self.memory = memory\n",
        "        self.profile = profile\n",
        "        self.shared_memory = shared_memory  # Global Memory\n",
        "        self.workflow = self._build_workflow()\n",
        "\n",
        "    def _build_workflow(self) -> StateGraph:\n",
        "        \"\"\"Orchestrates tools with LangGraph \"\"\"\n",
        "        workflow = StateGraph(AgentState)\n",
        "\n",
        "        # Define nodes\n",
        "        workflow.add_node(\"initial_reasoning\", self.initial_reasoning)\n",
        "        workflow.add_node(\"confidence_check\", self.check_confidence)\n",
        "        workflow.add_node(\"brand_analysis\", self.brand_personality_analysis)\n",
        "        workflow.add_node(\"integrate_challenge\", self.integrate_challenge)\n",
        "        workflow.add_node(\"finalize_answer\", self.finalize_answer)\n",
        "        workflow.add_node(\"apply_learning\", self.apply_learning_adjustment)\n",
        "\n",
        "        # Define Edges\n",
        "        workflow.set_entry_point(\"initial_reasoning\")\n",
        "        workflow.add_edge(\"initial_reasoning\", \"confidence_check\")\n",
        "\n",
        "        # Conditional routing based on Confidence\n",
        "        workflow.add_conditional_edges(\n",
        "            \"confidence_check\",\n",
        "            self.should_use_brand_analysis,\n",
        "            {\n",
        "                True: \"brand_analysis\",\n",
        "                False: \"finalize_answer\"\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Challenge integration if necessary\n",
        "        workflow.add_conditional_edges(\n",
        "            \"brand_analysis\",\n",
        "            self.needs_challenge,\n",
        "            {\n",
        "                True: \"integrate_challenge\",\n",
        "                False: \"finalize_answer\"\n",
        "            }\n",
        "        )\n",
        "\n",
        "        workflow.add_edge(\"integrate_challenge\", \"finalize_answer\")\n",
        "        workflow.add_edge(\"finalize_answer\", \"apply_learning\")\n",
        "        workflow.add_edge(\"apply_learning\", END)\n",
        "\n",
        "        return workflow.compile()\n",
        "\n",
        "    def _get_few_shot_examples(self, question_type: str) -> str:\n",
        "        \"\"\"Returns Few-Shot examples for the respective question type to ensure the correct format\"\"\"\n",
        "\n",
        "        if question_type == \"Q9_consistent\":\n",
        "            return \"\"\"\n",
        "Example for Q9_consistent (Actual Self):\n",
        "Statement: \"The personality of [Brand] is consistent with my actual self.\"\n",
        "\n",
        "Initial reaction: Looking at my actual self - hardworking, practical, down-to-earth - I initially feel some alignment with [Brand]. Starting rating: 4\n",
        "\n",
        "Arguments FOR agreement:\n",
        "- [Brand] emphasizes performance and achievement, which resonates with my hardworking nature\n",
        "- Their achiever mentality aligns with my action-oriented approach\n",
        "- The brand's athletic focus matches my preference for practical, functional products\n",
        "\n",
        "Arguments AGAINST agreement:\n",
        "- As a 45-year-old office worker with modest income, I'm not really their target demographic of young athletes\n",
        "- The brand's premium pricing conflicts with my practical, value-conscious nature and their association with elite sports and celebrity endorsements feels distant from my everyday reality\n",
        "- Their biggest competitor adidas aligns better with me\n",
        "- Recent controversies about labor practices clash with my ethical values\n",
        "\n",
        "Critical weighing: While there's some surface-level alignment with achievement and action, the deeper analysis reveals significant disconnects. My practical, middle-class reality doesn't match their premium, youth-oriented image. The ethical concerns particularly trouble me.\n",
        "So I neither agree nor disagree that the personality of [Brand] is consistent with my actual self. Therefore my rating changed to 3.\n",
        "\n",
        "RATING I: 3\n",
        "\"\"\"\n",
        "\n",
        "        elif question_type == \"Q10_consistent\":\n",
        "            return \"\"\"\n",
        "Example for Q10_consistent (Ideal Self):\n",
        "Statement: \"The personality of [Brand] is consistent with how I ideally would like to be.\"\n",
        "\n",
        "Initial reaction: My ideal self aspires to be innovative and creative. [Brand] has some innovative aspects. Starting rating: 3\n",
        "\n",
        "Arguments FOR agreement:\n",
        "- [Brand] represents innovation in technology, which aligns with my desire to be more tech-savvy\n",
        "- Their minimalist aesthetic appeals to my ideal of being more organized and focused\n",
        "- The brand's association with creativity in their marketing resonates with my aspirations\n",
        "\n",
        "Arguments AGAINST agreement:\n",
        "- The brand's elitist image conflicts with my ideal of being inclusive and approachable\n",
        "- Their closed ecosystem goes against my ideal value of openness and flexibility, which is important to me as a young student\n",
        "- The high price points don't align with my ideal of being financially responsible and my income\n",
        "- Their corporate culture seems demanding and stressful, not the balanced life I aspire to\n",
        "\n",
        "Critical weighing: While [Brand]'s innovation is appealing, their exclusivity and closed approach fundamentally conflicts with my ideal values of openness and inclusivity. So I disagree that the personality of [Brand] is consistent with how I ideally would like to be. Therefore my rating changed to 2.\n",
        "\n",
        "RATING I: 2\n",
        "\"\"\"\n",
        "\n",
        "        elif question_type.startswith(\"Q11_Affection\"):\n",
        "            return \"\"\"\n",
        "Example for Q11_Affection (Brand Attachment):\n",
        "Statement: \"My feelings toward [Brand] can be characterized by Affection.\"\n",
        "\n",
        "Initial reaction: I have mixed feelings about [Brand]. They make decent products but I do not see myself as someone who feels emotions toward a brand. Starting rating: 1\n",
        "\n",
        "Arguments FOR affection:\n",
        "- I've owned their jeans for years and they've been reliable companions\n",
        "- There's some nostalgia from wearing them in my younger days\n",
        "- I appreciate their durability and classic style\n",
        "- Based on my memory it seems like that the brand and I do have some similar beliefs\n",
        "\n",
        "Arguments AGAINST affection:\n",
        "- It's more habit than affection - I buy them because they're available, not out of love\n",
        "- No emotional excitement when I see their products or advertising\n",
        "- Their recent quality seems to have declined while prices increased\n",
        "- Other brands offer better value and innovation now\n",
        "- The relationship feels transactional rather than emotional\n",
        "\n",
        "Critical weighing: While there's some mild positive association from past experiences, true affection requires emotional warmth that's simply not there. So I disagree that my feelings toward the brand can be characterized by affection. Therefore my rating changed to 2.\n",
        "\n",
        "RATING I: 2\n",
        "\"\"\"\n",
        "\n",
        "        else:\n",
        "            # For other Q11 emotions a generic example\n",
        "            return \"\"\"\n",
        "Example for Brand Attachment:\n",
        "Follow the same structure: Start with initial rating, list FOR/AGAINST arguments considering your demographics and the specific emotion, then critically weigh to reach final rating.\n",
        "\"\"\"\n",
        "\n",
        "    def initial_reasoning(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Conducts structured pro/con CoT reasoning\"\"\"\n",
        "        question_type = state[\"question_type\"]\n",
        "        brand = state[\"current_brand\"]\n",
        "\n",
        "        # Get brand-specific memories (not cross-brand for survey)\n",
        "        brand_specific_memories = self.memory.get_brand_specific_memories(brand, include_all_brands=False)\n",
        "\n",
        "        # Get relevant reflection\n",
        "        ltm_data = self.memory.retrieve_long_term()\n",
        "\n",
        "        # Determine which reflection to use based on question type\n",
        "        if question_type.startswith(\"Q9\"):\n",
        "            context = ltm_data[\"actual_self_reflection\"]\n",
        "            focus = \"actual self\"\n",
        "        elif question_type.startswith(\"Q10\"):\n",
        "            context = ltm_data[\"ideal_self_reflection\"]\n",
        "            focus = \"ideal self\"\n",
        "        else:  # Q11\n",
        "            context = ltm_data[\"reflective_summary\"]\n",
        "            focus = \"overall personality\"\n",
        "\n",
        "        # Include demographic data\n",
        "        demographics = f\"\"\"Demographics:\n",
        "        - Age: {self.profile['age']} years old\n",
        "        - Gender: {self.profile['gender']}\n",
        "        - Occupation: {self.profile['occupation']}\n",
        "        - Income: {self.profile['income_range']}\"\"\"\n",
        "\n",
        "        # Only use brand-specific history\n",
        "        history_context = \"\"\n",
        "        brand_memories = [m for m in brand_specific_memories if m.get(\"brand\") == brand and \"reasoning\" in m]\n",
        "        if brand_memories:\n",
        "            recent = brand_memories[-2:]\n",
        "            history_context = f\"\\nPrevious thoughts about {brand}:\\n\"\n",
        "            history_context += \"\\n\".join([f\"- {h['reasoning'][:600]}...\" for h in recent])\n",
        "\n",
        "        # Structured system prompt with scale definition and few-shot examples\n",
        "        few_shot_examples = self._get_few_shot_examples(question_type)\n",
        "\n",
        "        system_msg = (\n",
        "            f\"You are answering a survey SPECIFICALLY about {brand} (not any other brand). \"\n",
        "            f\"Think step by step about {brand} based on your {focus} AND your demographics. \"\n",
        "            \"IMPORTANT SCALE DEFINITION:\\n\"\n",
        "            \"1 = Strongly disagree\\n\"\n",
        "            \"2 = Disagree\\n\"\n",
        "            \"3 = Neither agree nor disagree\\n\"\n",
        "            \"4 = Agree\\n\"\n",
        "            \"5 = Strongly agree\\n\\n\"\n",
        "            \"Follow this structured reasoning process:\\n\"\n",
        "            \"1. Find 2-3 arguments FOR agreement \\n\"\n",
        "            \"2. Find 2-3 arguments AGAINST agreement (including any negative associations)\\n\"\n",
        "            \"3. Consider all facets of the brand - not just surface level\\n\"\n",
        "            \"4. Start with an initial rating, then adjust it as you consider each argument\\n\"\n",
        "            \"5. Critically weigh which arguments are stronger\\n\"\n",
        "            \"6. End with a final rating based on your analysis\\n\\n\"\n",
        "            f\"EXAMPLE REASONING:\\n{few_shot_examples}\"\n",
        "        )\n",
        "\n",
        "        # User Prompt\n",
        "        user_msg = (\n",
        "            f\"CURRENT BRAND: {brand}\\n\\n\"\n",
        "            f\"My {focus}:\\n{context}\\n\\n\"\n",
        "            f\"{demographics}\\n\\n\"\n",
        "            f\"{history_context}\\n\\n\"\n",
        "            f\"Statement about {brand}: {state['current_question']}\\n\\n\"\n",
        "            \"Analyze this statement considering who I am as a person \"\n",
        "            f\"and all facets of {brand}. Does this statement make sense for ME specifically?\\n\\n\"\n",
        "            \"Structure your response as follows:\\n\"\n",
        "            \"1. Initial reaction and starting rating (1-5)\\n\"\n",
        "            \"2. Arguments FOR agreement (2-3 points)\\n\"\n",
        "            \"3. Arguments AGAINST agreement (2-3 points, include negative associations)\\n\"\n",
        "            \"4. Critical weighing of arguments\\n\"\n",
        "            \"5. Final rating with explanation\\n\\n\"\n",
        "            \"Important: End your response with a clear rating in this format:\\n\"\n",
        "            \"RATING I: [number 1-5]\"\n",
        "        )\n",
        "\n",
        "        resp = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_msg},\n",
        "                {\"role\": \"user\", \"content\": user_msg}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=1000\n",
        "        )\n",
        "\n",
        "        reasoning = resp.choices[0].message.content.strip()\n",
        "\n",
        "        # Validate brand consistency\n",
        "        reasoning = self._validate_brand_consistency(reasoning, brand)\n",
        "\n",
        "        state[\"survey_reasoning\"] = reasoning\n",
        "\n",
        "        # Extract initial rating from structured reasoning\n",
        "        initial_rating = self._extract_rating_from_structured_reasoning(reasoning)\n",
        "\n",
        "        state[\"initial_rating\"] = initial_rating\n",
        "\n",
        "        # Calculate Confidence Score\n",
        "        confidence = self._calculate_confidence(reasoning)\n",
        "        state[\"confidence_score\"] = confidence\n",
        "\n",
        "        # Save memories\n",
        "        self.memory.add_cot_thought(reasoning, True, False, confidence)\n",
        "\n",
        "        print(f\"   üß† Initial Reasoning - Rating I: {initial_rating} (Confidence: {confidence:.1f}/10)\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _extract_rating_from_structured_reasoning(self, reasoning: str) -> int:\n",
        "        \"\"\"Extracts the final rating from the structured reasoning\"\"\"\n",
        "        rating_match = re.search(r'RATING I:\\s*(\\d)', reasoning, re.IGNORECASE)\n",
        "        if rating_match:\n",
        "            rating = int(rating_match.group(1))\n",
        "            print(f\"      ‚úì Rating aus 'RATING I' extrahiert: {rating}\")\n",
        "            return rating\n",
        "\n",
        "        # Fallback: Search for other rating patterns\n",
        "        patterns = [\n",
        "            r'final rating is\\s*(\\d)',\n",
        "            r'rate this\\s*(\\d)',\n",
        "            r'rating:\\s*(\\d)',\n",
        "            r'give it a\\s*(\\d)'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, reasoning, re.IGNORECASE)\n",
        "            if match:\n",
        "                rating = int(match.group(1))\n",
        "                if 1 <= rating <= 5:\n",
        "                    print(f\"      ‚ö° Rating from Fallback-Pattern '{pattern}' extracted: {rating}\")\n",
        "                    return rating\n",
        "\n",
        "        # Ultimate fallback based on content\n",
        "        print(f\"      ‚ö†Ô∏è WARNING: No explicit rating found - use fallback estimate\")\n",
        "        fallback_rating = self._estimate_rating_from_reasoning(reasoning)\n",
        "        print(f\"      ‚ö†Ô∏è Fallback-Rating estimate: {fallback_rating}\")\n",
        "        return fallback_rating\n",
        "\n",
        "    def _validate_brand_consistency(self, reasoning: str, current_brand: str) -> str:\n",
        "        \"\"\"Validates and corrects brand inconsistencies\"\"\"\n",
        "        other_brands = [b for b in brands if b != current_brand]\n",
        "\n",
        "        # Check if other brands are mentioned\n",
        "        for other_brand in other_brands:\n",
        "            if other_brand.lower() in reasoning.lower():\n",
        "                print(f\"   ‚ö†Ô∏è Brand inconsistency detected: {other_brand} in {current_brand} reasoning\")\n",
        "                # Remove the problematic reasoning and generate new\n",
        "                return f\"Thinking about {current_brand} specifically, {reasoning.split('.')[0]}.\"\n",
        "\n",
        "        return reasoning\n",
        "\n",
        "    def _calculate_confidence(self, reasoning: str) -> float:\n",
        "        \"\"\"Calculates Confidence Score from Reasoning\"\"\"\n",
        "        # Uncertainty markers\n",
        "        uncertainty_markers = [\n",
        "            \"not sure\", \"uncertain\", \"maybe\", \"perhaps\", \"might\",\n",
        "            \"hard to say\", \"difficult\", \"unclear\", \"don't know\",\n",
        "            \"conflicted\", \"mixed\", \"ambivalent\", \"tough call\"\n",
        "        ]\n",
        "\n",
        "        reasoning_lower = reasoning.lower()\n",
        "        uncertainty_count = sum(1 for marker in uncertainty_markers if marker in reasoning_lower)\n",
        "\n",
        "        # Base confidence\n",
        "        confidence = 7.0\n",
        "\n",
        "        # Reduce for each uncertainty marker\n",
        "        confidence -= uncertainty_count * 0.8\n",
        "\n",
        "        # Boost for clear statements\n",
        "        if any(word in reasoning_lower for word in [\"definitely\", \"certainly\", \"absolutely\", \"clearly\", \"strongly\"]):\n",
        "            confidence += 1.5\n",
        "\n",
        "        # Check for balanced arguments (indicates thoughtful analysis)\n",
        "        if \"however\" in reasoning_lower and \"but\" in reasoning_lower:\n",
        "            confidence += 0.5\n",
        "\n",
        "        return max(1.0, min(10.0, confidence))\n",
        "\n",
        "    def check_confidence(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Checks confidence and decides on brand analysis\"\"\"\n",
        "        confidence = state[\"confidence_score\"]\n",
        "        question_type = state[\"question_type\"]\n",
        "\n",
        "        # Brand Analysis only for Q9/Q10 consistent\n",
        "        if question_type in [\"Q9_consistent\", \"Q10_consistent\"]:\n",
        "            # Threshold: At medium/low confidence ‚Üí Brand Analysis\n",
        "            state[\"use_brand_analysis\"] = confidence < 7.5\n",
        "\n",
        "            # Always analyze if first interaction\n",
        "            if not self.memory.get_brand_reasoning_history(state[\"current_brand\"]):\n",
        "                state[\"use_brand_analysis\"] = True\n",
        "                print(f\"   üí° First {state['current_brand']} Interaction - Brand Analysis activated\")\n",
        "        else:\n",
        "            state[\"use_brand_analysis\"] = False\n",
        "\n",
        "        return state\n",
        "\n",
        "    def should_use_brand_analysis(self, state: AgentState) -> bool:\n",
        "        \"\"\"Routing-Function for Brand Analysis\"\"\"\n",
        "        return state.get(\"use_brand_analysis\", False)\n",
        "\n",
        "    def brand_personality_analysis(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Performs Brand Personality Analysis with Big Five\"\"\"\n",
        "        brand = state[\"current_brand\"]\n",
        "        question_type = state[\"question_type\"]\n",
        "\n",
        "        # Determine which reflection to use\n",
        "        if question_type == \"Q9_consistent\":\n",
        "            reflection_type = \"actual_self\"\n",
        "            reflection_text = self.memory.actual_self_reflection\n",
        "        else:  # Q10_consistent\n",
        "            reflection_type = \"ideal_self\"\n",
        "            reflection_text = self.memory.ideal_self_reflection\n",
        "\n",
        "\n",
        "        evaluation = brand_personality_agent.compare_with_user_profile_enhanced(\n",
        "            brand,\n",
        "            reflection_type,\n",
        "            reflection_text,\n",
        "            self.profile,\n",
        "            client,\n",
        "            model\n",
        "        )\n",
        "\n",
        "        state[\"tool_results\"] = evaluation\n",
        "\n",
        "        # Create challenge if necessary - use initial_rating instead of estimate\n",
        "        initial_rating = state[\"initial_rating\"]\n",
        "        challenge_msg = brand_personality_agent.create_challenge_message(evaluation, initial_rating)\n",
        "\n",
        "        if challenge_msg:\n",
        "            state[\"messages\"].append(challenge_msg)\n",
        "            self.memory.add_agent_message(challenge_msg)\n",
        "            state[\"needs_challenge\"] = True\n",
        "        else:\n",
        "            state[\"needs_challenge\"] = False\n",
        "\n",
        "        return state\n",
        "\n",
        "    def needs_challenge(self, state: AgentState) -> bool:\n",
        "        \"\"\"Routing function for challenge integration\"\"\"\n",
        "        return state.get(\"needs_challenge\", False)\n",
        "\n",
        "    def integrate_challenge(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Integrates challenge from Brand Personality Tool Orchestrator\"\"\"\n",
        "        challenge = next((msg for msg in state[\"messages\"] if msg.message_type == MessageType.CHALLENGE), None)\n",
        "\n",
        "        if not challenge:\n",
        "            return state\n",
        "\n",
        "        print(f\"   ‚ö° Challenge received: {challenge.content['summary']}\")\n",
        "\n",
        "        # Brand-specific challenge handling\n",
        "        brand = state[\"current_brand\"]\n",
        "        initial_rating = state[\"initial_rating\"]\n",
        "\n",
        "        system_msg = (\n",
        "            f\"You received critical feedback about your {brand} brand assessment. \"\n",
        "            f\"Consider this feedback while staying true to your personality. \"\n",
        "            f\"Remember: This is about {brand} only.\\n\\n\"\n",
        "            \"SCALE: 1=Strongly disagree, 2=Disagree, 3=Neither agree nor disagree, 4=Agree, 5=Strongly agree\\n\\n\"\n",
        "            \"IMPORTANT: Based on this feedback, you may:\\n\"\n",
        "            \"1. Keep your original rating if you still believe it's justified\\n\"\n",
        "            \"2. Lower your rating if the feedback suggests a lower level of congruence with the brand.\\n\"\n",
        "            \"3. Increase your rating if the feedback suggests a higher level of congruence with the brand.\\n\"\n",
        "            \"End with: RATING II: [number 1-5]\"\n",
        "        )\n",
        "\n",
        "        user_msg = (\n",
        "            f\"My initial thoughts about {brand}:\\n{state['survey_reasoning']}\\n\\n\"\n",
        "            f\"Initial rating: {initial_rating}\\n\\n\"\n",
        "            f\"Critical feedback: {challenge.content['summary']}\\n\"\n",
        "            f\"Match score: {challenge.content['match_score']}%\\n\"\n",
        "            f\"Suggestion: {challenge.content['suggestion']}\\n\"\n",
        "            f\"Severity: {challenge.content.get('severity', 'medium')}\\n\\n\"\n",
        "            f\"Reconsidering my assessment of {brand}.\\n\"\n",
        "            \"End with: RATING II: [number 1-5]\"\n",
        "        )\n",
        "\n",
        "        resp = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_msg},\n",
        "                {\"role\": \"user\", \"content\": user_msg}\n",
        "            ],\n",
        "            temperature=0.6,\n",
        "            max_tokens=1500\n",
        "        )\n",
        "\n",
        "        revised_reasoning = resp.choices[0].message.content.strip()\n",
        "        state[\"survey_reasoning\"] = revised_reasoning  # Update reasoning\n",
        "\n",
        "        # Extract new rating\n",
        "        new_rating = self._extract_rating_ii(revised_reasoning)\n",
        "\n",
        "        # Clamp on valid range 1-5\n",
        "        new_rating = max(1, min(new_rating, 5))\n",
        "\n",
        "        # Track Brand Analysis Impact\n",
        "        rating_change = new_rating - initial_rating\n",
        "        state[\"brand_analysis_impact\"] = {\n",
        "            \"initial_rating\": initial_rating,\n",
        "            \"post_analysis_rating\": new_rating,\n",
        "            \"rating_change\": rating_change,\n",
        "            \"challenge_received\": True,\n",
        "            \"challenge_severity\": challenge.content.get('severity', 'medium'),\n",
        "            \"match_score\": challenge.content.get('match_score', 0)\n",
        "        }\n",
        "\n",
        "        if new_rating > initial_rating:\n",
        "            print(f\"   üìù Rating increased: {initial_rating} ‚Üí {new_rating}\")\n",
        "        elif new_rating < initial_rating:\n",
        "            print(f\"   üìù Rating lowered:  {initial_rating} ‚Üí {new_rating}\")\n",
        "        else:\n",
        "            print(f\"   üìù Rating retained: {initial_rating}\")\n",
        "\n",
        "\n",
        "        state[\"initial_rating\"] = new_rating\n",
        "\n",
        "        # Update confidence (slightly increased due to reflection)\n",
        "        state[\"confidence_score\"] = min(10.0, state[\"confidence_score\"] + 1.0)\n",
        "\n",
        "        self.memory.add_cot_thought(f\"Revised after challenge: {revised_reasoning}\", True, True)\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _extract_rating_ii(self, reasoning: str) -> int:\n",
        "        \"\"\"Extracts Rating II from the revised reasoning\"\"\"\n",
        "        # Search for \"RATING II: X\" pattern\n",
        "        rating_match = re.search(r'RATING II:\\s*(\\d)', reasoning, re.IGNORECASE)\n",
        "        if rating_match:\n",
        "            rating = int(rating_match.group(1))\n",
        "            return rating\n",
        "\n",
        "        # Fallback to standard extraction\n",
        "        return self._extract_rating_from_structured_reasoning(reasoning)\n",
        "\n",
        "    def finalize_answer(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Finalizes the survey response - only adjustment used for brand analysis\"\"\"\n",
        "        question_type = state[\"question_type\"]\n",
        "        brand = state[\"current_brand\"]\n",
        "        initial_rating = state[\"initial_rating\"]\n",
        "\n",
        "        # Check whether Brand Analysis has been used\n",
        "        brand_analysis_used = state.get(\"use_brand_analysis\", False)\n",
        "\n",
        "        # Track Brand Analysis Impact even if there was no challenge\n",
        "        if brand_analysis_used and \"brand_analysis_impact\" not in state:\n",
        "            state[\"brand_analysis_impact\"] = {\n",
        "                \"initial_rating\": initial_rating,\n",
        "                \"post_analysis_rating\": initial_rating,\n",
        "                \"rating_change\": 0,\n",
        "                \"challenge_received\": False,\n",
        "                \"challenge_severity\": \"none\",\n",
        "                \"match_score\": state.get(\"tool_results\", {}).get(\"user_comparison\", {}).get(\"comparison\", {}).get(\"match_score\", 0)\n",
        "            }\n",
        "        elif not brand_analysis_used:\n",
        "            # No Brand Analysis\n",
        "            state[\"brand_analysis_impact\"] = {\n",
        "                \"initial_rating\": initial_rating,\n",
        "                \"post_analysis_rating\": initial_rating,\n",
        "                \"rating_change\": 0,\n",
        "                \"challenge_received\": False,\n",
        "                \"challenge_severity\": \"not_used\",\n",
        "                \"match_score\": None\n",
        "            }\n",
        "\n",
        "        if not brand_analysis_used:\n",
        "            # No brand analysis ‚Üí Take over initial rating directly\n",
        "            state[\"pre_learning_rating\"] = initial_rating\n",
        "            state[\"final_answer\"]        = initial_rating\n",
        "            state[\"final_reasoning\"]     = state[\"survey_reasoning\"]\n",
        "            print(f\"   ‚úÖ No Brand Analysis ‚Üí Rating II correctly adopted: {initial_rating}\")\n",
        "        else:\n",
        "            # Brand Analysis was used ‚Üí Reasoning may already be adjusted\n",
        "            # Rating may have already been adjusted in integrate_challenge\n",
        "            state[\"pre_learning_rating\"] = initial_rating\n",
        "            state[\"final_answer\"]        = initial_rating\n",
        "            state[\"final_reasoning\"]     = state[\"survey_reasoning\"]\n",
        "            print(f\"   ‚úÖ Brand Analysis used ‚Üí Rating II: {initial_rating}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def apply_learning_adjustment(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Applies Learning Adjustment as the last step\"\"\"\n",
        "        question_type = state[\"question_type\"]\n",
        "        brand = state[\"current_brand\"]\n",
        "        pre_learning_rating = state.get(\"pre_learning_rating\", 3)\n",
        "\n",
        "        # Get Learning Adjustment\n",
        "        learning_adjustment = self.shared_memory.get_learning_adjustment(question_type, brand)\n",
        "\n",
        "        # Improved limit check\n",
        "        if learning_adjustment != 0:\n",
        "            # Determine step based on ¬±0.3 threshold\n",
        "            if learning_adjustment >= 0.3:\n",
        "                step = 1\n",
        "            elif learning_adjustment <= -0.3:\n",
        "                step = -1\n",
        "            else:\n",
        "                step = 0\n",
        "\n",
        "            # Check whether customization is actually possible\n",
        "            if step != 0:\n",
        "                adjusted_rating = max(1, min(5, pre_learning_rating + step))\n",
        "                print(f\"   üìö Learning Adjustment: {pre_learning_rating} ‚Üí {adjusted_rating} (Œî{learning_adjustment:+.1f})\")\n",
        "                state[\"final_answer\"] = adjusted_rating\n",
        "            else:\n",
        "                print(f\"   üìö Learning Adjustment too small for change (Œî{learning_adjustment:+.1f})\")\n",
        "                state[\"final_answer\"] = pre_learning_rating\n",
        "        else:\n",
        "            # No learning adjustment necessary\n",
        "            state[\"final_answer\"] = pre_learning_rating\n",
        "\n",
        "        # Save to memory with final answer\n",
        "        self.memory.add_survey_reasoning(\n",
        "            brand,\n",
        "            question_type,\n",
        "            state[\"final_answer\"],\n",
        "            state[\"survey_reasoning\"],\n",
        "            state[\"confidence_score\"]\n",
        "        )\n",
        "\n",
        "        print(f\"   ‚úÖ {question_type}: {state['final_answer']} (Confidence: {state['confidence_score']:.1f})\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _estimate_rating_from_reasoning(self, reasoning: str) -> int:\n",
        "        \"\"\"Fallback method for rating estimation from reasoning text\"\"\"\n",
        "        print(f\"      üîç Fallback estimation is performed...\")\n",
        "        r = reasoning.lower()\n",
        "\n",
        "        # Strong positive indicators\n",
        "        if any(phrase in r for phrase in [\"strongly agree\", \"definitely agree\", \"absolutely agree\"]):\n",
        "            print(f\"      ‚Üí Strong positive indicators found ‚Üí Rating: 5\")\n",
        "            return 5\n",
        "\n",
        "        # Mild positive indicators\n",
        "        if any(phrase in r for phrase in [\"agree\", \"align\"]):\n",
        "            # Check ob nicht negiert\n",
        "            if not any(neg in r for neg in [\"don't agree\", \"not agree\", \"disagree\"]):\n",
        "                print(f\"      ‚Üí Mild positive indicators found ‚Üí Rating: 4\")\n",
        "                return 4\n",
        "\n",
        "        # Strong negative indicators\n",
        "        if any(phrase in r for phrase in [\"strongly disagree\", \"definitely disagree\", \"not at all\"]):\n",
        "            print(f\"      ‚Üí Strong negative indicators found ‚Üí Rating: 1\")\n",
        "            return 1\n",
        "\n",
        "        # Mild negative indicators\n",
        "        if any(phrase in r for phrase in [\"disagree\", \"not align\", \"different from\", \"not consistent\"]):\n",
        "            print(f\"      ‚Üí Mild negative indicators found ‚Üí Rating: 2\")\n",
        "            return 2\n",
        "\n",
        "        # Neutral indicators\n",
        "        if any(phrase in r for phrase in [\"neither\", \"neutral\", \"mixed\", \"some aspects\"]):\n",
        "            print(f\"      ‚Üí Neutral indicators found ‚Üí Rating: 3\")\n",
        "            return 3\n",
        "\n",
        "        # Default neutral\n",
        "        print(f\"      ‚Üí No strong indicators - Default neutral ‚Üí Rating: 3\")\n",
        "        return 3\n",
        "\n",
        "    async def process_question(self, brand: str, question_code: str, question_text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Processes a single survey question\"\"\"\n",
        "        # Initialize State\n",
        "        initial_state = {\n",
        "            \"current_question\": question_text,\n",
        "            \"current_brand\": brand,\n",
        "            \"question_type\": question_code,\n",
        "            \"profile\": self.profile,\n",
        "            \"memory\": self.memory,\n",
        "            \"messages\": []\n",
        "        }\n",
        "\n",
        "        # Execute workflow\n",
        "        final_state = await self.workflow.ainvoke(initial_state)\n",
        "\n",
        "        # Save pre-learning rating for later analysis and brand analysis impact\n",
        "        return {\n",
        "            \"rating\": final_state[\"final_answer\"],\n",
        "            \"pre_learning_rating\": final_state.get(\"pre_learning_rating\", final_state[\"final_answer\"]),\n",
        "            \"reasoning\": final_state[\"final_reasoning\"],\n",
        "            \"confidence\": final_state[\"confidence_score\"],\n",
        "            \"brand_analysis_impact\": final_state.get(\"brand_analysis_impact\", {})\n",
        "        }\n",
        "\n",
        "\n",
        "async def run_agent_survey(profile: dict, brand: str, memory: EnhancedMemoryModule, shared_memory: EnhancedMemoryModule) -> Tuple[Dict[str, Any], Dict[str, Any], List[Dict[str, Any]]]:\n",
        "    \"\"\"Conducts survey with agent system, learning and brand analysis tracking\"\"\"\n",
        "    results = {\"username\": profile[\"username\"], \"brand\": brand}\n",
        "    pre_learning_results = {\"username\": profile[\"username\"], \"brand\": brand}\n",
        "    brand_analysis_tracking = []\n",
        "\n",
        "    # Create Orchestrator with Shared Memory\n",
        "    orchestrator = AgentSurveyOrchestrator(memory, profile, shared_memory)\n",
        "\n",
        "    print(f\"\\n   üìã {brand} Survey with Agent System:\")\n",
        "\n",
        "    # Process all questions\n",
        "    all_questions = list(actual_qs.keys()) + list(ideal_qs.keys()) + [f\"Q11_{item}\" for item in attachment_items]\n",
        "\n",
        "    for question_code in all_questions:\n",
        "        # Determine question text\n",
        "        if question_code in actual_qs:\n",
        "            question_text = actual_qs[question_code].format(brand=brand)\n",
        "        elif question_code in ideal_qs:\n",
        "            question_text = ideal_qs[question_code].format(brand=brand)\n",
        "        else:\n",
        "            item = question_code.split(\"_\")[1]\n",
        "            question_text = f\"My feelings toward {brand} can be characterized by {item}.\"\n",
        "\n",
        "        # Process question\n",
        "        result = await orchestrator.process_question(brand, question_code, question_text)\n",
        "\n",
        "        # Save result\n",
        "        results[question_code] = result[\"rating\"]\n",
        "        pre_learning_results[question_code] = result[\"pre_learning_rating\"]\n",
        "\n",
        "        # Track Brand Analysis Impact\n",
        "        brand_analysis_impact = result.get(\"brand_analysis_impact\", {})\n",
        "        if brand_analysis_impact:  # Only if Brand Analysis Impact exists\n",
        "            brand_analysis_tracking.append({\n",
        "                \"username\": profile[\"username\"],\n",
        "                \"brand\": brand,\n",
        "                \"question\": question_code,\n",
        "                \"initial_rating\": brand_analysis_impact.get(\"initial_rating\", 0),\n",
        "                \"post_analysis_rating\": brand_analysis_impact.get(\"post_analysis_rating\", 0),\n",
        "                \"rating_change\": brand_analysis_impact.get(\"rating_change\", 0),\n",
        "                \"challenge_received\": brand_analysis_impact.get(\"challenge_received\", False),\n",
        "                \"challenge_severity\": brand_analysis_impact.get(\"challenge_severity\", \"none\"),\n",
        "                \"match_score\": brand_analysis_impact.get(\"match_score\", None),\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "    return results, pre_learning_results, brand_analysis_tracking\n",
        "\n",
        "print(\"‚úÖ Agent Survey System with brand-specific memory retrieval!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm47LvPGY573"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 5: Agent Enhanced Bidding with CoT and Learning\n",
        "# =============================================================================\n",
        "\n",
        "class AgentBiddingOrchestrator:\n",
        "    \"\"\"Orchestrates Agent Bidding with CoT Reasoning and Learning\"\"\"\n",
        "\n",
        "    def __init__(self, memory: EnhancedMemoryModule, profile: dict, shared_memory: EnhancedMemoryModule):\n",
        "        self.memory = memory\n",
        "        self.profile = profile\n",
        "        self.shared_memory = shared_memory\n",
        "        self.workflow = self._build_workflow()\n",
        "\n",
        "    def _build_workflow(self) -> StateGraph:\n",
        "        \"\"\"Builds the Bidding Workflow with Learning\"\"\"\n",
        "        workflow = StateGraph(AgentState)\n",
        "\n",
        "        # Nodes\n",
        "        workflow.add_node(\"get_price_info\", self.get_price_information)\n",
        "        workflow.add_node(\"check_learning\", self.check_bidding_learning)\n",
        "        workflow.add_node(\"initial_cot\", self.initial_chain_of_thought)\n",
        "        workflow.add_node(\"calculate_bid\", self.calculate_final_bid)\n",
        "\n",
        "        # Flow\n",
        "        workflow.set_entry_point(\"get_price_info\")\n",
        "        workflow.add_edge(\"get_price_info\", \"check_learning\")\n",
        "        workflow.add_edge(\"check_learning\", \"initial_cot\")\n",
        "        workflow.add_edge(\"initial_cot\", \"calculate_bid\")\n",
        "        workflow.add_edge(\"calculate_bid\", END)\n",
        "\n",
        "        return workflow.compile()\n",
        "\n",
        "    def get_price_information(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Gets price information from Price Research Tool Orchestrator\"\"\"\n",
        "        product = state[\"current_product\"]\n",
        "\n",
        "        # Create Price Message\n",
        "        price_msg = price_research_agent.create_price_message(product)\n",
        "\n",
        "        # Save Message\n",
        "        self.memory.add_agent_message(price_msg)\n",
        "        state[\"messages\"] = [price_msg]\n",
        "\n",
        "        # Extract price info\n",
        "        state[\"tool_results\"] = {\n",
        "            \"reference_price\": price_msg.content[\"reference_price\"]\n",
        "        }\n",
        "\n",
        "        print(f\"   üí∞ Reference price: ‚Ç¨{price_msg.content['reference_price']:.2f}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def check_bidding_learning(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Checks Learning Feedback for Bidding\"\"\"\n",
        "        product = state[\"current_product\"]\n",
        "\n",
        "        # Extract Brand from Product\n",
        "        brand = None\n",
        "        for b in brands:\n",
        "            if b.lower() in product.lower():\n",
        "                brand = b\n",
        "                break\n",
        "\n",
        "        if brand:\n",
        "            # Get Bidding Adjustment\n",
        "            bidding_adjustment = self.shared_memory.get_bidding_adjustment(brand)\n",
        "\n",
        "            # Get accumulated wisdom for Bidding\n",
        "            wisdom = self.shared_memory.get_accumulated_wisdom()\n",
        "            bidding_wisdom = [w for w in wisdom if f\"bidding_{brand}\" in str(w.get(\"insight\", {}))]\n",
        "\n",
        "            learning_context = {\n",
        "                \"brand\": brand,\n",
        "                \"adjustment\": bidding_adjustment,\n",
        "                \"samples\": len(bidding_wisdom),\n",
        "                \"wisdom\": bidding_wisdom[-2:] if bidding_wisdom else []\n",
        "            }\n",
        "\n",
        "            state[\"learning_feedback\"] = learning_context\n",
        "\n",
        "            if bidding_adjustment != 0:\n",
        "                print(f\"   üìö Learning Adjustment for {brand} Bidding: ‚Ç¨{bidding_adjustment:+.2f}\")\n",
        "        else:\n",
        "            state[\"learning_feedback\"] = {}\n",
        "\n",
        "        return state\n",
        "\n",
        "    def initial_chain_of_thought(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Performs initial Chain-of-Thought Reasoning for Bidding with Learning and cross-brand memories\"\"\"\n",
        "        product = state[\"current_product\"]\n",
        "        product_desc = state.get(\"product_description\", \"\")\n",
        "        reference_price = state[\"tool_results\"][\"reference_price\"]\n",
        "        learning_feedback = state.get(\"learning_feedback\", {})\n",
        "\n",
        "        # Extract Brand\n",
        "        brand = learning_feedback.get(\"brand\", None)\n",
        "        if not brand:\n",
        "            for b in brands:\n",
        "                if b.lower() in product.lower():\n",
        "                    brand = b\n",
        "                    break\n",
        "\n",
        "        # Get relevant psychological reflections from LTM\n",
        "        ltm_data = self.memory.retrieve_long_term()\n",
        "        general_reflection = ltm_data.get(\"reflective_summary\", \"\")\n",
        "\n",
        "        # Get ALL brand memories for bidding (cross-brand experiences)\n",
        "        all_brand_memories = self.memory.get_brand_specific_memories(brand, include_all_brands=True)\n",
        "\n",
        "        # Create memory context from ALL brand experiences\n",
        "        memory_context = \"\"\n",
        "        if all_brand_memories:\n",
        "            memory_context = \"My brand experiences from the survey:\\n\"\n",
        "\n",
        "            # Group by brand for clarity\n",
        "            brand_groups = {}\n",
        "            for mem in all_brand_memories:\n",
        "                mem_brand = mem.get(\"brand\", \"Unknown\")\n",
        "                if mem_brand not in brand_groups:\n",
        "                    brand_groups[mem_brand] = []\n",
        "                brand_groups[mem_brand].append(mem)\n",
        "\n",
        "            # Add memories from all brands\n",
        "            for mem_brand, memories in brand_groups.items():\n",
        "                if memories:\n",
        "                    memory_context += f\"\\n{mem_brand}:\\n\"\n",
        "                    # Take most recent/relevant memories\n",
        "                    relevant_memories = [m for m in memories if \"reasoning\" in m][-3:]\n",
        "                    for mem in relevant_memories:\n",
        "                        q_type = mem.get(\"question\", \"\").split(\"_\", 2)[-1]\n",
        "                        memory_context += f\"- {q_type}: {mem['reasoning'][:200]}...\\n\"\n",
        "\n",
        "        # Learning Context for Bidding\n",
        "        learning_context = \"\"\n",
        "        if learning_feedback.get(\"adjustment\", 0) != 0:\n",
        "            learning_context = f\"\\nLearning insight: Previous agents' bids for {brand} tend to be ‚Ç¨{learning_feedback['adjustment']:+.2f} different than initial instinct.\"\n",
        "\n",
        "        if learning_feedback.get(\"wisdom\"):\n",
        "            for w in learning_feedback[\"wisdom\"]:\n",
        "                if \"recommendations\" in w.get(\"insight\", {}):\n",
        "                    rec = w[\"insight\"][\"recommendations\"]\n",
        "                    if f\"bidding_{brand}\" in rec:\n",
        "                        learning_context += f\"\\n{rec[f'bidding_{brand}']}\"\n",
        "\n",
        "        # Chain-of-Thought for Bidding\n",
        "        cot_system = (\n",
        "            \"You are making a purchasing decision. Think through this step by step:\\n\"\n",
        "            \"1. Consider your personality and profile\\n\"\n",
        "            \"2. Reflect on your relationship with ALL brands from the survey (not just the current product's brand)\\n\"\n",
        "            \"3. Compare how you feel about different brands\\n\"\n",
        "            \"4. Assess this specific brand's value to you personally\\n\"\n",
        "            \"5. Consider the reference price as one factor among many\\n\"\n",
        "            \"6. Determine what you're willing to pay based strongly on YOUR values and preferences\"\n",
        "        )\n",
        "\n",
        "        cot_user = (\n",
        "            f\"My self-concept:\\n{general_reflection}\\n\\n\"\n",
        "            f\"{memory_context}\\n\"\n",
        "            f\"{learning_context}\\n\\n\"\n",
        "            f\"Product to bid on:\\n\"\n",
        "            f\"- Name: {product}\\n\"\n",
        "            f\"- Description: {product_desc}\\n\"\n",
        "            f\"- Market reference: Products like this typically cost around ‚Ç¨{reference_price:.2f}\\n\\n\"\n",
        "            \"Let me think through my bidding decision step by step, considering all my brand experiences...\"\n",
        "        )\n",
        "\n",
        "        # Generate Chain-of-Thought\n",
        "        resp = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": cot_system},\n",
        "                {\"role\": \"user\", \"content\": cot_user}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=700\n",
        "        )\n",
        "\n",
        "        cot_process = resp.choices[0].message.content.strip()\n",
        "        state[\"bidding_reasoning\"] = cot_process\n",
        "\n",
        "        # Consistency Check for Bidding\n",
        "        consistency_system = (\n",
        "            \"Evaluate if this bidding thought process is consistent with the person's \"\n",
        "            \"overall self-concept, profile, and ALL their survey responses across different brands. \"\n",
        "            \"Consider if the reasoning makes sense given their traits and previous responses to all brands.\"\n",
        "        )\n",
        "\n",
        "        consistency_user = (\n",
        "            f\"Person's profile:\\n\"\n",
        "            f\"- Overall self-concept: {general_reflection[:200]}...\\n\"\n",
        "            f\"- Age: {self.profile['age']}\\n\"\n",
        "            f\"- Gender: {self.profile['gender']}\\n\"\n",
        "            f\"- Occupation: {self.profile['occupation']}\\n\"\n",
        "            f\"- Income: {self.profile['income_range']}\\n\\n\"\n",
        "            f\"Bidding thought process:\\n{cot_process}\\n\\n\"\n",
        "            \"Is this bidding reasoning consistent with their profile and all brand experiences? Provide brief feedback.\"\n",
        "        )\n",
        "\n",
        "        resp = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": consistency_system},\n",
        "                {\"role\": \"user\", \"content\": consistency_user}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            max_tokens=150\n",
        "        )\n",
        "\n",
        "        consistency_feedback = resp.choices[0].message.content.strip()\n",
        "        state[\"consistency_feedback\"] = consistency_feedback\n",
        "\n",
        "        # Add CoT to Memory\n",
        "        self.memory.add_cot_thought(cot_process, True, False)\n",
        "\n",
        "        print(f\"   üß† Chain-of-Thought Reasoning completed (using cross-brand experiences)\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def calculate_final_bid(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Calculates final bid based on CoT with Learning Adjustments\"\"\"\n",
        "        cot_process = state[\"bidding_reasoning\"]\n",
        "        consistency_feedback = state.get(\"consistency_feedback\", \"\")\n",
        "        reference_price = state[\"tool_results\"][\"reference_price\"]\n",
        "        learning_feedback = state.get(\"learning_feedback\", {})\n",
        "\n",
        "        # Final Bid Decision\n",
        "        bid_system = (\n",
        "            \"Based on the chain of thought and consistency feedback, determine the final bid amount. \"\n",
        "            \"The bid should reflect the person's authentic willingness to pay based on their \"\n",
        "            \"overall self-concept, values, and relationship with ALL brands from their survey experiences. \"\n",
        "            \"Let the person's reasoning guide the decision naturally. \"\n",
        "            \"End with ONLY the bid amount as a number on the last line.\"\n",
        "        )\n",
        "\n",
        "        bid_user = (\n",
        "            f\"Chain of thought:\\n{cot_process}\\n\\n\"\n",
        "            f\"Consistency feedback:\\n{consistency_feedback}\\n\\n\"\n",
        "            f\"Reference price: ‚Ç¨{reference_price:.2f}\\n\\n\"\n",
        "            \"Considering everything, my final bid amount in euros:\"\n",
        "        )\n",
        "\n",
        "        resp = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": bid_system},\n",
        "                {\"role\": \"user\", \"content\": bid_user}\n",
        "            ],\n",
        "            temperature=0.5,\n",
        "            max_tokens=500\n",
        "        )\n",
        "\n",
        "        bid_text = resp.choices[0].message.content.strip()\n",
        "\n",
        "        # Parse Bid\n",
        "        bid = 0.0\n",
        "        all_numbers = re.findall(r'‚Ç¨?\\s*(\\d+(?:\\.\\d+)?)', bid_text)\n",
        "        if all_numbers:\n",
        "            bid = float(all_numbers[-1])\n",
        "\n",
        "        # Fallback if no number found\n",
        "        if bid == 0.0:\n",
        "            bid = reference_price * 0.9  # 90% of reference price as fallback\n",
        "\n",
        "        # Apply Learning Adjustment\n",
        "        if learning_feedback.get(\"adjustment\", 0) != 0:\n",
        "            original_bid = bid\n",
        "            bid = bid + learning_feedback[\"adjustment\"]\n",
        "            bid = max(40.0, bid)  # Minimum ‚Ç¨40\n",
        "            if bid != original_bid:\n",
        "                print(f\"      üìö Learning Adjustment applied: ‚Ç¨{original_bid:.2f} ‚Üí ‚Ç¨{bid:.2f}\")\n",
        "\n",
        "        # Minimum bid ‚Ç¨40\n",
        "        bid = max(40.0, bid)\n",
        "\n",
        "        state[\"final_answer\"] = round(bid, 2)\n",
        "        state[\"final_reasoning\"] = cot_process\n",
        "\n",
        "        # Extract Brand for storage\n",
        "        product = state[\"current_product\"]\n",
        "        brand = learning_feedback.get(\"brand\", \"Unknown\")\n",
        "        if brand == \"Unknown\":\n",
        "            brand = next((b for b in brands if b.lower() in product.lower()), \"Unknown\")\n",
        "\n",
        "        # Save in Memory\n",
        "        self.memory.add_brand_reasoning(\n",
        "            brand=brand,\n",
        "            interaction_type=\"bidding\",\n",
        "            question=f\"WTP for {product}\",\n",
        "            reasoning=f\"{cot_process[:200]}... Reference: ‚Ç¨{reference_price:.2f}\",\n",
        "            rating=int(bid)\n",
        "        )\n",
        "\n",
        "        print(f\"   ‚úÖ Final bid: ‚Ç¨{bid:.2f}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    async def process_bidding(self, product_name: str, product_desc: str) -> Tuple[float, str]:\n",
        "        \"\"\"Processes Bidding for a product\"\"\"\n",
        "        # Initialize State\n",
        "        initial_state = {\n",
        "            \"current_product\": product_name,\n",
        "            \"product_description\": product_desc,\n",
        "            \"profile\": self.profile,\n",
        "            \"memory\": self.memory,\n",
        "            \"messages\": []\n",
        "        }\n",
        "\n",
        "        # Execute Workflow\n",
        "        final_state = await self.workflow.ainvoke(initial_state)\n",
        "\n",
        "        return final_state[\"final_answer\"], final_state[\"final_reasoning\"]\n",
        "\n",
        "\n",
        "async def calculate_agent_bid(\n",
        "    profile: dict,\n",
        "    product_name: str,\n",
        "    product_desc: str,\n",
        "    memory: EnhancedMemoryModule,\n",
        "    shared_memory: EnhancedMemoryModule\n",
        ") -> Tuple[float, str]:\n",
        "    \"\"\"Wrapper for Agent Bidding with CoT and Learning\"\"\"\n",
        "    orchestrator = AgentBiddingOrchestrator(memory, profile, shared_memory)\n",
        "    return await orchestrator.process_bidding(product_name, product_desc)\n",
        "\n",
        "print(\"‚úÖ Agent Bidding System!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgoJ4UgyZDcQ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 6: Enhanced Bidding Agent\n",
        "# =============================================================================\n",
        "\n",
        "async def run_agent_shop_bot(profile: dict, memory: EnhancedMemoryModule, shared_memory: EnhancedMemoryModule):\n",
        "    \"\"\"Runs Bidding Agent\"\"\"\n",
        "    bidding_results = []\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(\n",
        "            headless=True,\n",
        "            args=['--no-sandbox', '--disable-setuid-sandbox']\n",
        "        )\n",
        "        page = await browser.new_page()\n",
        "        page.set_default_navigation_timeout(60000)\n",
        "        page.set_default_timeout(60000)\n",
        "\n",
        "        # Accept username dialog\n",
        "        page.on(\"dialog\", lambda dialog: asyncio.create_task(\n",
        "            dialog.accept(profile[\"username\"])\n",
        "        ))\n",
        "\n",
        "        print(f\"\\nüõí Start Agent Bidding for {profile['username']}\")\n",
        "        print(\"   Navigate to the website...\")\n",
        "\n",
        "        await page.goto(\n",
        "            \"https://auction-shop-agent.web.app/\",\n",
        "            wait_until=\"load\",\n",
        "            timeout=60000\n",
        "        )\n",
        "\n",
        "\n",
        "        for product_idx in range(3):\n",
        "            print(f\"\\n   üì¶ Product {product_idx+1}/3:\")\n",
        "\n",
        "            await page.wait_for_selector(\".product-content h2\", timeout=60000)\n",
        "            product_name = await page.text_content(\".product-content h2\")\n",
        "            product_desc = await page.text_content(\".product-content p\")\n",
        "            print(f\"      Name: {product_name}\")\n",
        "\n",
        "            # Calculate bid with Agent System\n",
        "            bid, reasoning = await calculate_agent_bid(\n",
        "                profile, product_name, product_desc, memory, shared_memory\n",
        "            )\n",
        "\n",
        "            # Determine brand\n",
        "            brand = next((b for b in brands if b.lower() in product_name.lower()), \"Unknown\")\n",
        "\n",
        "            # Save results\n",
        "            bidding_results.append({\n",
        "                \"username\": profile[\"username\"],\n",
        "                \"product_idx\": product_idx + 1,\n",
        "                \"product_name\": product_name,\n",
        "                \"product_desc\": product_desc,\n",
        "                \"bid\": bid,\n",
        "                \"brand\": brand,\n",
        "                \"reasoning\": reasoning,\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "            # Place a bid\n",
        "            await page.fill(\"input[type=number]\", f\"{bid:.2f}\")\n",
        "            await page.click(\"button\")\n",
        "\n",
        "            # Waiting for next product or final page\n",
        "            if product_idx < 2:\n",
        "                await page.wait_for_function(\n",
        "                    \"([selector, oldText]) => document.querySelector(selector)?.textContent !== oldText\",\n",
        "                    arg=[\".product-content h2\", product_name],\n",
        "                    timeout=60000\n",
        "                )\n",
        "            else:\n",
        "                await page.wait_for_selector(\"h1:has-text('Thank you')\", timeout=60000)\n",
        "                print(f\"\\n   Shopping completed for {profile['username']}!\")\n",
        "\n",
        "        await browser.close()\n",
        "    return bidding_results\n",
        "\n",
        "print(\"‚úÖ Agent shopping defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd3sD7qcZE5C"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 7: Complete Agent Survey Execution with Feedback Loop\n",
        "# =============================================================================\n",
        "\n",
        "async def run_complete_agent_survey_for_single_agent(profile: dict, memory: EnhancedMemoryModule, shared_memory: EnhancedMemoryModule):\n",
        "    \"\"\"Carries out the complete survey for a single agent and all brands\"\"\"\n",
        "\n",
        "    agent_name = profile[\"username\"]\n",
        "    survey_results = []\n",
        "    pre_learning_results = []\n",
        "    reasoning_data = []\n",
        "    agent_messages = []\n",
        "\n",
        "    # Dictionary for agent responses (for feedback)\n",
        "    agent_responses = {}\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üë§ Agent: {agent_name}\")\n",
        "    print(f\"   Actual: {' > '.join(profile['actual_traits'][:2])}...\")\n",
        "    print(f\"   Ideal: {' > '.join(profile['ideal_traits'][:2])}...\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Empty STM, CoT History and Messages before each new agent\n",
        "    memory.short_term_memory.clear()\n",
        "    memory.cot_history.clear()\n",
        "    memory.agent_messages.clear()\n",
        "    memory._survey_ratings.clear()\n",
        "\n",
        "    # Survey for every brand\n",
        "    for brand in brands:\n",
        "        print(f\"\\n   üìã {brand} Survey with Agent Orchestration:\")\n",
        "\n",
        "        try:\n",
        "            results, pre_learning = await run_agent_survey(profile, brand, memory, shared_memory)\n",
        "\n",
        "            # Collect data for DataFrame\n",
        "            for question_code in [\"Q9_consistent\", \"Q9_mirror\", \"Q10_consistent\", \"Q10_mirror\"] + [f\"Q11_{item}\" for item in attachment_items]:\n",
        "                rating = results[question_code]\n",
        "                pre_learning_rating = pre_learning[question_code]\n",
        "\n",
        "                # Save for feedback\n",
        "                agent_responses[question_code] = rating\n",
        "\n",
        "                survey_results.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"brand\": brand,\n",
        "                    \"question\": question_code,\n",
        "                    \"rating\": rating,\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "\n",
        "                # Save Pre-Learning Results\n",
        "                pre_learning_results.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"brand\": brand,\n",
        "                    \"question\": question_code,\n",
        "                    \"rating\": pre_learning_rating,\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "\n",
        "                # Find the corresponding reasoning from the memory\n",
        "                reasoning_entry = next(\n",
        "                    (m for m in memory.short_term_memory\n",
        "                     if m.get(\"question\", \"\").endswith(question_code) and m.get(\"brand\") == brand),\n",
        "                    None\n",
        "                )\n",
        "\n",
        "                if reasoning_entry:\n",
        "                    reasoning_data.append({\n",
        "                        \"username\": agent_name,\n",
        "                        \"brand\": brand,\n",
        "                        \"question\": question_code,\n",
        "                        \"rating\": rating,\n",
        "                        \"pre_learning_rating\": pre_learning_rating,\n",
        "                        \"reasoning\": reasoning_entry[\"reasoning\"],\n",
        "                        \"confidence\": reasoning_entry.get(\"confidence\", None)\n",
        "                    })\n",
        "\n",
        "            # Collect agent messages for this brand\n",
        "            brand_messages = [msg for msg in memory.agent_messages\n",
        "                            if any(brand in str(msg.content) for brand in [brand])]\n",
        "\n",
        "            for msg in brand_messages:\n",
        "                agent_messages.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"brand\": brand,\n",
        "                    \"from_agent\": msg.from_agent,\n",
        "                    \"to_agent\": msg.to_agent,\n",
        "                    \"message_type\": msg.message_type.value,\n",
        "                    \"priority\": msg.priority,\n",
        "                    \"content_summary\": msg.content.get(\"summary\", \"\"),\n",
        "                    \"timestamp\": msg.timestamp\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error at {brand}: {str(e)}\")\n",
        "\n",
        "    return survey_results, pre_learning_results, reasoning_data, agent_messages, agent_responses\n",
        "\n",
        "print(\"‚úÖ Agent Survey Execution System defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JludpQufKZOY"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 8: Complete Agent Study\n",
        "# =============================================================================\n",
        "\n",
        "async def run_complete_agent_experiment():\n",
        "    \"\"\"Runs the complete study\"\"\"\n",
        "\n",
        "    print(\"üöÄ Start study\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Initialization of global shared memory for learning\n",
        "    print(\"\\nüß† Create global shared memory for collective learning...\")\n",
        "    shared_learning_memory = EnhancedMemoryModule(\n",
        "        agent_name=\"SharedLearning\",\n",
        "        max_short_term_size=100\n",
        "    )\n",
        "    print(\"‚úÖ Shared Learning Memory\")\n",
        "\n",
        "    # Collection storage for all results\n",
        "    all_survey_results = []\n",
        "    all_pre_learning_results = []\n",
        "    all_reasoning_data = []\n",
        "    all_survey_messages = []\n",
        "    all_bidding_results = []\n",
        "    all_bidding_messages = []\n",
        "    all_feedback_messages = []\n",
        "    all_brand_analysis_tracking = []\n",
        "\n",
        "    total_agents = len(profiles_en)\n",
        "    start_time = time.time()\n",
        "    save_interval = 10  # Save all 10 agents\n",
        "\n",
        "    # Output directories\n",
        "    output_dir_survey = \"/content/drive/MyDrive/agent_survey_results/\"\n",
        "    output_dir_bidding = \"/content/drive/MyDrive/agent_bidding_results/\"\n",
        "    output_dir_learning = \"/content/drive/MyDrive/agent_learning_results/\"\n",
        "\n",
        "    import os\n",
        "    os.makedirs(output_dir_survey, exist_ok=True)\n",
        "    os.makedirs(output_dir_bidding, exist_ok=True)\n",
        "    os.makedirs(output_dir_learning, exist_ok=True)\n",
        "\n",
        "    # Base timestamp for consistent file names\n",
        "    base_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # MAIN LOOP: Complete sequential processing for each agent\n",
        "    for agent_idx, profile in enumerate(profiles_en):\n",
        "        agent_name = profile[\"username\"]\n",
        "\n",
        "        print(f\"\\n{'#'*80}\")\n",
        "        print(f\"# AGENT {agent_idx + 1}/{total_agents}: {agent_name}\")\n",
        "        print(f\"{'#'*80}\")\n",
        "\n",
        "        # STEP 1: Profile processing and reflections\n",
        "        print(f\"\\nüîÆ PHASE 1: PROFILE PROCESSING for {agent_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Create memory module for this agent\n",
        "        print(f\"   üß† Create memory module for {agent_name}...\")\n",
        "        memory = EnhancedMemoryModule(\n",
        "            agent_name=agent_name,\n",
        "            max_short_term_size=20\n",
        "        )\n",
        "\n",
        "        # Use global embedding model for efficiency\n",
        "        if 'embeddings_model' not in globals():\n",
        "            print(\"   üì• Load Embedding-Modell...\")\n",
        "            from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "            embeddings_model = HuggingFaceEmbeddings(\n",
        "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "            )\n",
        "        memory.embeddings = embeddings_model\n",
        "\n",
        "        # Create psychological reflections\n",
        "        print(\"   üîÆ Create psychological summaries...\")\n",
        "        general_summary = reflect_profile(profile)\n",
        "        actual_self_summary = reflect_actual_self(profile)\n",
        "        ideal_self_summary = reflect_ideal_self(profile)\n",
        "\n",
        "        # Save profile and all reflections\n",
        "        memory.store_profile(profile, general_summary, actual_self_summary, ideal_self_summary)\n",
        "\n",
        "        print(f\"   üìù General: {general_summary[:100]}...\")\n",
        "        print(f\"   üéØ Actual Self: {actual_self_summary[:80]}...\")\n",
        "        print(f\"   ‚≠ê Ideal Self: {ideal_self_summary[:80]}...\")\n",
        "        print(f\"   ‚úÖ Memory Module for {agent_name} created!\")\n",
        "\n",
        "        # Dictionary for all agent responses\n",
        "        all_agent_responses = {}\n",
        "\n",
        "        # STEP 2: Survey for this agent\n",
        "        print(f\"\\nüìã PHASE 2: SURVEY for {agent_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Updated Call with Brand Analysis Tracking\n",
        "        agent_survey_results, agent_pre_learning_results, agent_reasoning_data, agent_survey_messages, agent_survey_responses, agent_brand_analysis_tracking = \\\n",
        "            await run_complete_agent_survey_for_single_agent(profile, memory, shared_learning_memory)\n",
        "\n",
        "        # Collect Survey-results\n",
        "        all_survey_results.extend(agent_survey_results)\n",
        "        all_pre_learning_results.extend(agent_pre_learning_results)\n",
        "        all_reasoning_data.extend(agent_reasoning_data)\n",
        "        all_survey_messages.extend(agent_survey_messages)\n",
        "        all_brand_analysis_tracking.extend(agent_brand_analysis_tracking)\n",
        "\n",
        "        # Update agent responses dictionary\n",
        "        all_agent_responses.update(agent_survey_responses)\n",
        "\n",
        "        print(f\"\\n‚úÖ Survey for {agent_name} done!\")\n",
        "\n",
        "        # Show Brand Attachment Scores\n",
        "        print(\"   üìä Brand Attachment Scores:\")\n",
        "        for brand in brands:\n",
        "            attachment = calculate_brand_attachment_score(memory, brand)\n",
        "            print(f\"      - {brand}: {attachment:.2%}\")\n",
        "\n",
        "        # STEP 3: Bidding for this agent\n",
        "        print(f\"\\nüõí PHASE 3: Bidding for {agent_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Clear agent messages for bidding\n",
        "        memory.agent_messages.clear()\n",
        "\n",
        "        try:\n",
        "            bidding_results = await run_agent_shop_bot(profile, memory, shared_learning_memory)\n",
        "            all_bidding_results.extend(bidding_results)\n",
        "\n",
        "            # Add bidding results to agent_responses\n",
        "            for bid_result in bidding_results:\n",
        "                brand = bid_result[\"brand\"]\n",
        "                all_agent_responses[f\"bid_{brand}\"] = bid_result[\"bid\"]\n",
        "\n",
        "            # Collect Agent Messages from bidding\n",
        "            for msg in memory.agent_messages:\n",
        "                all_bidding_messages.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"from_agent\": msg.from_agent,\n",
        "                    \"to_agent\": msg.to_agent,\n",
        "                    \"message_type\": msg.message_type.value,\n",
        "                    \"priority\": msg.priority,\n",
        "                    \"content_summary\": msg.content.get(\"summary\", \"\"),\n",
        "                    \"timestamp\": msg.timestamp\n",
        "                })\n",
        "\n",
        "            print(f\"\\n‚úÖ Bidding for {agent_name} done!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n   ‚ùå Critical error when bidding for {agent_name}: {e!s}\")\n",
        "\n",
        "        # SCHRITT 4: FEEDBACK LOOP\n",
        "        print(f\"\\nüìä PHASE 4: FEEDBACK for {agent_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Generate feedback for each brand\n",
        "        for brand in brands:\n",
        "            feedback_msg = feedback_agent.create_feedback_message(all_agent_responses, profile, brand)\n",
        "\n",
        "            # Save feedback in shared memory\n",
        "            shared_learning_memory.add_learning_feedback(feedback_msg)\n",
        "\n",
        "            # Also save in the individual memory\n",
        "            memory.add_agent_message(feedback_msg)\n",
        "\n",
        "            # Collect for logging\n",
        "            all_feedback_messages.append({\n",
        "                \"username\": agent_name,\n",
        "                \"brand\": brand,\n",
        "                \"from_agent\": feedback_msg.from_agent,\n",
        "                \"to_agent\": feedback_msg.to_agent,\n",
        "                \"message_type\": feedback_msg.message_type.value,\n",
        "                \"priority\": feedback_msg.priority,\n",
        "                \"profile_match_score\": feedback_msg.content.get(\"profile_match_score\", 0),\n",
        "                \"matches_used\": feedback_msg.content.get(\"matches_used\", 0),\n",
        "                \"recommendations\": json.dumps(feedback_msg.content.get(\"recommendations\", {})),\n",
        "                \"timestamp\": feedback_msg.timestamp\n",
        "            })\n",
        "\n",
        "            print(f\"   üìö Feedback generated for {brand} and saved in the shared memory\")\n",
        "\n",
        "        # Show Learning Summary\n",
        "        print(f\"\\n   üìà Learning Summary after {agent_name}:\")\n",
        "        print(f\"      Survey Adjustments: {len(shared_learning_memory.learning_feedback['survey_adjustments'])} Pattern\")\n",
        "        print(f\"      Bidding Adjustments: {len(shared_learning_memory.learning_feedback['bidding_adjustments'])} Pattern\")\n",
        "        print(f\"      Accumulated Wisdom: {len(shared_learning_memory.learning_feedback['accumulated_wisdom'])} Insights\")\n",
        "\n",
        "        # Progress bar\n",
        "        elapsed = time.time() - start_time\n",
        "        progress = ((agent_idx + 1) / total_agents) * 100\n",
        "        eta = (elapsed / (agent_idx + 1)) * (total_agents - agent_idx - 1) if agent_idx < total_agents - 1 else 0\n",
        "\n",
        "        print(f\"\\nüìà Overall progress: {progress:.1f}% ({agent_idx + 1}/{total_agents} agents)\")\n",
        "        if eta > 0:\n",
        "            print(f\"   Estimated remaining time: {eta:.0f}s\")\n",
        "\n",
        "        # INCREMENTAL STORAGE: All 10 agents\n",
        "        if (agent_idx + 1) % save_interval == 0 or (agent_idx + 1) == total_agents:\n",
        "            print(f\"\\nüíæ Save after every {agent_idx + 1} Agent...\")\n",
        "\n",
        "            # Create DataFrames\n",
        "            df_survey_results = pd.DataFrame(all_survey_results)\n",
        "            if len(df_survey_results) > 0:\n",
        "                df_pivot = df_survey_results.pivot_table(\n",
        "                    index=['username', 'brand'],\n",
        "                    columns='question',\n",
        "                    values='rating'\n",
        "                ).reset_index()\n",
        "            else:\n",
        "                df_pivot = pd.DataFrame()\n",
        "\n",
        "            # Pre-Learning Results DataFrame\n",
        "            df_pre_learning_results = pd.DataFrame(all_pre_learning_results)\n",
        "            if len(df_pre_learning_results) > 0:\n",
        "                df_pre_learning_pivot = df_pre_learning_results.pivot_table(\n",
        "                    index=['username', 'brand'],\n",
        "                    columns='question',\n",
        "                    values='rating'\n",
        "                ).reset_index()\n",
        "            else:\n",
        "                df_pre_learning_pivot = pd.DataFrame()\n",
        "\n",
        "            # Brand Analysis Tracking DataFrame\n",
        "            df_brand_analysis_tracking = pd.DataFrame(all_brand_analysis_tracking)\n",
        "\n",
        "            # Reasoning-data with Confidence\n",
        "            df_reasoning = pd.DataFrame(all_reasoning_data)\n",
        "\n",
        "            # Agent Communication Logs\n",
        "            df_survey_messages = pd.DataFrame(all_survey_messages)\n",
        "            df_bidding_messages = pd.DataFrame(all_bidding_messages)\n",
        "            df_feedback_messages = pd.DataFrame(all_feedback_messages)\n",
        "\n",
        "            # Bidding-results\n",
        "            df_bidding = pd.DataFrame(all_bidding_results)\n",
        "\n",
        "            # Learning Summary\n",
        "            learning_summary = {\n",
        "                \"survey_adjustments\": shared_learning_memory.learning_feedback[\"survey_adjustments\"],\n",
        "                \"bidding_adjustments\": shared_learning_memory.learning_feedback[\"bidding_adjustments\"],\n",
        "                \"wisdom_count\": len(shared_learning_memory.learning_feedback[\"accumulated_wisdom\"]),\n",
        "                \"agents_processed\": agent_idx + 1,\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "            # Save with checkpoint number\n",
        "            checkpoint_num = (agent_idx + 1) // save_interval\n",
        "            checkpoint_suffix = f\"checkpoint{checkpoint_num}_{agent_idx + 1}agents\"\n",
        "\n",
        "            # Survey-files\n",
        "            if len(df_pivot) > 0:\n",
        "                df_pivot.to_csv(f\"{output_dir_survey}agent_survey_results_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "            if len(df_pre_learning_pivot) > 0:\n",
        "                # Save Pre-Learning Results as Excel\n",
        "                df_pre_learning_pivot.to_excel(f\"{output_dir_survey}agent_survey_results_pre_learning_{base_timestamp}_{checkpoint_suffix}.xlsx\", index=False)\n",
        "\n",
        "            # Save Brand Analysis Tracking as Excel\n",
        "            if len(df_brand_analysis_tracking) > 0:\n",
        "                df_brand_analysis_tracking.to_excel(f\"{output_dir_survey}agent_brand_analysis_impact_{base_timestamp}_{checkpoint_suffix}.xlsx\", index=False)\n",
        "\n",
        "            if len(df_reasoning) > 0:\n",
        "                df_reasoning.to_csv(f\"{output_dir_survey}agent_survey_reasoning_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "            if len(df_survey_messages) > 0:\n",
        "                df_survey_messages.to_csv(f\"{output_dir_survey}agent_survey_communications_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "\n",
        "            # Bidding-files\n",
        "            if len(df_bidding) > 0:\n",
        "                df_bidding.to_csv(f\"{output_dir_bidding}agent_bidding_results_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "            if len(df_bidding_messages) > 0:\n",
        "                df_bidding_messages.to_csv(f\"{output_dir_bidding}agent_bidding_communications_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "\n",
        "            # Learning-files\n",
        "            if len(df_feedback_messages) > 0:\n",
        "                df_feedback_messages.to_csv(f\"{output_dir_learning}agent_feedback_messages_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "\n",
        "            with open(f\"{output_dir_learning}agent_learning_summary_{base_timestamp}_{checkpoint_suffix}.json\", \"w\") as f:\n",
        "                json.dump(learning_summary, f, indent=2)\n",
        "\n",
        "            print(f\"   ‚úÖ Checkpoint {checkpoint_num} saved!\")\n",
        "            print(f\"      Survey Results (with Learning): {len(df_pivot)} Entries\")\n",
        "            print(f\"      Survey Results (Pre-Learning): {len(df_pre_learning_pivot)} Entries\")\n",
        "            print(f\"      Brand Analysis Impact: {len(df_brand_analysis_tracking)} Entries\")\n",
        "            print(f\"      Reasoning Data: {len(df_reasoning)} Entries\")\n",
        "            print(f\"      Bidding Results: {len(df_bidding)} bids\")\n",
        "            print(f\"      Feedback Messages: {len(df_feedback_messages)} Messages\")\n",
        "\n",
        "        # Short break between agents\n",
        "        if agent_idx < total_agents - 1:\n",
        "            print(\"\\n   ‚è≥ Waiting 2 seconds...\")\n",
        "            await asyncio.sleep(2)\n",
        "\n",
        "    # FINAL RESULTS\n",
        "    print(\"\\n\\nüìä Create final result DataFrames...\")\n",
        "\n",
        "    # Create final DataFrames\n",
        "    df_survey_results = pd.DataFrame(all_survey_results)\n",
        "    df_pivot = df_survey_results.pivot_table(\n",
        "        index=['username', 'brand'],\n",
        "        columns='question',\n",
        "        values='rating'\n",
        "    ).reset_index()\n",
        "\n",
        "    # Pre-Learning Results\n",
        "    df_pre_learning_results = pd.DataFrame(all_pre_learning_results)\n",
        "    df_pre_learning_pivot = df_pre_learning_results.pivot_table(\n",
        "        index=['username', 'brand'],\n",
        "        columns='question',\n",
        "        values='rating'\n",
        "    ).reset_index()\n",
        "\n",
        "    # Brand Analysis Tracking\n",
        "    df_brand_analysis_tracking = pd.DataFrame(all_brand_analysis_tracking)\n",
        "\n",
        "    df_reasoning = pd.DataFrame(all_reasoning_data)\n",
        "    df_survey_messages = pd.DataFrame(all_survey_messages)\n",
        "    df_bidding_messages = pd.DataFrame(all_bidding_messages)\n",
        "    df_feedback_messages = pd.DataFrame(all_feedback_messages)\n",
        "    df_bidding = pd.DataFrame(all_bidding_results)\n",
        "\n",
        "    # Final Learning Summary\n",
        "    learning_summary = {\n",
        "        \"survey_adjustments\": shared_learning_memory.learning_feedback[\"survey_adjustments\"],\n",
        "        \"bidding_adjustments\": shared_learning_memory.learning_feedback[\"bidding_adjustments\"],\n",
        "        \"wisdom_count\": len(shared_learning_memory.learning_feedback[\"accumulated_wisdom\"]),\n",
        "        \"final_insights\": shared_learning_memory.learning_feedback[\"accumulated_wisdom\"][-5:]\n",
        "    }\n",
        "\n",
        "    # Save final results\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Survey files\n",
        "    df_pivot.to_csv(f\"{output_dir_survey}agent_survey_results_{timestamp}_final.csv\", index=False)\n",
        "    df_pre_learning_pivot.to_excel(f\"{output_dir_survey}agent_survey_results_pre_learning_{timestamp}_final.xlsx\", index=False)\n",
        "\n",
        "    # Brand Analysis Impact as a final Excel file\n",
        "    if len(df_brand_analysis_tracking) > 0:\n",
        "        df_brand_analysis_tracking.to_excel(f\"{output_dir_survey}agent_brand_analysis_impact_{timestamp}_final.xlsx\", index=False)\n",
        "\n",
        "    df_reasoning.to_csv(f\"{output_dir_survey}agent_survey_reasoning_{timestamp}_final.csv\", index=False)\n",
        "    df_survey_messages.to_csv(f\"{output_dir_survey}agent_survey_communications_{timestamp}_final.csv\", index=False)\n",
        "\n",
        "    # Bidding-files\n",
        "    df_bidding.to_csv(f\"{output_dir_bidding}agent_bidding_results_{timestamp}_final.csv\", index=False)\n",
        "    df_bidding_messages.to_csv(f\"{output_dir_bidding}agent_bidding_communications_{timestamp}_final.csv\", index=False)\n",
        "\n",
        "    # Learning-files\n",
        "    df_feedback_messages.to_csv(f\"{output_dir_learning}agent_feedback_messages_{timestamp}_final.csv\", index=False)\n",
        "    with open(f\"{output_dir_learning}agent_learning_summary_{timestamp}_final.json\", \"w\") as f:\n",
        "        json.dump(learning_summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nüíæ Alle finalen Ergebnisse gespeichert!\")\n",
        "    print(f\"   Survey Results (mit Learning): {len(df_pivot)} Eintr√§ge\")\n",
        "    print(f\"   Survey Results (Pre-Learning): {len(df_pre_learning_pivot)} Eintr√§ge\")\n",
        "    print(f\"   Brand Analysis Impact: {len(df_brand_analysis_tracking)} Eintr√§ge\")\n",
        "    print(f\"   Reasoning Data: {len(df_reasoning)} Eintr√§ge\")\n",
        "    print(f\"   Survey Messages: {len(df_survey_messages)} Nachrichten\")\n",
        "    print(f\"   Bidding Results: {len(df_bidding)} Gebote\")\n",
        "    print(f\"   Bidding Messages: {len(df_bidding_messages)} Nachrichten\")\n",
        "    print(f\"   Feedback Messages: {len(df_feedback_messages)} Feedback-Nachrichten\")\n",
        "    print(f\"   Learning Summary: {len(learning_summary['survey_adjustments'])} Survey-Muster, {len(learning_summary['bidding_adjustments'])} Bidding-Muster\")\n",
        "    print(f\"   Gesamtzeit: {time.time() - start_time:.1f} Sekunden\")\n",
        "\n",
        "    # Show Brand Analysis Impact statistics\n",
        "    if len(df_brand_analysis_tracking) > 0:\n",
        "        print(\"\\nüìä Brand Analysis Impact statistics:\")\n",
        "        total_challenges = df_brand_analysis_tracking['challenge_received'].sum()\n",
        "        total_rating_changes = (df_brand_analysis_tracking['rating_change'] != 0).sum()\n",
        "        avg_rating_change = df_brand_analysis_tracking['rating_change'].mean()\n",
        "\n",
        "        print(f\"   Challenges: {total_challenges}\")\n",
        "        print(f\"   Rating-changes: {total_rating_changes}\")\n",
        "        print(f\"   Average Rating-change: {avg_rating_change:.2f}\")\n",
        "\n",
        "        # Rating changes per brand\n",
        "        print(\"\\n   Rating changes per brand:\")\n",
        "        brand_changes = df_brand_analysis_tracking.groupby('brand')['rating_change'].agg(['count', 'mean', 'sum'])\n",
        "        print(brand_changes)\n",
        "\n",
        "    # Show statistics\n",
        "    if len(df_bidding) > 0:\n",
        "        print(\"\\nüìä Bidding statistics:\")\n",
        "        print(f\"   Average bid: ‚Ç¨{df_bidding['bid'].mean():.2f}\")\n",
        "        print(f\"   Bid range: ‚Ç¨{df_bidding['bid'].min():.2f} - ‚Ç¨{df_bidding['bid'].max():.2f}\")\n",
        "\n",
        "        print(\"\\nüìà Average bids per brand:\")\n",
        "        brand_stats = df_bidding.groupby('brand')['bid'].agg(['mean', 'std', 'count'])\n",
        "        print(brand_stats)\n",
        "\n",
        "        # Correlation between attachment and bids\n",
        "        print(\"\\nüîó Correlation Attachment ‚Üí Bids:\")\n",
        "        # Calculate correlations from collected data\n",
        "        correlations = {}\n",
        "        for brand in brands:\n",
        "            brand_bids = df_bidding[df_bidding['brand'] == brand]\n",
        "            if len(brand_bids) > 0:\n",
        "                # Collect attachment scores from survey data\n",
        "                attachments = []\n",
        "                bids = []\n",
        "\n",
        "                for _, bid_row in brand_bids.iterrows():\n",
        "                    username = bid_row['username']\n",
        "                    # Get attachment data from Survey Results\n",
        "                    user_survey = df_pivot[df_pivot['username'] == username]\n",
        "                    if len(user_survey) > 0:\n",
        "                        brand_survey = user_survey[user_survey['brand'] == brand]\n",
        "                        if len(brand_survey) > 0:\n",
        "                            # Calculate average attachment score\n",
        "                            attachment_cols = [col for col in df_pivot.columns if col.startswith('Q11_')]\n",
        "                            if attachment_cols:\n",
        "                                attachment_values = brand_survey[attachment_cols].values[0]\n",
        "                                # Filter NaN values\n",
        "                                valid_values = [v for v in attachment_values if pd.notna(v)]\n",
        "                                if valid_values:\n",
        "                                    avg_attachment = (sum(valid_values) / len(valid_values) - 1) / 4  # Convert 1-5 to 0-1\n",
        "                                    attachments.append(avg_attachment)\n",
        "                                    bids.append(bid_row['bid'])\n",
        "\n",
        "                if len(attachments) > 1:\n",
        "                    correlation = pd.Series(attachments).corr(pd.Series(bids))\n",
        "                    correlations[brand] = correlation\n",
        "                    print(f\"   {brand}: r={correlation:.3f}\")\n",
        "\n",
        "    # Show final Learning Insights\n",
        "    print(\"\\nüß† FINAL LEARNING INSIGHTS:\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nüìä Survey Learning Patterns:\")\n",
        "    for q_type, adj in learning_summary[\"survey_adjustments\"].items():\n",
        "        if adj[\"samples\"] > 0:\n",
        "            print(f\"   {q_type}: {adj['avg_diff']:+.2f} (based on {adj['samples']} Samples)\")\n",
        "\n",
        "    print(\"\\nüí∞ Bidding Learning Patterns:\")\n",
        "    for brand, adj in learning_summary[\"bidding_adjustments\"].items():\n",
        "        if adj[\"samples\"] > 0:\n",
        "            print(f\"   {brand}: ‚Ç¨{adj['avg_diff']:+.2f} (based on {adj['samples']} Samples)\")\n",
        "\n",
        "    print(\"\\nüí° Top Accumulated Wisdom:\")\n",
        "    for idx, wisdom in enumerate(learning_summary.get(\"final_insights\", [])[-3:], 1):\n",
        "        if isinstance(wisdom, dict) and \"insight\" in wisdom:\n",
        "            insight = wisdom[\"insight\"]\n",
        "            if isinstance(insight, dict) and \"general_tendency\" in insight:\n",
        "                print(f\"   {idx}. {insight['general_tendency']}\")\n",
        "\n",
        "    print(\"\\nüéâ AGENT STUDY DONE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nüìÇ Saved files:\")\n",
        "    print(f\"   Final results:\")\n",
        "    print(f\"   - Survey Results (with Learning): agent_survey_results_{timestamp}_final.csv\")\n",
        "    print(f\"   - Survey Results (Pre-Learning): agent_survey_results_pre_learning_{timestamp}_final.xlsx\")\n",
        "    print(f\"   - Brand Analysis Impact: agent_brand_analysis_impact_{timestamp}_final.xlsx\")\n",
        "    print(f\"   - Survey Reasoning: agent_survey_reasoning_{timestamp}_final.csv\")\n",
        "    print(f\"   - Survey Communications: agent_survey_communications_{timestamp}_final.csv\")\n",
        "    print(f\"   - Bidding Results: agent_bidding_results_{timestamp}_final.csv\")\n",
        "    print(f\"   - Bidding Communications: agent_bidding_communications_{timestamp}_final.csv\")\n",
        "    print(f\"   - Feedback Messages: agent_feedback_messages_{timestamp}_final.csv\")\n",
        "    print(f\"   - Learning Summary: agent_learning_summary_{timestamp}_final.json\")\n",
        "    print(f\"\\n   Plus {(total_agents // save_interval)} Checkpoint files for incremental analysis\")\n",
        "\n",
        "    return df_pivot, df_pre_learning_pivot, df_reasoning, df_bidding, learning_summary, df_brand_analysis_tracking\n",
        "\n",
        "\n",
        "async def run_complete_agent_survey_for_single_agent(profile: dict, memory: EnhancedMemoryModule, shared_memory: EnhancedMemoryModule):\n",
        "    \"\"\"Conducts the complete survey for a single agent and all brands\"\"\"\n",
        "\n",
        "    agent_name = profile[\"username\"]\n",
        "    survey_results = []\n",
        "    pre_learning_results = []\n",
        "    reasoning_data = []\n",
        "    agent_messages = []\n",
        "    brand_analysis_tracking = []\n",
        "\n",
        "    # Dictionary for agent responses (for feedback)\n",
        "    agent_responses = {}\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üë§ Agent: {agent_name}\")\n",
        "    print(f\"   Actual: {' > '.join(profile['actual_traits'][:2])}...\")\n",
        "    print(f\"   Ideal: {' > '.join(profile['ideal_traits'][:2])}...\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Empty STM, CoT History and Messages before each new person\n",
        "    memory.short_term_memory.clear()\n",
        "    memory.cot_history.clear()\n",
        "    memory.agent_messages.clear()\n",
        "    memory._survey_ratings.clear()\n",
        "\n",
        "    # Survey for every brand\n",
        "    for brand in brands:\n",
        "        print(f\"\\n   üìã {brand} Survey with agent:\")\n",
        "\n",
        "        try:\n",
        "            # Updated Call with Brand Analysis Tracking\n",
        "            results, pre_learning, brand_analysis = await run_agent_survey(profile, brand, memory, shared_memory)\n",
        "\n",
        "            # Collect data for DataFrame\n",
        "            for question_code in [\"Q9_consistent\", \"Q9_mirror\", \"Q10_consistent\", \"Q10_mirror\"] + [f\"Q11_{item}\" for item in attachment_items]:\n",
        "                rating = results[question_code]\n",
        "                pre_learning_rating = pre_learning[question_code]\n",
        "\n",
        "                # Save for feedback\n",
        "                agent_responses[question_code] = rating\n",
        "\n",
        "                survey_results.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"brand\": brand,\n",
        "                    \"question\": question_code,\n",
        "                    \"rating\": rating,\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "\n",
        "                # Save Pre-Learning Results\n",
        "                pre_learning_results.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"brand\": brand,\n",
        "                    \"question\": question_code,\n",
        "                    \"rating\": pre_learning_rating,\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "\n",
        "                # Find the corresponding reasoning from the memory\n",
        "                reasoning_entry = next(\n",
        "                    (m for m in memory.short_term_memory\n",
        "                     if m.get(\"question\", \"\").endswith(question_code) and m.get(\"brand\") == brand),\n",
        "                    None\n",
        "                )\n",
        "\n",
        "                if reasoning_entry:\n",
        "                    reasoning_data.append({\n",
        "                        \"username\": agent_name,\n",
        "                        \"brand\": brand,\n",
        "                        \"question\": question_code,\n",
        "                        \"rating\": rating,\n",
        "                        \"pre_learning_rating\": pre_learning_rating,\n",
        "                        \"reasoning\": reasoning_entry[\"reasoning\"],\n",
        "                        \"confidence\": reasoning_entry.get(\"confidence\", None)\n",
        "                    })\n",
        "\n",
        "            # Collect Brand Analysis Tracking\n",
        "            brand_analysis_tracking.extend(brand_analysis)\n",
        "\n",
        "            # Collect agent messages for this brand\n",
        "            brand_messages = [msg for msg in memory.agent_messages\n",
        "                            if any(brand in str(msg.content) for brand in [brand])]\n",
        "\n",
        "            for msg in brand_messages:\n",
        "                agent_messages.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"brand\": brand,\n",
        "                    \"from_agent\": msg.from_agent,\n",
        "                    \"to_agent\": msg.to_agent,\n",
        "                    \"message_type\": msg.message_type.value,\n",
        "                    \"priority\": msg.priority,\n",
        "                    \"content_summary\": msg.content.get(\"summary\", \"\"),\n",
        "                    \"timestamp\": msg.timestamp\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error {brand}: {str(e)}\")\n",
        "\n",
        "    # Return Brand Analysis Tracking with\n",
        "    return survey_results, pre_learning_results, reasoning_data, agent_messages, agent_responses, brand_analysis_tracking\n",
        "\n",
        "print(\"‚úÖ Sequential Agent Study runner defined!\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ Ready to start\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nThe agent system processes each agent sequentially:\")\n",
        "print(\"   1Ô∏è‚É£ Create profile & memory ‚Üí 2Ô∏è‚É£ Survey ‚Üí 3Ô∏è‚É£ Bidding ‚Üí 4Ô∏è‚É£ Feedback ‚Üí 5Ô∏è‚É£ Learning Update\")\n",
        "print(\"\\nüíæ Automatic saving every 10 agents\")\n",
        "print(\"üìä Brand Analysis Impact Tracking as Excel-Export\")\n",
        "\n",
        "# Start\n",
        "survey_results, pre_learning_results, reasoning_results, bidding_results, learning_summary, brand_analysis_tracking = await run_complete_agent_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### The following code parts are necessary to continue the simulation in branches after it has been interrupted ###"
      ],
      "metadata": {
        "id": "Izn3dKrtkkUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL A: Setup and Profile-Loading in branches\n",
        "# =============================================================================\n",
        "\n",
        "import os, re, json, nest_asyncio, pandas as pd\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "start_idx = 180  # Enter the first profile here that has not yet been translated, e.g., 180 here\n",
        "\n",
        "nest_asyncio.apply()\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# UniGPT-Client\n",
        "os.environ[\"OPENAI_API_KEY\"]  = \"sk-D7C8BrpeoPqd7qocibeI5Q\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://gpt.uni-muenster.de/v1\"\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key  = os.environ[\"OPENAI_API_KEY\"],\n",
        "    base_url = os.environ[\"OPENAI_API_BASE\"]\n",
        ")\n",
        "model = \"Llama-3.3-70B\"\n",
        "print(f\"‚úÖ LLM Client configured: {model}\")\n",
        "\n",
        "# Import profile data from Excel\n",
        "print(\"\\nüìä Load profiles from Excel...\")\n",
        "df_profiles = pd.read_excel('/content/drive/MyDrive/profiles_survey/All_profiles.xlsx')\n",
        "print(f\"   Found: {len(df_profiles)} Profile\")\n",
        "\n",
        "# Helper function for filtering empty traits\n",
        "def filter_traits(row, trait_type, max_count=4):\n",
        "    \"\"\"Filters empty/NaN traits from a row\"\"\"\n",
        "    traits = []\n",
        "    for i in range(1, max_count + 1):\n",
        "        trait_value = row[f\"{trait_type}_{i}\"]\n",
        "        # Check for NaN, None, empty strings or whitespace only\n",
        "        if pd.notna(trait_value) and str(trait_value).strip():\n",
        "            traits.append(str(trait_value).strip())\n",
        "    return traits\n",
        "\n",
        "# Convert profiles to structured format\n",
        "profiles = []\n",
        "for _, row in df_profiles.iterrows():\n",
        "    actual_traits = filter_traits(row, \"actual\")\n",
        "    ideal_traits = filter_traits(row, \"ideal\")\n",
        "\n",
        "    profiles.append({\n",
        "        \"age\":                  int(row.age),\n",
        "        \"income_range\":         row.income,\n",
        "        \"occupation\":           row.occupation,\n",
        "        \"gender\":               row.gender,\n",
        "        \"actual_traits\":        actual_traits,\n",
        "        \"actual_top_strength\":  row.actual_top_strength,\n",
        "        \"ideal_traits\":         ideal_traits,\n",
        "        \"ideal_top_strength\":   row.ideal_top_strength,\n",
        "        \"username\":             f\"{row.username} Agent\"\n",
        "    })\n",
        "\n",
        "# Translation function\n",
        "def translate_to_english(text: str) -> str:\n",
        "    \"\"\"Translates text into English (if necessary)\"\"\"\n",
        "    res = client.chat.completions.create(\n",
        "        model   = model,\n",
        "        messages= [\n",
        "            {\"role\":\"system\",\"content\":\"You are a precise translator. If the input is already in English, return it exactly as-is. Otherwise translate literally without introducing synonyms or rephrasings.\"},\n",
        "            {\"role\":\"user\",\"content\":f\"Translate into English (or return unchanged): \\\"{text}\\\"\"}\n",
        "        ],\n",
        "        temperature=0.1\n",
        "    )\n",
        "    return res.choices[0].message.content.strip()\n",
        "\n",
        "# Pre-processing function for profiles\n",
        "def preprocess_profile(profile: dict) -> dict:\n",
        "    \"\"\"Translates all traits of a profile into English\"\"\"\n",
        "    p = dict(profile)\n",
        "    p[\"actual_traits\"]       = [translate_to_english(t) for t in p[\"actual_traits\"]]\n",
        "    p[\"actual_top_strength\"] = translate_to_english(p[\"actual_top_strength\"])\n",
        "    p[\"ideal_traits\"]        = [translate_to_english(t) for t in p[\"ideal_traits\"]]\n",
        "    p[\"ideal_top_strength\"]  = translate_to_english(p[\"ideal_top_strength\"])\n",
        "    return p\n",
        "\n",
        "# Only translate the profiles from index x onwards\n",
        "print(f\"\\nüåê Translate profiles from index {start_idx} into English...\")\n",
        "profiles_en = []                  # Leere Liste (falls neu gestartet)\n",
        "\n",
        "for i, p in enumerate(profiles[start_idx:], start=start_idx):\n",
        "    print(f\"   Process {i+1}/{len(profiles)}: {p['username']}...\", end=\" \")\n",
        "    profiles_en.append(preprocess_profile(p))\n",
        "    print(\"‚úì\")\n",
        "\n",
        "print(f\"\\n‚úÖ Total translated so far {len(profiles_en)} Profils (after Index {start_idx})\")\n",
        "\n",
        "\n",
        "# Reflection functions\n",
        "def reflect_profile(profile: dict) -> str:\n",
        "    \"\"\"Creates a compact psychological summary of the profiles\"\"\"\n",
        "    system_msg = (\n",
        "        \"You are a psychologist. Create a SINGLE PARAGRAPH psychological summary \"\n",
        "        \"that integrates the person's actual self, ideal self, actual top strength, ideal top strength and demographics \"\n",
        "        \"into a cohesive personality profile. Focus on the key psychological dynamics \"\n",
        "        \"and self-concept. Keep it concise but insightful.\"\n",
        "    )\n",
        "\n",
        "    user_content = (\n",
        "        f\"Profile:\\n\"\n",
        "        f\"- Age: {profile['age']}, Gender: {profile['gender']}\\n\"\n",
        "        f\"- Occupation: {profile['occupation']}, Income: {profile['income_range']}\\n\"\n",
        "        f\"- Actual Self: {', '.join(profile['actual_traits'])} (top: {profile['actual_top_strength']})\\n\"\n",
        "        f\"- Ideal Self: {', '.join(profile['ideal_traits'])} (top: {profile['ideal_top_strength']})\\n\"\n",
        "    )\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model    = model,\n",
        "        messages = [\n",
        "            {\"role\":\"system\", \"content\": system_msg},\n",
        "            {\"role\":\"user\",   \"content\": user_content}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=500\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "def reflect_actual_self(profile: dict) -> str:\n",
        "    \"\"\"Creates a psychological expert reflection on the Actual Self\"\"\"\n",
        "    system_msg = (\n",
        "        \"You are a psychology expert. Analyze how this person sees themselves (actual self) \"\n",
        "        \"based on their traits and top strength. Consider that traits are listed in descending order of importance. \"\n",
        "        \"Focus on their self-perception, self-concept, and how they view their current personality. \"\n",
        "        \"Keep it focused and concise.\"\n",
        "    )\n",
        "\n",
        "    user_content = (\n",
        "        f\"Actual Self Analysis:\\n\"\n",
        "        f\"- Actual traits (in order of importance): {', '.join(profile['actual_traits'])}\\n\"\n",
        "        f\"- Actual top strength: {profile['actual_top_strength']}\\n\"\n",
        "        f\"How does this person see themselves? What is their self-concept?\"\n",
        "    )\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model    = model,\n",
        "        messages = [\n",
        "            {\"role\":\"system\", \"content\": system_msg},\n",
        "            {\"role\":\"user\",   \"content\": user_content}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "def reflect_ideal_self(profile: dict) -> str:\n",
        "    \"\"\"Creates a psychological expert reflection on the Ideal Self\"\"\"\n",
        "    system_msg = (\n",
        "        \"You are a psychology expert. Analyze this person's ideal self and aspirations \"\n",
        "        \"based on their ideal traits and top ideal strength. Consider that traits are listed in descending order of importance. \"\n",
        "        \"Focus on their aspirations, what they want to become, and gaps between actual and ideal self. \"\n",
        "        \"Keep it focused and concise.\"\n",
        "    )\n",
        "\n",
        "    user_content = (\n",
        "        f\"Ideal Self Analysis:\\n\"\n",
        "        f\"- Ideal traits (in order of importance): {', '.join(profile['ideal_traits'])}\\n\"\n",
        "        f\"- Ideal top strength: {profile['ideal_top_strength']}\\n\"\n",
        "        f\"What does this person aspire to be? What are their self-improvement goals?\"\n",
        "    )\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model    = model,\n",
        "        messages = [\n",
        "            {\"role\":\"system\", \"content\": system_msg},\n",
        "            {\"role\":\"user\",   \"content\": user_content}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "# Initialize helper tool orchestrators (global, one-time)\n",
        "print(\"\\nü§ñ Initialize Helper Tool Orchestrators...\")\n",
        "# Renaming\n",
        "brand_personality_agent = BrandPersonalityToolOrchestrator()\n",
        "price_research_agent = PriceResearchToolOrchestrator()\n",
        "feedback_agent = FeedbackToolOrchestrator()\n",
        "\n",
        "print(\"\\n‚úÖ Setup done!\")\n",
        "print(f\"   Number Profiles: {len(profiles_en)}\")\n",
        "print(\"   Helper Tool Orchestrators: Ready\")\n",
        "\n",
        "brands = [\"Nike\", \"Apple\", \"Levi's\"]"
      ],
      "metadata": {
        "id": "W6rfOP6XndvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL B: Access to shared memory\n",
        "# =============================================================================\n",
        "\n",
        "##the checkpoint files need to get pasted in here\n",
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# 1. Create a shared learning memory again\n",
        "shared_learning_memory = EnhancedMemoryModule(\n",
        "    agent_name=\"SharedLearning\",\n",
        "    max_short_term_size=100\n",
        ")\n",
        "\n",
        "# 2. List of the checkpoint files\n",
        "paths = [\n",
        "    \"/content/drive/MyDrive/agent_learning_results/\"\n",
        "    \"agent_learning_summary_20250628_193840_checkpoint4_40agents.json\",\n",
        "    \"/content/drive/MyDrive/agent_learning_results/\"\n",
        "    \"agent_learning_summary_20250629_090026_checkpoint3_30agents.json\"\n",
        "]\n",
        "\n",
        "# 3. Auxiliary structures for merging\n",
        "merged_survey  = {}\n",
        "merged_bidding = {}\n",
        "merged_wisdom  = []\n",
        "\n",
        "for path in paths:\n",
        "    with open(path, \"r\") as f:\n",
        "        chk = json.load(f)\n",
        "    #  Survey adjustments\n",
        "    for qtype, adj in chk[\"survey_adjustments\"].items():\n",
        "        total = adj[\"avg_diff\"] * adj[\"samples\"]\n",
        "        if qtype not in merged_survey:\n",
        "            merged_survey[qtype] = {\"total_diff\": total, \"samples\": adj[\"samples\"]}\n",
        "        else:\n",
        "            merged_survey[qtype][\"total_diff\"] += total\n",
        "            merged_survey[qtype][\"samples\"]    += adj[\"samples\"]\n",
        "    # Merging Bidding adjustments\n",
        "    for brand, adj in chk[\"bidding_adjustments\"].items():\n",
        "        total = adj[\"avg_diff\"] * adj[\"samples\"]\n",
        "        if brand not in merged_bidding:\n",
        "            merged_bidding[brand] = {\"total_diff\": total, \"samples\": adj[\"samples\"]}\n",
        "        else:\n",
        "            merged_bidding[brand][\"total_diff\"] += total\n",
        "            merged_bidding[brand][\"samples\"]    += adj[\"samples\"]\n",
        "    # Attach wisdom\n",
        "    merged_wisdom.extend(chk.get(\"accumulated_wisdom\", []))\n",
        "\n",
        "# 4. Recalculate avg_diff\n",
        "for d in merged_survey.values():\n",
        "    d[\"avg_diff\"] = d[\"total_diff\"] / d[\"samples\"]\n",
        "for d in merged_bidding.values():\n",
        "    d[\"avg_diff\"] = d[\"total_diff\"] / d[\"samples\"]\n",
        "\n",
        "# 5. Save to shared memory\n",
        "shared_learning_memory.learning_feedback[\"survey_adjustments\"]  = merged_survey\n",
        "shared_learning_memory.learning_feedback[\"bidding_adjustments\"] = merged_bidding\n",
        "shared_learning_memory.learning_feedback[\"accumulated_wisdom\"]  = merged_wisdom\n",
        "\n",
        "print(\"‚úÖ Merged Learning Memory with\",\n",
        "      f\"{sum(d['samples'] for d in merged_survey.values())} Surveys,\",\n",
        "      f\"{sum(d['samples'] for d in merged_bidding.values())} Bids,\",\n",
        "      f\"{len(merged_wisdom)} Wisdom-Entrances\")\n"
      ],
      "metadata": {
        "id": "A7uAqlJ_eHdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL C: Starting the study again\n",
        "# =============================================================================\n",
        "\n",
        "async def run_complete_agent_experiment():\n",
        "    \"\"\"Runs the complete study\"\"\"\n",
        "\n",
        "    print(\"üöÄ Start study\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Collection storage for all results\n",
        "    all_survey_results = []\n",
        "    all_pre_learning_results = []\n",
        "    all_reasoning_data = []\n",
        "    all_survey_messages = []\n",
        "    all_bidding_results = []\n",
        "    all_bidding_messages = []\n",
        "    all_feedback_messages = []\n",
        "    all_brand_analysis_tracking = []\n",
        "\n",
        "    total_agents = len(profiles_en)\n",
        "    start_time = time.time()\n",
        "    save_interval = 10  # Save all 10 agents\n",
        "\n",
        "    # Output directories\n",
        "    output_dir_survey = \"/content/drive/MyDrive/agent_survey_results/\"\n",
        "    output_dir_bidding = \"/content/drive/MyDrive/agent_bidding_results/\"\n",
        "    output_dir_learning = \"/content/drive/MyDrive/agent_learning_results/\"\n",
        "\n",
        "    import os\n",
        "    os.makedirs(output_dir_survey, exist_ok=True)\n",
        "    os.makedirs(output_dir_bidding, exist_ok=True)\n",
        "    os.makedirs(output_dir_learning, exist_ok=True)\n",
        "\n",
        "    # Base timestamp for consistent file names\n",
        "    base_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # MAIN LOOP:\n",
        "    for agent_idx, profile in enumerate(profiles_en):\n",
        "\n",
        "        agent_name = profile[\"username\"]\n",
        "\n",
        "        print(f\"\\n{'#'*80}\")\n",
        "        print(f\"# AGENT {agent_idx + 1}/{total_agents}: {agent_name}\")\n",
        "        print(f\"{'#'*80}\")\n",
        "\n",
        "        # STEP 1: Profile processing and reflections\n",
        "        print(f\"\\nüîÆ PHASE 1: PROFILE PROCESSING for {agent_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Create memory module for this agent\n",
        "        print(f\"   üß† Create memory module for {agent_name}...\")\n",
        "        memory = EnhancedMemoryModule(\n",
        "            agent_name=agent_name,\n",
        "            max_short_term_size=20\n",
        "        )\n",
        "\n",
        "        # Use global embedding model for efficiency\n",
        "        if 'embeddings_model' not in globals():\n",
        "            print(\"   üì• Load Embedding-Modell...\")\n",
        "            from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "            embeddings_model = HuggingFaceEmbeddings(\n",
        "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "            )\n",
        "        memory.embeddings = embeddings_model\n",
        "\n",
        "        # Create psychological reflections\n",
        "        print(\"   üîÆ Create psychological summaries...\")\n",
        "        general_summary = reflect_profile(profile)\n",
        "        actual_self_summary = reflect_actual_self(profile)\n",
        "        ideal_self_summary = reflect_ideal_self(profile)\n",
        "\n",
        "        # Save profile and all reflections\n",
        "        memory.store_profile(profile, general_summary, actual_self_summary, ideal_self_summary)\n",
        "\n",
        "        print(f\"   üìù General: {general_summary[:100]}...\")\n",
        "        print(f\"   üéØ Actual Self: {actual_self_summary[:80]}...\")\n",
        "        print(f\"   ‚≠ê Ideal Self: {ideal_self_summary[:80]}...\")\n",
        "        print(f\"   ‚úÖ Memory Module for {agent_name} created!\")\n",
        "\n",
        "        # Dictionary for all agent responses\n",
        "        all_agent_responses = {}\n",
        "\n",
        "        # STEP 2: Survey for this agent\n",
        "        print(f\"\\nüìã PHASE 2: SURVEY for {agent_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Updated Call with Brand Analysis Tracking\n",
        "        agent_survey_results, agent_pre_learning_results, agent_reasoning_data, agent_survey_messages, agent_survey_responses, agent_brand_analysis_tracking = \\\n",
        "            await run_complete_agent_survey_for_single_agent(profile, memory, shared_learning_memory)\n",
        "\n",
        "        # Collect Survey-results\n",
        "        all_survey_results.extend(agent_survey_results)\n",
        "        all_pre_learning_results.extend(agent_pre_learning_results)\n",
        "        all_reasoning_data.extend(agent_reasoning_data)\n",
        "        all_survey_messages.extend(agent_survey_messages)\n",
        "        all_brand_analysis_tracking.extend(agent_brand_analysis_tracking)\n",
        "\n",
        "        # Update agent responses dictionary\n",
        "        all_agent_responses.update(agent_survey_responses)\n",
        "\n",
        "        print(f\"\\n‚úÖ Survey for {agent_name} done!\")\n",
        "\n",
        "        # Show Brand Attachment Scores\n",
        "        print(\"   üìä Brand Attachment Scores:\")\n",
        "        for brand in brands:\n",
        "            attachment = calculate_brand_attachment_score(memory, brand)\n",
        "            print(f\"      - {brand}: {attachment:.2%}\")\n",
        "\n",
        "        # STEP 3: Bidding for this agent\n",
        "        print(f\"\\nüõí PHASE 3: Bidding for {agent_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Clear agent messages for bidding\n",
        "        memory.agent_messages.clear()\n",
        "\n",
        "        try:\n",
        "            bidding_results = await run_agent_shop_bot(profile, memory, shared_learning_memory)\n",
        "            all_bidding_results.extend(bidding_results)\n",
        "\n",
        "            # Add bidding results to agent_responses\n",
        "            for bid_result in bidding_results:\n",
        "                brand = bid_result[\"brand\"]\n",
        "                all_agent_responses[f\"bid_{brand}\"] = bid_result[\"bid\"]\n",
        "\n",
        "            # Collect Agent Messages from bidding\n",
        "            for msg in memory.agent_messages:\n",
        "                all_bidding_messages.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"from_agent\": msg.from_agent,\n",
        "                    \"to_agent\": msg.to_agent,\n",
        "                    \"message_type\": msg.message_type.value,\n",
        "                    \"priority\": msg.priority,\n",
        "                    \"content_summary\": msg.content.get(\"summary\", \"\"),\n",
        "                    \"timestamp\": msg.timestamp\n",
        "                })\n",
        "\n",
        "            print(f\"\\n‚úÖ Bidding for {agent_name} done!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n   ‚ùå Critical error when bidding for {agent_name}: {e!s}\")\n",
        "\n",
        "        # SCHRITT 4: FEEDBACK LOOP\n",
        "        print(f\"\\nüìä PHASE 4: FEEDBACK for {agent_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Generate feedback for each brand\n",
        "        for brand in brands:\n",
        "            feedback_msg = feedback_agent.create_feedback_message(all_agent_responses, profile, brand)\n",
        "\n",
        "            # Save feedback in shared memory\n",
        "            shared_learning_memory.add_learning_feedback(feedback_msg)\n",
        "\n",
        "            # Also save in the individual memory\n",
        "            memory.add_agent_message(feedback_msg)\n",
        "\n",
        "            # Collect for logging\n",
        "            all_feedback_messages.append({\n",
        "                \"username\": agent_name,\n",
        "                \"brand\": brand,\n",
        "                \"from_agent\": feedback_msg.from_agent,\n",
        "                \"to_agent\": feedback_msg.to_agent,\n",
        "                \"message_type\": feedback_msg.message_type.value,\n",
        "                \"priority\": feedback_msg.priority,\n",
        "                \"profile_match_score\": feedback_msg.content.get(\"profile_match_score\", 0),\n",
        "                \"matches_used\": feedback_msg.content.get(\"matches_used\", 0),\n",
        "                \"recommendations\": json.dumps(feedback_msg.content.get(\"recommendations\", {})),\n",
        "                \"timestamp\": feedback_msg.timestamp\n",
        "            })\n",
        "\n",
        "            print(f\"   üìö Feedback generated for {brand} and saved in the shared memory\")\n",
        "\n",
        "        # Show Learning Summary\n",
        "        print(f\"\\n   üìà Learning Summary after {agent_name}:\")\n",
        "        print(f\"      Survey Adjustments: {len(shared_learning_memory.learning_feedback['survey_adjustments'])} Pattern\")\n",
        "        print(f\"      Bidding Adjustments: {len(shared_learning_memory.learning_feedback['bidding_adjustments'])} Pattern\")\n",
        "        print(f\"      Accumulated Wisdom: {len(shared_learning_memory.learning_feedback['accumulated_wisdom'])} Insights\")\n",
        "\n",
        "        # Progress bar\n",
        "        elapsed = time.time() - start_time\n",
        "        progress = ((agent_idx + 1) / total_agents) * 100\n",
        "        eta = (elapsed / (agent_idx + 1)) * (total_agents - agent_idx - 1) if agent_idx < total_agents - 1 else 0\n",
        "\n",
        "        print(f\"\\nüìà Overall progress: {progress:.1f}% ({agent_idx + 1}/{total_agents} agents)\")\n",
        "        if eta > 0:\n",
        "            print(f\"   Estimated remaining time: {eta:.0f}s\")\n",
        "\n",
        "        # INCREMENTAL STORAGE: All 10 agents\n",
        "        if (agent_idx + 1) % save_interval == 0 or (agent_idx + 1) == total_agents:\n",
        "            print(f\"\\nüíæ Save after every {agent_idx + 1} Agent...\")\n",
        "\n",
        "            # Create DataFrames\n",
        "            df_survey_results = pd.DataFrame(all_survey_results)\n",
        "            if len(df_survey_results) > 0:\n",
        "                df_pivot = df_survey_results.pivot_table(\n",
        "                    index=['username', 'brand'],\n",
        "                    columns='question',\n",
        "                    values='rating'\n",
        "                ).reset_index()\n",
        "            else:\n",
        "                df_pivot = pd.DataFrame()\n",
        "\n",
        "            # Pre-Learning Results DataFrame\n",
        "            df_pre_learning_results = pd.DataFrame(all_pre_learning_results)\n",
        "            if len(df_pre_learning_results) > 0:\n",
        "                df_pre_learning_pivot = df_pre_learning_results.pivot_table(\n",
        "                    index=['username', 'brand'],\n",
        "                    columns='question',\n",
        "                    values='rating'\n",
        "                ).reset_index()\n",
        "            else:\n",
        "                df_pre_learning_pivot = pd.DataFrame()\n",
        "\n",
        "            # Brand Analysis Tracking DataFrame\n",
        "            df_brand_analysis_tracking = pd.DataFrame(all_brand_analysis_tracking)\n",
        "\n",
        "            # Reasoning-data with Confidence\n",
        "            df_reasoning = pd.DataFrame(all_reasoning_data)\n",
        "\n",
        "            # Agent Communication Logs\n",
        "            df_survey_messages = pd.DataFrame(all_survey_messages)\n",
        "            df_bidding_messages = pd.DataFrame(all_bidding_messages)\n",
        "            df_feedback_messages = pd.DataFrame(all_feedback_messages)\n",
        "\n",
        "            # Bidding-results\n",
        "            df_bidding = pd.DataFrame(all_bidding_results)\n",
        "\n",
        "            # Learning Summary\n",
        "            learning_summary = {\n",
        "                \"survey_adjustments\": shared_learning_memory.learning_feedback[\"survey_adjustments\"],\n",
        "                \"bidding_adjustments\": shared_learning_memory.learning_feedback[\"bidding_adjustments\"],\n",
        "                \"wisdom_count\": len(shared_learning_memory.learning_feedback[\"accumulated_wisdom\"]),\n",
        "                \"agents_processed\": agent_idx + 1,\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "            # Save with checkpoint number\n",
        "            checkpoint_num = (agent_idx + 1) // save_interval\n",
        "            checkpoint_suffix = f\"checkpoint{checkpoint_num}_{agent_idx + 1}agents\"\n",
        "\n",
        "            # Survey-files\n",
        "            if len(df_pivot) > 0:\n",
        "                df_pivot.to_csv(f\"{output_dir_survey}agent_survey_results_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "            if len(df_pre_learning_pivot) > 0:\n",
        "                # Save Pre-Learning Results as Excel\n",
        "                df_pre_learning_pivot.to_excel(f\"{output_dir_survey}agent_survey_results_pre_learning_{base_timestamp}_{checkpoint_suffix}.xlsx\", index=False)\n",
        "\n",
        "            # Save Brand Analysis Tracking as Excel\n",
        "            if len(df_brand_analysis_tracking) > 0:\n",
        "                df_brand_analysis_tracking.to_excel(f\"{output_dir_survey}agent_brand_analysis_impact_{base_timestamp}_{checkpoint_suffix}.xlsx\", index=False)\n",
        "\n",
        "            if len(df_reasoning) > 0:\n",
        "                df_reasoning.to_csv(f\"{output_dir_survey}agent_survey_reasoning_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "            if len(df_survey_messages) > 0:\n",
        "                df_survey_messages.to_csv(f\"{output_dir_survey}agent_survey_communications_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "\n",
        "            # Bidding-files\n",
        "            if len(df_bidding) > 0:\n",
        "                df_bidding.to_csv(f\"{output_dir_bidding}agent_bidding_results_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "            if len(df_bidding_messages) > 0:\n",
        "                df_bidding_messages.to_csv(f\"{output_dir_bidding}agent_bidding_communications_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "\n",
        "            # Learning-files\n",
        "            if len(df_feedback_messages) > 0:\n",
        "                df_feedback_messages.to_csv(f\"{output_dir_learning}agent_feedback_messages_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "\n",
        "            with open(f\"{output_dir_learning}agent_learning_summary_{base_timestamp}_{checkpoint_suffix}.json\", \"w\") as f:\n",
        "                json.dump(learning_summary, f, indent=2)\n",
        "\n",
        "            print(f\"   ‚úÖ Checkpoint {checkpoint_num} saved!\")\n",
        "            print(f\"      Survey Results (with Learning): {len(df_pivot)} Entries\")\n",
        "            print(f\"      Survey Results (Pre-Learning): {len(df_pre_learning_pivot)} Entries\")\n",
        "            print(f\"      Brand Analysis Impact: {len(df_brand_analysis_tracking)} Entries\")\n",
        "            print(f\"      Reasoning Data: {len(df_reasoning)} Entries\")\n",
        "            print(f\"      Bidding Results: {len(df_bidding)} bids\")\n",
        "            print(f\"      Feedback Messages: {len(df_feedback_messages)} Messages\")\n",
        "\n",
        "        # Short break between agents\n",
        "        if agent_idx < total_agents - 1:\n",
        "            print(\"\\n   ‚è≥ Waiting 2 seconds...\")\n",
        "            await asyncio.sleep(2)\n",
        "\n",
        "    # FINAL RESULTS\n",
        "    print(\"\\n\\nüìä Create final result DataFrames...\")\n",
        "\n",
        "    # Create final DataFrames\n",
        "    df_survey_results = pd.DataFrame(all_survey_results)\n",
        "    df_pivot = df_survey_results.pivot_table(\n",
        "        index=['username', 'brand'],\n",
        "        columns='question',\n",
        "        values='rating'\n",
        "    ).reset_index()\n",
        "\n",
        "    # Pre-Learning Results\n",
        "    df_pre_learning_results = pd.DataFrame(all_pre_learning_results)\n",
        "    df_pre_learning_pivot = df_pre_learning_results.pivot_table(\n",
        "        index=['username', 'brand'],\n",
        "        columns='question',\n",
        "        values='rating'\n",
        "    ).reset_index()\n",
        "\n",
        "    # Brand Analysis Tracking\n",
        "    df_brand_analysis_tracking = pd.DataFrame(all_brand_analysis_tracking)\n",
        "\n",
        "    df_reasoning = pd.DataFrame(all_reasoning_data)\n",
        "    df_survey_messages = pd.DataFrame(all_survey_messages)\n",
        "    df_bidding_messages = pd.DataFrame(all_bidding_messages)\n",
        "    df_feedback_messages = pd.DataFrame(all_feedback_messages)\n",
        "    df_bidding = pd.DataFrame(all_bidding_results)\n",
        "\n",
        "    # Final Learning Summary\n",
        "    learning_summary = {\n",
        "        \"survey_adjustments\": shared_learning_memory.learning_feedback[\"survey_adjustments\"],\n",
        "        \"bidding_adjustments\": shared_learning_memory.learning_feedback[\"bidding_adjustments\"],\n",
        "        \"wisdom_count\": len(shared_learning_memory.learning_feedback[\"accumulated_wisdom\"]),\n",
        "        \"final_insights\": shared_learning_memory.learning_feedback[\"accumulated_wisdom\"][-5:]\n",
        "    }\n",
        "\n",
        "    # Save final results\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Survey files\n",
        "    df_pivot.to_csv(f\"{output_dir_survey}agent_survey_results_{timestamp}_final.csv\", index=False)\n",
        "    df_pre_learning_pivot.to_excel(f\"{output_dir_survey}agent_survey_results_pre_learning_{timestamp}_final.xlsx\", index=False)\n",
        "\n",
        "    # Brand Analysis Impact as a final Excel file\n",
        "    if len(df_brand_analysis_tracking) > 0:\n",
        "        df_brand_analysis_tracking.to_excel(f\"{output_dir_survey}agent_brand_analysis_impact_{timestamp}_final.xlsx\", index=False)\n",
        "\n",
        "    df_reasoning.to_csv(f\"{output_dir_survey}agent_survey_reasoning_{timestamp}_final.csv\", index=False)\n",
        "    df_survey_messages.to_csv(f\"{output_dir_survey}agent_survey_communications_{timestamp}_final.csv\", index=False)\n",
        "\n",
        "    # Bidding-files\n",
        "    df_bidding.to_csv(f\"{output_dir_bidding}agent_bidding_results_{timestamp}_final.csv\", index=False)\n",
        "    df_bidding_messages.to_csv(f\"{output_dir_bidding}agent_bidding_communications_{timestamp}_final.csv\", index=False)\n",
        "\n",
        "    # Learning-files\n",
        "    df_feedback_messages.to_csv(f\"{output_dir_learning}agent_feedback_messages_{timestamp}_final.csv\", index=False)\n",
        "    with open(f\"{output_dir_learning}agent_learning_summary_{timestamp}_final.json\", \"w\") as f:\n",
        "        json.dump(learning_summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nüíæ Alle finalen Ergebnisse gespeichert!\")\n",
        "    print(f\"   Survey Results (mit Learning): {len(df_pivot)} Eintr√§ge\")\n",
        "    print(f\"   Survey Results (Pre-Learning): {len(df_pre_learning_pivot)} Eintr√§ge\")\n",
        "    print(f\"   Brand Analysis Impact: {len(df_brand_analysis_tracking)} Eintr√§ge\")\n",
        "    print(f\"   Reasoning Data: {len(df_reasoning)} Eintr√§ge\")\n",
        "    print(f\"   Survey Messages: {len(df_survey_messages)} Nachrichten\")\n",
        "    print(f\"   Bidding Results: {len(df_bidding)} Gebote\")\n",
        "    print(f\"   Bidding Messages: {len(df_bidding_messages)} Nachrichten\")\n",
        "    print(f\"   Feedback Messages: {len(df_feedback_messages)} Feedback-Nachrichten\")\n",
        "    print(f\"   Learning Summary: {len(learning_summary['survey_adjustments'])} Survey-Muster, {len(learning_summary['bidding_adjustments'])} Bidding-Muster\")\n",
        "    print(f\"   Gesamtzeit: {time.time() - start_time:.1f} Sekunden\")\n",
        "\n",
        "    # Show Brand Analysis Impact statistics\n",
        "    if len(df_brand_analysis_tracking) > 0:\n",
        "        print(\"\\nüìä Brand Analysis Impact statistics:\")\n",
        "        total_challenges = df_brand_analysis_tracking['challenge_received'].sum()\n",
        "        total_rating_changes = (df_brand_analysis_tracking['rating_change'] != 0).sum()\n",
        "        avg_rating_change = df_brand_analysis_tracking['rating_change'].mean()\n",
        "\n",
        "        print(f\"   Challenges: {total_challenges}\")\n",
        "        print(f\"   Rating-changes: {total_rating_changes}\")\n",
        "        print(f\"   Average Rating-change: {avg_rating_change:.2f}\")\n",
        "\n",
        "        # Rating changes per brand\n",
        "        print(\"\\n   Rating changes per brand:\")\n",
        "        brand_changes = df_brand_analysis_tracking.groupby('brand')['rating_change'].agg(['count', 'mean', 'sum'])\n",
        "        print(brand_changes)\n",
        "\n",
        "    # Show statistics\n",
        "    if len(df_bidding) > 0:\n",
        "        print(\"\\nüìä Bidding statistics:\")\n",
        "        print(f\"   Average bid: ‚Ç¨{df_bidding['bid'].mean():.2f}\")\n",
        "        print(f\"   Bid range: ‚Ç¨{df_bidding['bid'].min():.2f} - ‚Ç¨{df_bidding['bid'].max():.2f}\")\n",
        "\n",
        "        print(\"\\nüìà Average bids per brand:\")\n",
        "        brand_stats = df_bidding.groupby('brand')['bid'].agg(['mean', 'std', 'count'])\n",
        "        print(brand_stats)\n",
        "\n",
        "        # Correlation between attachment and bids\n",
        "        print(\"\\nüîó Correlation Attachment ‚Üí Bids:\")\n",
        "        # Calculate correlations from collected data\n",
        "        correlations = {}\n",
        "        for brand in brands:\n",
        "            brand_bids = df_bidding[df_bidding['brand'] == brand]\n",
        "            if len(brand_bids) > 0:\n",
        "                # Collect attachment scores from survey data\n",
        "                attachments = []\n",
        "                bids = []\n",
        "\n",
        "                for _, bid_row in brand_bids.iterrows():\n",
        "                    username = bid_row['username']\n",
        "                    # Get attachment data from Survey Results\n",
        "                    user_survey = df_pivot[df_pivot['username'] == username]\n",
        "                    if len(user_survey) > 0:\n",
        "                        brand_survey = user_survey[user_survey['brand'] == brand]\n",
        "                        if len(brand_survey) > 0:\n",
        "                            # Calculate average attachment score\n",
        "                            attachment_cols = [col for col in df_pivot.columns if col.startswith('Q11_')]\n",
        "                            if attachment_cols:\n",
        "                                attachment_values = brand_survey[attachment_cols].values[0]\n",
        "                                # Filter NaN values\n",
        "                                valid_values = [v for v in attachment_values if pd.notna(v)]\n",
        "                                if valid_values:\n",
        "                                    avg_attachment = (sum(valid_values) / len(valid_values) - 1) / 4  # Convert 1-5 to 0-1\n",
        "                                    attachments.append(avg_attachment)\n",
        "                                    bids.append(bid_row['bid'])\n",
        "\n",
        "                if len(attachments) > 1:\n",
        "                    correlation = pd.Series(attachments).corr(pd.Series(bids))\n",
        "                    correlations[brand] = correlation\n",
        "                    print(f\"   {brand}: r={correlation:.3f}\")\n",
        "\n",
        "    # Show final Learning Insights\n",
        "    print(\"\\nüß† FINAL LEARNING INSIGHTS:\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nüìä Survey Learning Patterns:\")\n",
        "    for q_type, adj in learning_summary[\"survey_adjustments\"].items():\n",
        "        if adj[\"samples\"] > 0:\n",
        "            print(f\"   {q_type}: {adj['avg_diff']:+.2f} (based on {adj['samples']} Samples)\")\n",
        "\n",
        "    print(\"\\nüí∞ Bidding Learning Patterns:\")\n",
        "    for brand, adj in learning_summary[\"bidding_adjustments\"].items():\n",
        "        if adj[\"samples\"] > 0:\n",
        "            print(f\"   {brand}: ‚Ç¨{adj['avg_diff']:+.2f} (based on {adj['samples']} Samples)\")\n",
        "\n",
        "    print(\"\\nüí° Top Accumulated Wisdom:\")\n",
        "    for idx, wisdom in enumerate(learning_summary.get(\"final_insights\", [])[-3:], 1):\n",
        "        if isinstance(wisdom, dict) and \"insight\" in wisdom:\n",
        "            insight = wisdom[\"insight\"]\n",
        "            if isinstance(insight, dict) and \"general_tendency\" in insight:\n",
        "                print(f\"   {idx}. {insight['general_tendency']}\")\n",
        "\n",
        "    print(\"\\nüéâ AGENT STUDY DONE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nüìÇ Saved files:\")\n",
        "    print(f\"   Final results:\")\n",
        "    print(f\"   - Survey Results (with Learning): agent_survey_results_{timestamp}_final.csv\")\n",
        "    print(f\"   - Survey Results (Pre-Learning): agent_survey_results_pre_learning_{timestamp}_final.xlsx\")\n",
        "    print(f\"   - Brand Analysis Impact: agent_brand_analysis_impact_{timestamp}_final.xlsx\")\n",
        "    print(f\"   - Survey Reasoning: agent_survey_reasoning_{timestamp}_final.csv\")\n",
        "    print(f\"   - Survey Communications: agent_survey_communications_{timestamp}_final.csv\")\n",
        "    print(f\"   - Bidding Results: agent_bidding_results_{timestamp}_final.csv\")\n",
        "    print(f\"   - Bidding Communications: agent_bidding_communications_{timestamp}_final.csv\")\n",
        "    print(f\"   - Feedback Messages: agent_feedback_messages_{timestamp}_final.csv\")\n",
        "    print(f\"   - Learning Summary: agent_learning_summary_{timestamp}_final.json\")\n",
        "    print(f\"\\n   Plus {(total_agents // save_interval)} Checkpoint files for incremental analysis\")\n",
        "\n",
        "    return df_pivot, df_pre_learning_pivot, df_reasoning, df_bidding, learning_summary, df_brand_analysis_tracking\n",
        "\n",
        "\n",
        "async def run_complete_agent_survey_for_single_agent(profile: dict, memory: EnhancedMemoryModule, shared_memory: EnhancedMemoryModule):\n",
        "    \"\"\"Conducts the complete survey for a single agent and all brands\"\"\"\n",
        "\n",
        "    agent_name = profile[\"username\"]\n",
        "    survey_results = []\n",
        "    pre_learning_results = []\n",
        "    reasoning_data = []\n",
        "    agent_messages = []\n",
        "    brand_analysis_tracking = []\n",
        "\n",
        "    # Dictionary for agent responses (for feedback)\n",
        "    agent_responses = {}\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üë§ Agent: {agent_name}\")\n",
        "    print(f\"   Actual: {' > '.join(profile['actual_traits'][:2])}...\")\n",
        "    print(f\"   Ideal: {' > '.join(profile['ideal_traits'][:2])}...\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Empty STM, CoT History and Messages before each new person\n",
        "    memory.short_term_memory.clear()\n",
        "    memory.cot_history.clear()\n",
        "    memory.agent_messages.clear()\n",
        "    memory._survey_ratings.clear()\n",
        "\n",
        "    # Survey for every brand\n",
        "    for brand in brands:\n",
        "        print(f\"\\n   üìã {brand} Survey with agent:\")\n",
        "\n",
        "        try:\n",
        "            # Updated Call with Brand Analysis Tracking\n",
        "            results, pre_learning, brand_analysis = await run_agent_survey(profile, brand, memory, shared_memory)\n",
        "\n",
        "            # Collect data for DataFrame\n",
        "            for question_code in [\"Q9_consistent\", \"Q9_mirror\", \"Q10_consistent\", \"Q10_mirror\"] + [f\"Q11_{item}\" for item in attachment_items]:\n",
        "                rating = results[question_code]\n",
        "                pre_learning_rating = pre_learning[question_code]\n",
        "\n",
        "                # Save for feedback\n",
        "                agent_responses[question_code] = rating\n",
        "\n",
        "                survey_results.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"brand\": brand,\n",
        "                    \"question\": question_code,\n",
        "                    \"rating\": rating,\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "\n",
        "                # Save Pre-Learning Results\n",
        "                pre_learning_results.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"brand\": brand,\n",
        "                    \"question\": question_code,\n",
        "                    \"rating\": pre_learning_rating,\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "\n",
        "                # Find the corresponding reasoning from the memory\n",
        "                reasoning_entry = next(\n",
        "                    (m for m in memory.short_term_memory\n",
        "                     if m.get(\"question\", \"\").endswith(question_code) and m.get(\"brand\") == brand),\n",
        "                    None\n",
        "                )\n",
        "\n",
        "                if reasoning_entry:\n",
        "                    reasoning_data.append({\n",
        "                        \"username\": agent_name,\n",
        "                        \"brand\": brand,\n",
        "                        \"question\": question_code,\n",
        "                        \"rating\": rating,\n",
        "                        \"pre_learning_rating\": pre_learning_rating,\n",
        "                        \"reasoning\": reasoning_entry[\"reasoning\"],\n",
        "                        \"confidence\": reasoning_entry.get(\"confidence\", None)\n",
        "                    })\n",
        "\n",
        "            # Collect Brand Analysis Tracking\n",
        "            brand_analysis_tracking.extend(brand_analysis)\n",
        "\n",
        "            # Collect agent messages for this brand\n",
        "            brand_messages = [msg for msg in memory.agent_messages\n",
        "                            if any(brand in str(msg.content) for brand in [brand])]\n",
        "\n",
        "            for msg in brand_messages:\n",
        "                agent_messages.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"brand\": brand,\n",
        "                    \"from_agent\": msg.from_agent,\n",
        "                    \"to_agent\": msg.to_agent,\n",
        "                    \"message_type\": msg.message_type.value,\n",
        "                    \"priority\": msg.priority,\n",
        "                    \"content_summary\": msg.content.get(\"summary\", \"\"),\n",
        "                    \"timestamp\": msg.timestamp\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error {brand}: {str(e)}\")\n",
        "\n",
        "    # Return Brand Analysis Tracking with\n",
        "    return survey_results, pre_learning_results, reasoning_data, agent_messages, agent_responses, brand_analysis_tracking\n",
        "\n",
        "print(\"‚úÖ Sequential Agent Study runner defined!\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ Ready to start\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nThe agent system processes each agent sequentially:\")\n",
        "print(\"   1Ô∏è‚É£ Create profile & memory ‚Üí 2Ô∏è‚É£ Survey ‚Üí 3Ô∏è‚É£ Bidding ‚Üí 4Ô∏è‚É£ Feedback ‚Üí 5Ô∏è‚É£ Learning Update\")\n",
        "print(\"\\nüíæ Automatic saving every 10 agents\")\n",
        "print(\"üìä Brand Analysis Impact Tracking as Excel-Export\")\n",
        "\n",
        "# Start\n",
        "survey_results, pre_learning_results, reasoning_results, bidding_results, learning_summary, brand_analysis_tracking = await run_complete_agent_experiment()"
      ],
      "metadata": {
        "id": "ry79eZsueLoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next time its stops same procedure. Cell A need to be adapted first.\n",
        "\n",
        "# =============================================================================\n",
        "# CELL B.1\n",
        "# =============================================================================\n",
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# ‚ë† Neues Shared Learning Memory anlegen\n",
        "shared_learning_memory = EnhancedMemoryModule(\n",
        "    agent_name=\"SharedLearning\",\n",
        "    max_short_term_size=500\n",
        ")\n",
        "\n",
        "# ‚ë° Liste aller drei Checkpoint-Dateien\n",
        "paths = [\n",
        "    \"/content/drive/MyDrive/agent_learning_results/\"\n",
        "    \"agent_learning_summary_20250628_193840_checkpoint4_40agents.json\",\n",
        "    \"/content/drive/MyDrive/agent_learning_results/\"\n",
        "    \"agent_learning_summary_20250629_090026_checkpoint3_30agents.json\",\n",
        "    \"/content/drive/MyDrive/agent_learning_results/\"\n",
        "    \"agent_learning_summary_20250629_213736_checkpoint3_30agents.json\"\n",
        "]\n",
        "\n",
        "# ‚ë¢ Hilfsstrukturen zum Zusammenf√ºhren\n",
        "merged_survey  = {}\n",
        "merged_bidding = {}\n",
        "merged_wisdom  = []\n",
        "\n",
        "for path in paths:\n",
        "    with open(path, \"r\") as f:\n",
        "        chk = json.load(f)\n",
        "    # ‚ñ∫ Survey adjustments mergen\n",
        "    for qtype, adj in chk[\"survey_adjustments\"].items():\n",
        "        total = adj[\"avg_diff\"] * adj[\"samples\"]\n",
        "        if qtype not in merged_survey:\n",
        "            merged_survey[qtype] = {\"total_diff\": total, \"samples\": adj[\"samples\"]}\n",
        "        else:\n",
        "            merged_survey[qtype][\"total_diff\"] += total\n",
        "            merged_survey[qtype][\"samples\"]    += adj[\"samples\"]\n",
        "    # ‚ñ∫ Bidding adjustments mergen\n",
        "    for brand, adj in chk[\"bidding_adjustments\"].items():\n",
        "        total = adj[\"avg_diff\"] * adj[\"samples\"]\n",
        "        if brand not in merged_bidding:\n",
        "            merged_bidding[brand] = {\"total_diff\": total, \"samples\": adj[\"samples\"]}\n",
        "        else:\n",
        "            merged_bidding[brand][\"total_diff\"] += total\n",
        "            merged_bidding[brand][\"samples\"]    += adj[\"samples\"]\n",
        "    # ‚ñ∫ Wisdom anh√§ngen\n",
        "    merged_wisdom.extend(chk.get(\"accumulated_wisdom\", []))\n",
        "\n",
        "# ‚ë£ avg_diff neu berechnen\n",
        "for d in merged_survey.values():\n",
        "    d[\"avg_diff\"] = d[\"total_diff\"] / d[\"samples\"]\n",
        "for d in merged_bidding.values():\n",
        "    d[\"avg_diff\"] = d[\"total_diff\"] / d[\"samples\"]\n",
        "\n",
        "# ‚ë§ In Shared Memory speichern\n",
        "shared_learning_memory.learning_feedback[\"survey_adjustments\"]  = merged_survey\n",
        "shared_learning_memory.learning_feedback[\"bidding_adjustments\"] = merged_bidding\n",
        "shared_learning_memory.learning_feedback[\"accumulated_wisdom\"]  = merged_wisdom\n",
        "\n",
        "print(\"‚úÖ Merged Learning Memory mit\",\n",
        "      f\"{sum(d['samples'] for d in merged_survey.values())} Umfragen,\",\n",
        "      f\"{sum(d['samples'] for d in merged_bidding.values())} Bids,\",\n",
        "      f\"{len(merged_wisdom)} Wisdom-Eintr√§gen\")\n"
      ],
      "metadata": {
        "id": "lNTJSeOzDa-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL C.1\n",
        "# =============================================================================\n",
        "\n",
        "async def run_complete_agent_experiment():\n",
        "    \"\"\"Runs the complete study\"\"\"\n",
        "\n",
        "    print(\"üöÄ Start study\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "    # Collection storage for all results\n",
        "    all_survey_results = []\n",
        "    all_pre_learning_results = []\n",
        "    all_reasoning_data = []\n",
        "    all_survey_messages = []\n",
        "    all_bidding_results = []\n",
        "    all_bidding_messages = []\n",
        "    all_feedback_messages = []\n",
        "    all_brand_analysis_tracking = []\n",
        "\n",
        "    total_agents = len(profiles_en)\n",
        "    start_time = time.time()\n",
        "    save_interval = 10  # Save all 10 agents\n",
        "\n",
        "    # Output directories\n",
        "    output_dir_survey = \"/content/drive/MyDrive/agent_survey_results/\"\n",
        "    output_dir_bidding = \"/content/drive/MyDrive/agent_bidding_results/\"\n",
        "    output_dir_learning = \"/content/drive/MyDrive/agent_learning_results/\"\n",
        "\n",
        "    import os\n",
        "    os.makedirs(output_dir_survey, exist_ok=True)\n",
        "    os.makedirs(output_dir_bidding, exist_ok=True)\n",
        "    os.makedirs(output_dir_learning, exist_ok=True)\n",
        "\n",
        "    # Base timestamp for consistent file names\n",
        "    base_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # MAIN LOOP:\n",
        "    for agent_idx, profile in enumerate(profiles_en):\n",
        "\n",
        "        agent_name = profile[\"username\"]\n",
        "\n",
        "        print(f\"\\n{'#'*80}\")\n",
        "        print(f\"# AGENT {agent_idx + 1}/{total_agents}: {agent_name}\")\n",
        "        print(f\"{'#'*80}\")\n",
        "\n",
        "        # STEP 1: Profile processing and reflections\n",
        "        print(f\"\\nüîÆ PHASE 1: PROFILE PROCESSING for {agent_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Create memory module for this agent\n",
        "        print(f\"   üß† Create memory module for {agent_name}...\")\n",
        "        memory = EnhancedMemoryModule(\n",
        "            agent_name=agent_name,\n",
        "            max_short_term_size=20\n",
        "        )\n",
        "\n",
        "        # Use global embedding model for efficiency\n",
        "        if 'embeddings_model' not in globals():\n",
        "            print(\"   üì• Load Embedding-Modell...\")\n",
        "            from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "            embeddings_model = HuggingFaceEmbeddings(\n",
        "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "            )\n",
        "        memory.embeddings = embeddings_model\n",
        "\n",
        "        # Create psychological reflections\n",
        "        print(\"   üîÆ Create psychological summaries...\")\n",
        "        general_summary = reflect_profile(profile)\n",
        "        actual_self_summary = reflect_actual_self(profile)\n",
        "        ideal_self_summary = reflect_ideal_self(profile)\n",
        "\n",
        "        # Save profile and all reflections\n",
        "        memory.store_profile(profile, general_summary, actual_self_summary, ideal_self_summary)\n",
        "\n",
        "        print(f\"   üìù General: {general_summary[:100]}...\")\n",
        "        print(f\"   üéØ Actual Self: {actual_self_summary[:80]}...\")\n",
        "        print(f\"   ‚≠ê Ideal Self: {ideal_self_summary[:80]}...\")\n",
        "        print(f\"   ‚úÖ Memory Module for {agent_name} created!\")\n",
        "\n",
        "        # Dictionary for all agent responses\n",
        "        all_agent_responses = {}\n",
        "\n",
        "        # STEP 2: Survey for this agent\n",
        "        print(f\"\\nüìã PHASE 2: SURVEY for {agent_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Updated Call with Brand Analysis Tracking\n",
        "        agent_survey_results, agent_pre_learning_results, agent_reasoning_data, agent_survey_messages, agent_survey_responses, agent_brand_analysis_tracking = \\\n",
        "            await run_complete_agent_survey_for_single_agent(profile, memory, shared_learning_memory)\n",
        "\n",
        "        # Collect Survey-results\n",
        "        all_survey_results.extend(agent_survey_results)\n",
        "        all_pre_learning_results.extend(agent_pre_learning_results)\n",
        "        all_reasoning_data.extend(agent_reasoning_data)\n",
        "        all_survey_messages.extend(agent_survey_messages)\n",
        "        all_brand_analysis_tracking.extend(agent_brand_analysis_tracking)\n",
        "\n",
        "        # Update agent responses dictionary\n",
        "        all_agent_responses.update(agent_survey_responses)\n",
        "\n",
        "        print(f\"\\n‚úÖ Survey for {agent_name} done!\")\n",
        "\n",
        "        # Show Brand Attachment Scores\n",
        "        print(\"   üìä Brand Attachment Scores:\")\n",
        "        for brand in brands:\n",
        "            attachment = calculate_brand_attachment_score(memory, brand)\n",
        "            print(f\"      - {brand}: {attachment:.2%}\")\n",
        "\n",
        "        # STEP 3: Bidding for this agent\n",
        "        print(f\"\\nüõí PHASE 3: Bidding for {agent_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Clear agent messages for bidding\n",
        "        memory.agent_messages.clear()\n",
        "\n",
        "        try:\n",
        "            bidding_results = await run_agent_shop_bot(profile, memory, shared_learning_memory)\n",
        "            all_bidding_results.extend(bidding_results)\n",
        "\n",
        "            # Add bidding results to agent_responses\n",
        "            for bid_result in bidding_results:\n",
        "                brand = bid_result[\"brand\"]\n",
        "                all_agent_responses[f\"bid_{brand}\"] = bid_result[\"bid\"]\n",
        "\n",
        "            # Collect Agent Messages from bidding\n",
        "            for msg in memory.agent_messages:\n",
        "                all_bidding_messages.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"from_agent\": msg.from_agent,\n",
        "                    \"to_agent\": msg.to_agent,\n",
        "                    \"message_type\": msg.message_type.value,\n",
        "                    \"priority\": msg.priority,\n",
        "                    \"content_summary\": msg.content.get(\"summary\", \"\"),\n",
        "                    \"timestamp\": msg.timestamp\n",
        "                })\n",
        "\n",
        "            print(f\"\\n‚úÖ Bidding for {agent_name} done!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n   ‚ùå Critical error when bidding for {agent_name}: {e!s}\")\n",
        "\n",
        "        # SCHRITT 4: FEEDBACK LOOP\n",
        "        print(f\"\\nüìä PHASE 4: FEEDBACK for {agent_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Generate feedback for each brand\n",
        "        for brand in brands:\n",
        "            feedback_msg = feedback_agent.create_feedback_message(all_agent_responses, profile, brand)\n",
        "\n",
        "            # Save feedback in shared memory\n",
        "            shared_learning_memory.add_learning_feedback(feedback_msg)\n",
        "\n",
        "            # Also save in the individual memory\n",
        "            memory.add_agent_message(feedback_msg)\n",
        "\n",
        "            # Collect for logging\n",
        "            all_feedback_messages.append({\n",
        "                \"username\": agent_name,\n",
        "                \"brand\": brand,\n",
        "                \"from_agent\": feedback_msg.from_agent,\n",
        "                \"to_agent\": feedback_msg.to_agent,\n",
        "                \"message_type\": feedback_msg.message_type.value,\n",
        "                \"priority\": feedback_msg.priority,\n",
        "                \"profile_match_score\": feedback_msg.content.get(\"profile_match_score\", 0),\n",
        "                \"matches_used\": feedback_msg.content.get(\"matches_used\", 0),\n",
        "                \"recommendations\": json.dumps(feedback_msg.content.get(\"recommendations\", {})),\n",
        "                \"timestamp\": feedback_msg.timestamp\n",
        "            })\n",
        "\n",
        "            print(f\"   üìö Feedback generated for {brand} and saved in the shared memory\")\n",
        "\n",
        "        # Show Learning Summary\n",
        "        print(f\"\\n   üìà Learning Summary after {agent_name}:\")\n",
        "        print(f\"      Survey Adjustments: {len(shared_learning_memory.learning_feedback['survey_adjustments'])} Pattern\")\n",
        "        print(f\"      Bidding Adjustments: {len(shared_learning_memory.learning_feedback['bidding_adjustments'])} Pattern\")\n",
        "        print(f\"      Accumulated Wisdom: {len(shared_learning_memory.learning_feedback['accumulated_wisdom'])} Insights\")\n",
        "\n",
        "        # Progress bar\n",
        "        elapsed = time.time() - start_time\n",
        "        progress = ((agent_idx + 1) / total_agents) * 100\n",
        "        eta = (elapsed / (agent_idx + 1)) * (total_agents - agent_idx - 1) if agent_idx < total_agents - 1 else 0\n",
        "\n",
        "        print(f\"\\nüìà Overall progress: {progress:.1f}% ({agent_idx + 1}/{total_agents} agents)\")\n",
        "        if eta > 0:\n",
        "            print(f\"   Estimated remaining time: {eta:.0f}s\")\n",
        "\n",
        "        # INCREMENTAL STORAGE: All 10 agents\n",
        "        if (agent_idx + 1) % save_interval == 0 or (agent_idx + 1) == total_agents:\n",
        "            print(f\"\\nüíæ Save after every {agent_idx + 1} Agent...\")\n",
        "\n",
        "            # Create DataFrames\n",
        "            df_survey_results = pd.DataFrame(all_survey_results)\n",
        "            if len(df_survey_results) > 0:\n",
        "                df_pivot = df_survey_results.pivot_table(\n",
        "                    index=['username', 'brand'],\n",
        "                    columns='question',\n",
        "                    values='rating'\n",
        "                ).reset_index()\n",
        "            else:\n",
        "                df_pivot = pd.DataFrame()\n",
        "\n",
        "            # Pre-Learning Results DataFrame\n",
        "            df_pre_learning_results = pd.DataFrame(all_pre_learning_results)\n",
        "            if len(df_pre_learning_results) > 0:\n",
        "                df_pre_learning_pivot = df_pre_learning_results.pivot_table(\n",
        "                    index=['username', 'brand'],\n",
        "                    columns='question',\n",
        "                    values='rating'\n",
        "                ).reset_index()\n",
        "            else:\n",
        "                df_pre_learning_pivot = pd.DataFrame()\n",
        "\n",
        "            # Brand Analysis Tracking DataFrame\n",
        "            df_brand_analysis_tracking = pd.DataFrame(all_brand_analysis_tracking)\n",
        "\n",
        "            # Reasoning-data with Confidence\n",
        "            df_reasoning = pd.DataFrame(all_reasoning_data)\n",
        "\n",
        "            # Agent Communication Logs\n",
        "            df_survey_messages = pd.DataFrame(all_survey_messages)\n",
        "            df_bidding_messages = pd.DataFrame(all_bidding_messages)\n",
        "            df_feedback_messages = pd.DataFrame(all_feedback_messages)\n",
        "\n",
        "            # Bidding-results\n",
        "            df_bidding = pd.DataFrame(all_bidding_results)\n",
        "\n",
        "            # Learning Summary\n",
        "            learning_summary = {\n",
        "                \"survey_adjustments\": shared_learning_memory.learning_feedback[\"survey_adjustments\"],\n",
        "                \"bidding_adjustments\": shared_learning_memory.learning_feedback[\"bidding_adjustments\"],\n",
        "                \"wisdom_count\": len(shared_learning_memory.learning_feedback[\"accumulated_wisdom\"]),\n",
        "                \"agents_processed\": agent_idx + 1,\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "            # Save with checkpoint number\n",
        "            checkpoint_num = (agent_idx + 1) // save_interval\n",
        "            checkpoint_suffix = f\"checkpoint{checkpoint_num}_{agent_idx + 1}agents\"\n",
        "\n",
        "            # Survey-files\n",
        "            if len(df_pivot) > 0:\n",
        "                df_pivot.to_csv(f\"{output_dir_survey}agent_survey_results_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "            if len(df_pre_learning_pivot) > 0:\n",
        "                # Save Pre-Learning Results as Excel\n",
        "                df_pre_learning_pivot.to_excel(f\"{output_dir_survey}agent_survey_results_pre_learning_{base_timestamp}_{checkpoint_suffix}.xlsx\", index=False)\n",
        "\n",
        "            # Save Brand Analysis Tracking as Excel\n",
        "            if len(df_brand_analysis_tracking) > 0:\n",
        "                df_brand_analysis_tracking.to_excel(f\"{output_dir_survey}agent_brand_analysis_impact_{base_timestamp}_{checkpoint_suffix}.xlsx\", index=False)\n",
        "\n",
        "            if len(df_reasoning) > 0:\n",
        "                df_reasoning.to_csv(f\"{output_dir_survey}agent_survey_reasoning_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "            if len(df_survey_messages) > 0:\n",
        "                df_survey_messages.to_csv(f\"{output_dir_survey}agent_survey_communications_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "\n",
        "            # Bidding-files\n",
        "            if len(df_bidding) > 0:\n",
        "                df_bidding.to_csv(f\"{output_dir_bidding}agent_bidding_results_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "            if len(df_bidding_messages) > 0:\n",
        "                df_bidding_messages.to_csv(f\"{output_dir_bidding}agent_bidding_communications_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "\n",
        "            # Learning-files\n",
        "            if len(df_feedback_messages) > 0:\n",
        "                df_feedback_messages.to_csv(f\"{output_dir_learning}agent_feedback_messages_{base_timestamp}_{checkpoint_suffix}.csv\", index=False)\n",
        "\n",
        "            with open(f\"{output_dir_learning}agent_learning_summary_{base_timestamp}_{checkpoint_suffix}.json\", \"w\") as f:\n",
        "                json.dump(learning_summary, f, indent=2)\n",
        "\n",
        "            print(f\"   ‚úÖ Checkpoint {checkpoint_num} saved!\")\n",
        "            print(f\"      Survey Results (with Learning): {len(df_pivot)} Entries\")\n",
        "            print(f\"      Survey Results (Pre-Learning): {len(df_pre_learning_pivot)} Entries\")\n",
        "            print(f\"      Brand Analysis Impact: {len(df_brand_analysis_tracking)} Entries\")\n",
        "            print(f\"      Reasoning Data: {len(df_reasoning)} Entries\")\n",
        "            print(f\"      Bidding Results: {len(df_bidding)} bids\")\n",
        "            print(f\"      Feedback Messages: {len(df_feedback_messages)} Messages\")\n",
        "\n",
        "        # Short break between agents\n",
        "        if agent_idx < total_agents - 1:\n",
        "            print(\"\\n   ‚è≥ Waiting 2 seconds...\")\n",
        "            await asyncio.sleep(2)\n",
        "\n",
        "    # FINAL RESULTS\n",
        "    print(\"\\n\\nüìä Create final result DataFrames...\")\n",
        "\n",
        "    # Create final DataFrames\n",
        "    df_survey_results = pd.DataFrame(all_survey_results)\n",
        "    df_pivot = df_survey_results.pivot_table(\n",
        "        index=['username', 'brand'],\n",
        "        columns='question',\n",
        "        values='rating'\n",
        "    ).reset_index()\n",
        "\n",
        "    # Pre-Learning Results\n",
        "    df_pre_learning_results = pd.DataFrame(all_pre_learning_results)\n",
        "    df_pre_learning_pivot = df_pre_learning_results.pivot_table(\n",
        "        index=['username', 'brand'],\n",
        "        columns='question',\n",
        "        values='rating'\n",
        "    ).reset_index()\n",
        "\n",
        "    # Brand Analysis Tracking\n",
        "    df_brand_analysis_tracking = pd.DataFrame(all_brand_analysis_tracking)\n",
        "\n",
        "    df_reasoning = pd.DataFrame(all_reasoning_data)\n",
        "    df_survey_messages = pd.DataFrame(all_survey_messages)\n",
        "    df_bidding_messages = pd.DataFrame(all_bidding_messages)\n",
        "    df_feedback_messages = pd.DataFrame(all_feedback_messages)\n",
        "    df_bidding = pd.DataFrame(all_bidding_results)\n",
        "\n",
        "    # Final Learning Summary\n",
        "    learning_summary = {\n",
        "        \"survey_adjustments\": shared_learning_memory.learning_feedback[\"survey_adjustments\"],\n",
        "        \"bidding_adjustments\": shared_learning_memory.learning_feedback[\"bidding_adjustments\"],\n",
        "        \"wisdom_count\": len(shared_learning_memory.learning_feedback[\"accumulated_wisdom\"]),\n",
        "        \"final_insights\": shared_learning_memory.learning_feedback[\"accumulated_wisdom\"][-5:]\n",
        "    }\n",
        "\n",
        "    # Save final results\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Survey files\n",
        "    df_pivot.to_csv(f\"{output_dir_survey}agent_survey_results_{timestamp}_final.csv\", index=False)\n",
        "    df_pre_learning_pivot.to_excel(f\"{output_dir_survey}agent_survey_results_pre_learning_{timestamp}_final.xlsx\", index=False)\n",
        "\n",
        "    # Brand Analysis Impact as a final Excel file\n",
        "    if len(df_brand_analysis_tracking) > 0:\n",
        "        df_brand_analysis_tracking.to_excel(f\"{output_dir_survey}agent_brand_analysis_impact_{timestamp}_final.xlsx\", index=False)\n",
        "\n",
        "    df_reasoning.to_csv(f\"{output_dir_survey}agent_survey_reasoning_{timestamp}_final.csv\", index=False)\n",
        "    df_survey_messages.to_csv(f\"{output_dir_survey}agent_survey_communications_{timestamp}_final.csv\", index=False)\n",
        "\n",
        "    # Bidding-files\n",
        "    df_bidding.to_csv(f\"{output_dir_bidding}agent_bidding_results_{timestamp}_final.csv\", index=False)\n",
        "    df_bidding_messages.to_csv(f\"{output_dir_bidding}agent_bidding_communications_{timestamp}_final.csv\", index=False)\n",
        "\n",
        "    # Learning-files\n",
        "    df_feedback_messages.to_csv(f\"{output_dir_learning}agent_feedback_messages_{timestamp}_final.csv\", index=False)\n",
        "    with open(f\"{output_dir_learning}agent_learning_summary_{timestamp}_final.json\", \"w\") as f:\n",
        "        json.dump(learning_summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nüíæ Alle finalen Ergebnisse gespeichert!\")\n",
        "    print(f\"   Survey Results (mit Learning): {len(df_pivot)} Eintr√§ge\")\n",
        "    print(f\"   Survey Results (Pre-Learning): {len(df_pre_learning_pivot)} Eintr√§ge\")\n",
        "    print(f\"   Brand Analysis Impact: {len(df_brand_analysis_tracking)} Eintr√§ge\")\n",
        "    print(f\"   Reasoning Data: {len(df_reasoning)} Eintr√§ge\")\n",
        "    print(f\"   Survey Messages: {len(df_survey_messages)} Nachrichten\")\n",
        "    print(f\"   Bidding Results: {len(df_bidding)} Gebote\")\n",
        "    print(f\"   Bidding Messages: {len(df_bidding_messages)} Nachrichten\")\n",
        "    print(f\"   Feedback Messages: {len(df_feedback_messages)} Feedback-Nachrichten\")\n",
        "    print(f\"   Learning Summary: {len(learning_summary['survey_adjustments'])} Survey-Muster, {len(learning_summary['bidding_adjustments'])} Bidding-Muster\")\n",
        "    print(f\"   Gesamtzeit: {time.time() - start_time:.1f} Sekunden\")\n",
        "\n",
        "    # Show Brand Analysis Impact statistics\n",
        "    if len(df_brand_analysis_tracking) > 0:\n",
        "        print(\"\\nüìä Brand Analysis Impact statistics:\")\n",
        "        total_challenges = df_brand_analysis_tracking['challenge_received'].sum()\n",
        "        total_rating_changes = (df_brand_analysis_tracking['rating_change'] != 0).sum()\n",
        "        avg_rating_change = df_brand_analysis_tracking['rating_change'].mean()\n",
        "\n",
        "        print(f\"   Challenges: {total_challenges}\")\n",
        "        print(f\"   Rating-changes: {total_rating_changes}\")\n",
        "        print(f\"   Average Rating-change: {avg_rating_change:.2f}\")\n",
        "\n",
        "        # Rating changes per brand\n",
        "        print(\"\\n   Rating changes per brand:\")\n",
        "        brand_changes = df_brand_analysis_tracking.groupby('brand')['rating_change'].agg(['count', 'mean', 'sum'])\n",
        "        print(brand_changes)\n",
        "\n",
        "    # Show statistics\n",
        "    if len(df_bidding) > 0:\n",
        "        print(\"\\nüìä Bidding statistics:\")\n",
        "        print(f\"   Average bid: ‚Ç¨{df_bidding['bid'].mean():.2f}\")\n",
        "        print(f\"   Bid range: ‚Ç¨{df_bidding['bid'].min():.2f} - ‚Ç¨{df_bidding['bid'].max():.2f}\")\n",
        "\n",
        "        print(\"\\nüìà Average bids per brand:\")\n",
        "        brand_stats = df_bidding.groupby('brand')['bid'].agg(['mean', 'std', 'count'])\n",
        "        print(brand_stats)\n",
        "\n",
        "        # Correlation between attachment and bids\n",
        "        print(\"\\nüîó Correlation Attachment ‚Üí Bids:\")\n",
        "        # Calculate correlations from collected data\n",
        "        correlations = {}\n",
        "        for brand in brands:\n",
        "            brand_bids = df_bidding[df_bidding['brand'] == brand]\n",
        "            if len(brand_bids) > 0:\n",
        "                # Collect attachment scores from survey data\n",
        "                attachments = []\n",
        "                bids = []\n",
        "\n",
        "                for _, bid_row in brand_bids.iterrows():\n",
        "                    username = bid_row['username']\n",
        "                    # Get attachment data from Survey Results\n",
        "                    user_survey = df_pivot[df_pivot['username'] == username]\n",
        "                    if len(user_survey) > 0:\n",
        "                        brand_survey = user_survey[user_survey['brand'] == brand]\n",
        "                        if len(brand_survey) > 0:\n",
        "                            # Calculate average attachment score\n",
        "                            attachment_cols = [col for col in df_pivot.columns if col.startswith('Q11_')]\n",
        "                            if attachment_cols:\n",
        "                                attachment_values = brand_survey[attachment_cols].values[0]\n",
        "                                # Filter NaN values\n",
        "                                valid_values = [v for v in attachment_values if pd.notna(v)]\n",
        "                                if valid_values:\n",
        "                                    avg_attachment = (sum(valid_values) / len(valid_values) - 1) / 4  # Convert 1-5 to 0-1\n",
        "                                    attachments.append(avg_attachment)\n",
        "                                    bids.append(bid_row['bid'])\n",
        "\n",
        "                if len(attachments) > 1:\n",
        "                    correlation = pd.Series(attachments).corr(pd.Series(bids))\n",
        "                    correlations[brand] = correlation\n",
        "                    print(f\"   {brand}: r={correlation:.3f}\")\n",
        "\n",
        "    # Show final Learning Insights\n",
        "    print(\"\\nüß† FINAL LEARNING INSIGHTS:\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nüìä Survey Learning Patterns:\")\n",
        "    for q_type, adj in learning_summary[\"survey_adjustments\"].items():\n",
        "        if adj[\"samples\"] > 0:\n",
        "            print(f\"   {q_type}: {adj['avg_diff']:+.2f} (based on {adj['samples']} Samples)\")\n",
        "\n",
        "    print(\"\\nüí∞ Bidding Learning Patterns:\")\n",
        "    for brand, adj in learning_summary[\"bidding_adjustments\"].items():\n",
        "        if adj[\"samples\"] > 0:\n",
        "            print(f\"   {brand}: ‚Ç¨{adj['avg_diff']:+.2f} (based on {adj['samples']} Samples)\")\n",
        "\n",
        "    print(\"\\nüí° Top Accumulated Wisdom:\")\n",
        "    for idx, wisdom in enumerate(learning_summary.get(\"final_insights\", [])[-3:], 1):\n",
        "        if isinstance(wisdom, dict) and \"insight\" in wisdom:\n",
        "            insight = wisdom[\"insight\"]\n",
        "            if isinstance(insight, dict) and \"general_tendency\" in insight:\n",
        "                print(f\"   {idx}. {insight['general_tendency']}\")\n",
        "\n",
        "    print(\"\\nüéâ AGENT STUDY DONE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nüìÇ Saved files:\")\n",
        "    print(f\"   Final results:\")\n",
        "    print(f\"   - Survey Results (with Learning): agent_survey_results_{timestamp}_final.csv\")\n",
        "    print(f\"   - Survey Results (Pre-Learning): agent_survey_results_pre_learning_{timestamp}_final.xlsx\")\n",
        "    print(f\"   - Brand Analysis Impact: agent_brand_analysis_impact_{timestamp}_final.xlsx\")\n",
        "    print(f\"   - Survey Reasoning: agent_survey_reasoning_{timestamp}_final.csv\")\n",
        "    print(f\"   - Survey Communications: agent_survey_communications_{timestamp}_final.csv\")\n",
        "    print(f\"   - Bidding Results: agent_bidding_results_{timestamp}_final.csv\")\n",
        "    print(f\"   - Bidding Communications: agent_bidding_communications_{timestamp}_final.csv\")\n",
        "    print(f\"   - Feedback Messages: agent_feedback_messages_{timestamp}_final.csv\")\n",
        "    print(f\"   - Learning Summary: agent_learning_summary_{timestamp}_final.json\")\n",
        "    print(f\"\\n   Plus {(total_agents // save_interval)} Checkpoint files for incremental analysis\")\n",
        "\n",
        "    return df_pivot, df_pre_learning_pivot, df_reasoning, df_bidding, learning_summary, df_brand_analysis_tracking\n",
        "\n",
        "\n",
        "async def run_complete_agent_survey_for_single_agent(profile: dict, memory: EnhancedMemoryModule, shared_memory: EnhancedMemoryModule):\n",
        "    \"\"\"Conducts the complete survey for a single agent and all brands\"\"\"\n",
        "\n",
        "    agent_name = profile[\"username\"]\n",
        "    survey_results = []\n",
        "    pre_learning_results = []\n",
        "    reasoning_data = []\n",
        "    agent_messages = []\n",
        "    brand_analysis_tracking = []\n",
        "\n",
        "    # Dictionary for agent responses (for feedback)\n",
        "    agent_responses = {}\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üë§ Agent: {agent_name}\")\n",
        "    print(f\"   Actual: {' > '.join(profile['actual_traits'][:2])}...\")\n",
        "    print(f\"   Ideal: {' > '.join(profile['ideal_traits'][:2])}...\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Empty STM, CoT History and Messages before each new person\n",
        "    memory.short_term_memory.clear()\n",
        "    memory.cot_history.clear()\n",
        "    memory.agent_messages.clear()\n",
        "    memory._survey_ratings.clear()\n",
        "\n",
        "    # Survey for every brand\n",
        "    for brand in brands:\n",
        "        print(f\"\\n   üìã {brand} Survey with agent:\")\n",
        "\n",
        "        try:\n",
        "            # Updated Call with Brand Analysis Tracking\n",
        "            results, pre_learning, brand_analysis = await run_agent_survey(profile, brand, memory, shared_memory)\n",
        "\n",
        "            # Collect data for DataFrame\n",
        "            for question_code in [\"Q9_consistent\", \"Q9_mirror\", \"Q10_consistent\", \"Q10_mirror\"] + [f\"Q11_{item}\" for item in attachment_items]:\n",
        "                rating = results[question_code]\n",
        "                pre_learning_rating = pre_learning[question_code]\n",
        "\n",
        "                # Save for feedback\n",
        "                agent_responses[question_code] = rating\n",
        "\n",
        "                survey_results.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"brand\": brand,\n",
        "                    \"question\": question_code,\n",
        "                    \"rating\": rating,\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "\n",
        "                # Save Pre-Learning Results\n",
        "                pre_learning_results.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"brand\": brand,\n",
        "                    \"question\": question_code,\n",
        "                    \"rating\": pre_learning_rating,\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "\n",
        "                # Find the corresponding reasoning from the memory\n",
        "                reasoning_entry = next(\n",
        "                    (m for m in memory.short_term_memory\n",
        "                     if m.get(\"question\", \"\").endswith(question_code) and m.get(\"brand\") == brand),\n",
        "                    None\n",
        "                )\n",
        "\n",
        "                if reasoning_entry:\n",
        "                    reasoning_data.append({\n",
        "                        \"username\": agent_name,\n",
        "                        \"brand\": brand,\n",
        "                        \"question\": question_code,\n",
        "                        \"rating\": rating,\n",
        "                        \"pre_learning_rating\": pre_learning_rating,\n",
        "                        \"reasoning\": reasoning_entry[\"reasoning\"],\n",
        "                        \"confidence\": reasoning_entry.get(\"confidence\", None)\n",
        "                    })\n",
        "\n",
        "            # Collect Brand Analysis Tracking\n",
        "            brand_analysis_tracking.extend(brand_analysis)\n",
        "\n",
        "            # Collect agent messages for this brand\n",
        "            brand_messages = [msg for msg in memory.agent_messages\n",
        "                            if any(brand in str(msg.content) for brand in [brand])]\n",
        "\n",
        "            for msg in brand_messages:\n",
        "                agent_messages.append({\n",
        "                    \"username\": agent_name,\n",
        "                    \"brand\": brand,\n",
        "                    \"from_agent\": msg.from_agent,\n",
        "                    \"to_agent\": msg.to_agent,\n",
        "                    \"message_type\": msg.message_type.value,\n",
        "                    \"priority\": msg.priority,\n",
        "                    \"content_summary\": msg.content.get(\"summary\", \"\"),\n",
        "                    \"timestamp\": msg.timestamp\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error {brand}: {str(e)}\")\n",
        "\n",
        "    # Return Brand Analysis Tracking with\n",
        "    return survey_results, pre_learning_results, reasoning_data, agent_messages, agent_responses, brand_analysis_tracking\n",
        "\n",
        "print(\"‚úÖ Sequential Agent Study runner defined!\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ Ready to start\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nThe agent system processes each agent sequentially:\")\n",
        "print(\"   1Ô∏è‚É£ Create profile & memory ‚Üí 2Ô∏è‚É£ Survey ‚Üí 3Ô∏è‚É£ Bidding ‚Üí 4Ô∏è‚É£ Feedback ‚Üí 5Ô∏è‚É£ Learning Update\")\n",
        "print(\"\\nüíæ Automatic saving every 10 agents\")\n",
        "print(\"üìä Brand Analysis Impact Tracking as Excel-Export\")\n",
        "\n",
        "# Start\n",
        "survey_results, pre_learning_results, reasoning_results, bidding_results, learning_summary, brand_analysis_tracking = await run_complete_agent_experiment()"
      ],
      "metadata": {
        "id": "x1Gj4Ehckf0i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}